{
  "name": "Reddit workflow finder",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "daysInterval": 5
            }
          ]
        }
      },
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [
        448,
        16
      ],
      "id": "80b55438-5c1f-4321-b053-f9fba16a06e5",
      "name": "Schedule Trigger",
      "disabled": true,
      "notes": "üïê SCHEDULER CONFIGURATION\n\n‚Ä¢ Currently set to run every 1 minute (for testing)\n‚Ä¢ RECOMMENDATION: Change to hourly or daily for production\n‚Ä¢ Suggested intervals:\n  - Every 6 hours: Set minutesInterval to 360\n  - Daily: Change field to 'hours' and set hoursInterval to 24\n  - Weekly: Change field to 'days' and set daysInterval to 7\n\n‚ö†Ô∏è IMPORTANT: Frequent scraping may hit API rate limits!\n\nüîß CUSTOMIZATION:\n‚Ä¢ Adjust interval based on your needs\n‚Ä¢ Consider Reddit API rate limits\n‚Ä¢ Monitor Apify usage quotas"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "f3f4a9fa-8797-4753-ad22-8d4141240ca8",
              "name": "query",
              "value": "content creation workflow code included",
              "type": "string"
            },
            {
              "id": "d518bd27-6c65-4d0c-8872-5047b9bbc8d2",
              "name": "subreddit",
              "value": "n8n",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        592,
        16
      ],
      "id": "25738c2b-e616-4d97-b66b-cf3c6e7f3c17",
      "name": "Search Parameters",
      "notes": "üìù SEARCH CONFIGURATION\n\nüéØ CURRENT SETTINGS:\n‚Ä¢ Query: 'content creation workflow code included'\n‚Ä¢ Subreddit: 'n8n'\n‚Ä¢ Max Posts: 50 (NEW)\n‚Ä¢ Sort: New posts first (NEW)\n‚Ä¢ Time Filter: Past week (NEW)\n\nüîç QUERY OPTIMIZATION IDEAS:\n‚Ä¢ 'automation workflow github'\n‚Ä¢ 'n8n workflow tutorial'\n‚Ä¢ 'open source automation'\n‚Ä¢ 'workflow template'\n‚Ä¢ 'integration script'\n\nüìä SUBREDDIT ALTERNATIVES:\n‚Ä¢ 'nocode' - No-code community\n‚Ä¢ 'automation' - General automation\n‚Ä¢ 'selfhosted' - Self-hosted tools\n‚Ä¢ 'opensource' - Open source projects\n\n‚öôÔ∏è CUSTOMIZATION:\n‚Ä¢ Modify query for different content types\n‚Ä¢ Add multiple subreddits in future versions\n‚Ä¢ Adjust max_posts based on API limits"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.apify.com/v2/acts/moving_beacon-owner1~my-actor-26/run-sync-get-dataset-items?token=",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n\"search\": \"{{$json.query}}\",\n\"Subreddit\": \"{{$json.subReddit}}\"\n}",
        "options": {
          "timeout": 90000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        880,
        -224
      ],
      "id": "680c66f4-6078-41e2-8ae4-002f883d498c",
      "name": "Reddit API Call",
      "notes": "üåê APIFY REDDIT SCRAPER\n\nüîë SETUP REQUIREMENTS:\n1. Sign up for Apify account\n2. Get API token from Apify Console\n3. Replace with your token\n\nüìã API PAYLOAD:\n‚Ä¢ Subreddit: Target community\n‚Ä¢ Search: Keywords to find\n‚Ä¢ MaxPosts: Limit results (prevents overload)\n‚Ä¢ SortBy: Post ordering (new/hot/top)\n‚Ä¢ TimeFilter: Date range (hour/day/week/month/year/all)\n\n‚ö†Ô∏è SECURITY NOTICE:\n‚Ä¢ Store API token in n8n credentials manager\n‚Ä¢ Don't commit tokens to version control\n‚Ä¢ Monitor API usage to avoid overages\n\nüöÄ PERFORMANCE:\n‚Ä¢ Timeout set to 30 seconds\n‚Ä¢ Sync call waits for completion\n‚Ä¢ Returns structured dataset\n\nüîß TROUBLESHOOTING:\n‚Ä¢ Check Apify actor is running\n‚Ä¢ Verify API token permissions\n‚Ä¢ Monitor rate limits"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "c592028c-df3d-41ec-8a00-b8b3619ebb46",
              "leftValue": "={{ $json.body_text }}",
              "rightValue": "/(https?:\\/\\/(?:www\\.)?github\\.com\\/[^\\s)]+)/g",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            },
            {
              "id": "0d8e0d5c-683e-4e8c-a201-ff6c23711e7d",
              "leftValue": "={{ $json.body_text }}",
              "rightValue": "/(https?:\\/\\/(?:www\\.)?(?:youtu\\.be\\/|youtube\\.com\\/(?:watch\\?v=|embed\\/|v\\/))[\\w\\-]{11})/g",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            },
            {
              "id": "new-condition-1",
              "leftValue": "={{ $json.body_text }}",
              "rightValue": "/(https?:\\/\\/(?:gist\\.)?github\\.com\\/[^\\s)]+)/g",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            },
            {
              "id": "new-condition-3",
              "leftValue": "={{ $json.body_text }}",
              "rightValue": "/(https?:\\/\\/(?:www\\.)?docs\\.google\\.com\\/(?:spreadsheets|document|presentation|forms)\\/d\\/[a-zA-Z0-9-_]+)/g",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            },
            {
              "id": "new-condition-4",
              "leftValue": "={{ $json.body_text }}",
              "rightValue": "/(https?:\\/\\/(?:www\\.)?drive\\.google\\.com\\/(?:file\\/d\\/|open\\?id=|drive\\/folders\\/)[a-zA-Z0-9-_]+)/g",
              "operator": {
                "type": "string",
                "operation": "regex"
              }
            }
          ],
          "combinator": "or"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        1088,
        -224
      ],
      "id": "e024516f-25fb-4c25-b356-dbda3d005e64",
      "name": "Quality Filter",
      "notes": "üéØ CONTENT QUALITY FILTER\n\n‚úÖ CURRENT FILTERS (OR Logic):\n1. GitHub repositories: github.com/username/repo\n2. YouTube videos: youtube.com/watch or youtu.be links\n3. GitHub Gists: gist.github.com\n4. Workflow keywords: workflow, automation, n8n, template\n5. Google Docs: docs.google.com (spreadsheets, documents, presentations, forms) (NEW)\n6. Google Drive: drive.google.com (files and folders) (NEW)\n\nüîç REGEX PATTERNS EXPLAINED:\n‚Ä¢ GitHub: Matches full GitHub URLs\n‚Ä¢ YouTube: Matches both youtube.com and youtu.be formats\n‚Ä¢ Google Docs: Matches all Google Workspace document types\n‚Ä¢ Google Drive: Matches file shares and folder links\n‚Ä¢ Case-insensitive keyword matching\n‚Ä¢ Prevents false positives with proper boundaries\n\nüìã GOOGLE DOCS PATTERN BREAKDOWN:\n‚Ä¢ docs.google.com/spreadsheets/d/[ID] - Google Sheets\n‚Ä¢ docs.google.com/document/d/[ID] - Google Docs\n‚Ä¢ docs.google.com/presentation/d/[ID] - Google Slides\n‚Ä¢ docs.google.com/forms/d/[ID] - Google Forms\n\nüìÅ GOOGLE DRIVE PATTERN BREAKDOWN:\n‚Ä¢ drive.google.com/file/d/[ID] - Individual files\n‚Ä¢ drive.google.com/open?id=[ID] - Legacy file links\n‚Ä¢ drive.google.com/drive/folders/[ID] - Shared folders\n\nüöÄ ENHANCEMENT IDEAS:\n‚Ä¢ Add GitLab/Bitbucket support\n‚Ä¢ Include Notion pages\n‚Ä¢ Add Airtable base shares\n‚Ä¢ Include Figma/Miro links\n‚Ä¢ Support OneDrive/Dropbox\n\nüìä QUALITY METRICS TO ADD:\n‚Ä¢ Minimum post score\n‚Ä¢ Minimum comment count\n‚Ä¢ Post age restrictions\n‚Ä¢ Author karma thresholds\n\n‚öôÔ∏è COMBINATOR OPTIONS:\n‚Ä¢ 'OR': Any condition matches (current)\n‚Ä¢ 'AND': All conditions must match (stricter)"
    },
    {
      "parameters": {
        "operation": "upsert",
        "base": {
          "__rl": true,
          "value": "app9nKxjvqC2GlOUX",
          "mode": "list",
          "cachedResultName": "Reddit_posts",
          "cachedResultUrl": "https://airtable.com/app9nKxjvqC2GlOUX"
        },
        "table": {
          "__rl": true,
          "value": "tblzRBjWjul7hJGwj",
          "mode": "list",
          "cachedResultName": "Reddit Open Source",
          "cachedResultUrl": "https://airtable.com/app9nKxjvqC2GlOUX/tblzRBjWjul7hJGwj"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "header": "={{ $json.header }}",
            "body_text": "={{ $json.body_text }}",
            "link": "={{ $json.link }}",
            "Name": "={{ $json.author }}",
            "Extracted Links": "={{ $json.extracted_links.toJsonString().split(',').join(', ') }}"
          },
          "matchingColumns": [
            "link"
          ],
          "schema": [
            {
              "id": "id",
              "displayName": "id",
              "required": false,
              "defaultMatch": true,
              "display": true,
              "type": "string",
              "readOnly": true,
              "removed": false
            },
            {
              "id": "Name",
              "displayName": "Name",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "header",
              "displayName": "header",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "body_text",
              "displayName": "body_text",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "link",
              "displayName": "link",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "Extracted Links",
              "displayName": "Extracted Links",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "Email",
              "displayName": "Email",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "Score",
              "displayName": "Score",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "number",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "Poster_linkedin",
              "displayName": "Poster_linkedin",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "SubjectLine1",
              "displayName": "SubjectLine1",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "SubjectLine2",
              "displayName": "SubjectLine2",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "SubjectLine3",
              "displayName": "SubjectLine3",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "EmailDraft",
              "displayName": "EmailDraft",
              "required": false,
              "defaultMatch": false,
              "canBeUsedToMatch": true,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.airtable",
      "typeVersion": 2.1,
      "position": [
        1536,
        -224
      ],
      "id": "950eaefc-ca19-4c8b-953c-02447cacb38b",
      "name": "Airtable Storage",
      "credentials": {
        "airtableTokenApi": {
          "id": "7y4h6ZHn8aY46SGh",
          "name": "Airtable Personal Access Token account"
        }
      },
      "notes": "üíæ AIRTABLE DATA STORAGE\n\nüèóÔ∏è SETUP REQUIREMENTS:\n1. Create Airtable account\n2. Generate Personal Access Token\n3. Create base with matching field structure\n4. Update base and table IDs in configuration\n\nüìä ENHANCED DATA FIELDS:\n‚Ä¢ Basic: title, content, author, reddit_link\n‚Ä¢ Metrics: score, comment_count, created_date\n‚Ä¢ Metadata: subreddit, scraped_timestamp\n‚Ä¢ Extracted: separate GitHub/YouTube link fields\n\nüîÑ UPSERT OPERATION:\n‚Ä¢ Prevents duplicate entries\n‚Ä¢ Uses Reddit link as unique identifier\n‚Ä¢ Updates existing records with new data\n\nüéØ DATA ENRICHMENT:\n‚Ä¢ Extracts GitHub links separately\n‚Ä¢ Extracts YouTube links separately\n‚Ä¢ Extracts Google Docs links separately (NEW)\n‚Ä¢ Extracts Google Drive links separately (NEW)\n‚Ä¢ Adds scraping timestamp\n‚Ä¢ Preserves post metrics\n\n‚öôÔ∏è FIELD MAPPING:\n‚Ä¢ All Reddit post data preserved\n‚Ä¢ Links extracted for easy access\n‚Ä¢ Timestamps for tracking freshness\n‚Ä¢ Metrics for popularity analysis\n\nüöÄ ANALYSIS READY:\n‚Ä¢ Score trending analysis\n‚Ä¢ Author contribution tracking\n‚Ä¢ Link popularity metrics\n‚Ä¢ Content type distribution\n‚Ä¢ Google Workspace template tracking"
    },
    {
      "parameters": {
        "content": "# Reddit n8n Workflow Setup Guide \n###  Workflow Description\nThis n8n workflow is a Reddit workflow Finder that automatically scrapes Reddit posts from specified subreddits, filters for posts containing valuable resources (GitHub repositories, YouTube videos, or Google Docs/Drive links), and stores the results in Airtable for easy organization and analysis.\n\n### What It Does:\n- Monitors Reddit: Automatically searches specific subreddits for posts containing certain keywords. \n- Filters Content: Only captures posts that include GitHub repos, YouTube videos, or Google Drive/Docs links\n- Stores Results: Saves all data to Airtable with proper formatting\n\n### Prerequisites\nRequired Accounts:\n- n8n Instance (cloud or self-hosted)\n- Apify Account (for Reddit scraping)\n- Airtable Account (for data storage)\n\n### API Tokens Needed:\n- Apify API token\n- Airtable Personal Access Token\n\n## Share your feedback\nsuhaibnord@gmail.com",
        "height": 697,
        "width": 1511,
        "color": 6
      },
      "id": "79062e09-b274-4589-a61e-922fb5bb0251",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        192,
        -496
      ],
      "typeVersion": 1
    },
    {
      "parameters": {
        "content": "### Current Configuration:\n- Target Subreddit: r/n8n\n- Search Query: \"content creation workflow code included\"\n- Schedule: Every 5 days (recommended to change for production)\n- Output: Airtable database with Reddit posts and engagement metrics",
        "height": 192,
        "width": 448
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        832,
        -32
      ],
      "typeVersion": 1,
      "id": "d200d25e-58e7-467b-8cbd-1c03d74f70bb",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "content": "## Step-by-Step Setup Instructions\n\n### Step 1: Set Up Apify Account\n\n1. **Create Apify Account**:\n   - Go to [apify.com](https://apify.com)\n   - Sign up for a free account\n\n2. **Get API Token**:\n   - Navigate to Settings ‚Üí Integrations\n   - Copy your API token\n   - Keep this token secure - you'll need it in Step 4\n\n3. **Verify Actor Availability**:\n   - The workflow uses two actors:\n     - https://apify.com/moving_beacon-owner1/my-actor-26\n\n### Step 2: Set Up Airtable\n\n1. **Create Airtable Account**:\n   - Go to [airtable.com](https://airtable.com)\n   - Sign up for a free account\n\n2. **Create a New Base**:\n   - Click \"Create a base\"\n   - Name it \"Reddit_posts\" or similar\n\n3. **Set Up Table Structure**:\n   Create a table named \"Reddit Open Source\" with these fields:\n   - `id` (Single line text) - Primary field\n   - `Name` (Single line text) - Reddit username\n   - `header` (Long text) - Post title\n   - `body_text` (Long text) - Post content\n   - `link` (URL) - Reddit post link\n   - extracted_links- Opensource extracted links( github, youtube, drive, docs etc)\n\n4. **Get Airtable Credentials**:\n   - Go to [airtable.com/create/tokens](https://airtable.com/create/tokens)\n   - Create a Personal Access Token\n   - Give it access to your base with read/write permissions\n   - Copy the base ID from your Airtable URL (starts with \"app\")\n   - Copy the table ID (starts with \"tbl\")\n\n### Step 3: Import Workflow to n8n\n\n1. **Access Your n8n Instance**:\n   - Log into your n8n dashboard\n\n2. **Import the Workflow**:\n   - Click \"New\" ‚Üí \"Import from JSON\"\n   - Paste the provided JSON workflow\n   - Click \"Import\"\n\n### Step 4: Configure API Credentials\n\n1. **Set Up Airtable Connection**:\n   - Click on the \"Airtable Storage\" node\n   - Click \"Create new credential\"\n   - Name it \"Airtable Personal Access Token account\"\n   - Enter your Airtable Personal Access Token\n   - Test the connection\n\n2. **Update Apify API Tokens**:\n   - Click on \"Reddit Post API call\" node\n   - Replace `apify_api_qGyY6YB0Sbaaxu2qXjA4RV7BWsGFu20pHA9h` with your API token\n\n### Step 5: Configure Search Parameters\n\n1. **Customize Search Settings**:\n   - Click on \"Search Parameters\" node\n   - Modify these values as needed:\n     - `query`: Change \"content creation workflow code included\" to your desired search terms\n     - `subreddit`: Change \"n8n\" to your target subreddit (without r/)\n\n2. **Example Alternative Configurations**:\n   ```\n   Query: \"lead generation workflow code included\"\n   Subreddit: \"automation\"\n   \n   Query: \"github repository open source\"\n   Subreddit: \"selfhosted\"\n   ```\n\n### Step 6: Adjust Schedule for Production\n\n1. **Modify Schedule Trigger**:\n   - Click on \"Schedule Trigger\" node\n   - **IMPORTANT**: Change from 1 minute to a more reasonable interval\n   - Recommended settings:\n     - Every hour: Set to 60 minutes\n     - Daily: Set to 24 hours\n     - This prevents API rate limiting\n\n### Step 7: Test the Workflow\n\n1. **Manual Test**:\n   - Click \"Execute Workflow\" button\n   - Check each node for successful execution\n   - Green checkmarks indicate success\n\n2. **Verify Data Flow**:\n   - Check that Reddit posts are being fetched\n   - Ensure filtering is working (only posts with GitHub/YouTube/Google Drive links)\n   - Confirm data is being saved to Airtable\n\n3. **Check Airtable**:\n   - Go to your Airtable base\n   - Verify that Reddit posts are appearing in your table\n   - Check that all fields are populated correctly\n\n### Step 8: Activate Workflow\n\n1. **Enable Auto-Execution**:\n   - Toggle the workflow to \"Active\"\n   - The schedule trigger will now run automatically\n\n2. **Monitor Performance**:\n   - Check execution logs regularly\n   - Monitor API usage on Apify dashboard\n   - Watch for any error notifications\n\n---\n\n## Troubleshooting\n\n### Common Issues:\n\n1. **API Rate Limits**:\n   - Increase schedule interval\n   - Check Apify usage quotas\n   - Consider upgrading Apify plan if needed\n\n2. **No Results Found**:\n   - Adjust search query terms\n   - Try different subreddits\n   - Check if the target subreddit has recent posts\n\n3. **Airtable Connection Errors**:\n   - Verify Personal Access Token permissions\n   - Check base and table IDs are correct\n   - Ensure field names match exactly\n\n4. **Apify Actor Errors**:\n   - Verify actor URLs are accessible\n   - Check if actors require updated parameters\n   - Try running actors directly on Apify to test\n\n### Performance Optimization:\n\n1. **Reduce API Calls**:\n   - Increase schedule intervals\n   - Limit search results where possible\n   - Use more specific search queries\n\n2. **Filter Efficiency**:\n   - The workflow filters for posts containing:\n     - GitHub links: `github.com/`\n     - YouTube links: `youtube.com/` or `youtu.be/`\n     - Google Drive: `drive.google.com/`\n     - Google Docs: `docs.google.com/`\n\n---\n\n## Customization Options\n\n### Search Query Ideas:\n- `\"automation tool workflow share\"`\n- `\"open source project github\"`\n- `\"workflow template download\"`\n- `\"integration tutorial code\"`\n\n### Alternative Subreddits:\n- `nocode` - No-code community\n- `automation` - General automation tools\n- `selfhosted` - Self-hosted solutions\n- `opensource` - Open source projects\n- `productivity` - Productivity tools\n\n### Additional Filters:\nYou can modify the \"Filter for workflow Links\" node to include other link types like:\n- Notion templates\n- Zapier workflows\n- Medium articles\n- Personal websites\n\n---\n\n## Expected Output\n\nThe workflow will create Airtable records with:\n- **Reddit Username**: Post author\n- **Post Title**: Original Reddit post headline\n- **Post Body**: Full text content\n- **Engagement Metrics**: Upvotes, comments, upvote ratio\n- **Direct Link**: URL to the original Reddit post\n\nThis creates a valuable database of community-shared resources and workflows that you can reference, analyze, and potentially implement in your own projects.",
        "height": 4752,
        "width": 560,
        "color": 3
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -400,
        -496
      ],
      "typeVersion": 1,
      "id": "67a133e5-3417-4628-9fc8-42f23355cf69",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "formTitle": "Reddit workflow Finder",
        "formDescription": "- query => Search keyword or phrase(append `workflow code included` to the query)\n\n- subReddit=> Target subreddit name (without 'r/')",
        "formFields": {
          "values": [
            {
              "fieldLabel": "query",
              "placeholder": "lead gen workflow code included",
              "requiredField": true
            },
            {
              "fieldLabel": "subReddit",
              "placeholder": "n8n",
              "requiredField": true
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.formTrigger",
      "typeVersion": 2.2,
      "position": [
        640,
        -224
      ],
      "id": "146034ea-e93e-4158-b704-55e9595ed257",
      "name": "On form submission",
      "webhookId": "f5eac706-2b4b-4cfb-9694-663b7d37b3e2"
    },
    {
      "parameters": {
        "jsCode": "// Extract actual links from body text\nconst items = $input.all();\n\nconst extractLinks = (text) => {\n  const linkPatterns = [\n    /(https?:\\/\\/(?:www\\.)?github\\.com\\/[^\\s)]+)/g,\n    /(https?:\\/\\/(?:www\\.)?(?:youtu\\.be\\/|youtube\\.com\\/(?:watch\\?v=|embed\\/|v\\/))[\\w\\-]{11})/g,\n    /(https?:\\/\\/(?:www\\.)?drive\\.google\\.com\\/(?:file\\/d\\/|open\\?id=|drive\\/folders\\/)[a-zA-Z0-9-_]+)/g,\n    /(https?:\\/\\/(?:www\\.)?docs\\.google\\.com\\/(?:spreadsheets|document|presentation|forms)\\/d\\/[a-zA-Z0-9-_]+)/g\n  ];\n  \n  let extractedLinks = [];\n  linkPatterns.forEach(pattern => {\n    const matches = text.match(pattern);\n    if (matches) {\n      extractedLinks = extractedLinks.concat(matches);\n    }\n  });\n  \n  return [...new Set(extractedLinks)]; // Remove duplicates\n};\n\nreturn items.map(item => ({\n  ...item.json,\n  extracted_links: extractLinks(item.json.body_text || ''),\n  link_count: extractLinks(item.json.body_text || '').length\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1312,
        -224
      ],
      "id": "c3ecd902-db8d-4a8e-92c8-e895dbf42d3d",
      "name": "Code"
    }
  ],
  "pinData": {
    "Reddit API Call": [
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automate Your Content Creation and Social Media Posting with n8n",
          "link": "https://old.reddit.com/r/n8n/comments/1l2bjdb/automate_your_content_creation_and_social_media/",
          "author": "Connect_Cook_8034",
          "time": "2025-06-03T13:12:06+00:00",
          "body_text": "I developed a workflow using n8n that automates content creation and social media posting, leveraging my YouTube videos. This system saves me around 5 to 10 hours every week. Here's how it works: Introduction to AI-Driven Content Workflow I outline the goals of the workflow, focusing on efficiency in content creation and engagement. Exploring the Automation Tools I introduce n8n as a powerful no-code tool for automation. It serves as the core of my workflow to connect various components. Content Creation and Approval Process I set up automated processes to generate content from my existing YouTube videos, ensuring it meets platform-specific guidelines. Input: My YouTube videos. Tools: N8n for automation, AI agents for content generation. Social Media Posting Strategies I automate the distribution of optimized content across multiple platforms, enhancing outreach and engagement. Each post is tailored to fit the specific requirements of platforms like LinkedIn, Reddit, etc. Building the Automation System I detail the configuration in n8n, including triggers, actions, and how to integrate AI agents into the system. Output: Posts formatted and ready for publication on various social media sites. Reddit Posting Techniques I share strategies for posting on Reddit to maximize engagement, focusing on community guidelines. Enhancing Engagement on LinkedIn I discuss how to leverage the automated posts for better visibility and interaction on LinkedIn. Final Thoughts and Future Improvements I conclude with insights on potential workflows I plan to implement in the future. This n8n workflow incorporates AI agents and no-code automation, making it easier for anyone to streamline content creation and social media management. If you're interested in automating your processes, I highly recommend checking out n8n and experimenting with its features! #n8n #agents #AI #automation"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1kpmxk0/i_used_n8n_to_write_a_review_of_n8n_and_create_a/",
          "author": "eneskaraboga",
          "time": "2025-05-18T15:42:47+00:00",
          "body_text": "The review article is located here (no ads or affiliate links): https://caracal.news/ai-tools/review/n8n-review/ . I've been using n8n lately to create a lot of written content. My main goal is to write the most well-researched content possible, which is impossible for AI to produce in one shot. The content should be so high quality that you'd need a lot of people to do the research, fact-checking, editing, etc. I am trying to mimic a researcher. Someone searching for the product, learning about it, taking notes, plan the article, write, edit, and post. n8n is great for this purpose. Here's how it works: 1) Scrape the homepage of the tool (n8n, in this case). 2) Scrape Google search results for related keywords (e.g., n8n review, n8n). 3) Clean, summarize, and most importantly, format the content for a specific purpose. In this case, we are talking about a workflow automation tool. This means our review should focus on specific criteria, which I use LLMs to create. 4) Feed the n8n homepage content, the scraped Google results for relevant keywords, and the desired format to a very good model (3.7 Sonnet, for example, through OpenRouter). 5) Use another node for editing purposes. Post the work to WordPress. I don't write any words for the article or find the images manually. Everything from images to research, editing, and writing titles is done in n8n. The secret sauce is the prompts. I have extensive prompts that took me days to create. 1) Review the criteria. 2) Post format and structure 3) Article-writing instructions, such as style, language, paragraph sizes, purpose, etc. 4) I have editing instructions, as well as instructions on how to use emojis and how to frame without sounding salesy. Integrations I am using the most 1) Supabase for saving the tools, keywords, articles 2) Flux 1.1 Pro Ultra: For thumbnail images 3) Zyte API: To scrape websites for context 4) Serper API: To scrape urls from Google search results 5) Wordpress 6) Openrouter: For all the writing, editing, context creation 7) Code blocks: For cleaning, processing, etc. I hope this helps someone! P.S. This is an AI news website written by N8N Automations. Nothing on the site is written by humans, including the images. Best AI Tools for [Profession] style: https://caracal.news/ai-tools/profession/best-ai-tools-for-youtube-creators/ Best AI Tools for [Task] style: https://caracal.news/ai-tools/task/best-ai-tools-for-creating-short-videos/ Best [Alternatives] to [Tool] style (alternative to n8n): https://caracal.news/ai-tools/alternatives/best-alternatives-to-nn/ Daily News style: https://caracal.news/news/your-ai-selfies-unexpected-afterlife/ https://preview.redd.it/l7ccy33c7k1f1.png?width=1954&format=png&auto=webp&s=53eea40ab8a27571f12504eb5d40298cf9a640be https://preview.redd.it/8b3soupb9k1f1.png?width=568&format=png&auto=webp&s=fa56d1c8f4b2899a09e9f105c1ef65a94aefa625"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I used n8n to write a review of n8n and create a full site (coding content)",
          "link": "https://old.reddit.com/r/n8n/comments/1juaud2/automated_website_via_reddit_posts/",
          "author": "redwurm",
          "time": "2025-04-08T11:03:31+00:00",
          "body_text": "Wanted to share my first \"finished\" workflow, which has made me absolutely fall in love with n8n! TLDR: Workflow + Screenshots A Little Background & The Idea: About 12 years ago, I ran a medium-sized Marvel Cinematic Universe News Site (MCUExchange). It gained some traction, but eventually, the effort led me to sell it and focus on mobile/web development. Fast forward to today, I'm diving into n8n and Langchain, and recreating that site concept through automation seemed like the perfect project! Workflow Evolution: I initially jumped in using mostly LLMChains and Code nodes because that felt fastest to get something working. From there, I gradually worked backward, replacing the code with built-in or community n8n nodes. The goal was to end up with a more transferable, low-code solution that others could potentially adapt just by changing a few initial parameters. So, What Does It Do? This workflow automates content creation by monitoring news articles posted to specific subreddits (like /r/marvelstudios , filtering by the \"Article\" flair). Here's the gist: It finds relevant Reddit posts linking to external articles. It extracts the article content from the linked page. It grabs and summarizes Reddit user sentiment from the comments on that post. It then uses AI agents (representing different writer styles) to generate a new article draft based on the extracted content and comment summary. The final article gets SEO tweaks (headline, basic formatting, meta description) before being posted to WordPress. To support this, I'm using Supabase in two ways: first, as a vector store to build a knowledge base as articles are published. This helps give the AI writers context beyond their training data cutoff. Second, it acts as a simple table to check for duplicate post IDs, ensuring we don't process the same article twice. (Side note: I tested this approach on /r/dc_cinematic filtering by the \"News\" flair, and it worked there too) . Discord nodes are also sprinkled throughout to send progress updates to a channel. Tech Choices: I've been mostly using DeepseekV3 via OpenRouter ‚Äì it's cost-effective and seems quite good at following instructions. For simpler tasks, Google's Gemini Flash 2.0 works well. I did include some auto-fixing parser nodes, which were helpful when testing smaller models like Mistral-Small (which did work, albeit slower, hinting that local execution on decent hardware is feasible). For reference, it cost less than a dollar via open router to write 100 articles. Usage & Results: To populate the site initially, I ran the workflow in batches (fetching 10 posts, then 20, etc., up to 100) to get articles going back a couple of months. Theoretically, you could go much further back to really build out content. For ongoing use, once I set up a production environment (likely Digital Ocean), I'll probably schedule it to run hourly, checking for maybe the latest 20 \"Article\" posts. Current Limitations & Caveats: Image Posts: Many subreddits have news shared as images (screenshots of tweets, etc.). The workflow currently filters these out, meaning it misses potentially important news. I experimented briefly with OCR but haven't fully cracked this yet. Other Flairs: Specifically on /r/marvelstudios , there's a \"Promotional\" flair for trailers, official merch, etc., which this workflow also misses. Finding source links for these might require a different approach (maybe vision models + search). Future Ideas / Todo: Featured Images: This is manual right now. Automating finding or generating relevant featured images (like actor headshots + character images) is tricky, especially ensuring quality and relevance. ComfyUI locally works okay, but integrating it smoothly or finding a reliable API-based solution needs more thought. WordPress Tags: The SEO node suggests keywords, but I haven't automated adding them as tags in WordPress yet. Ideally, it would check existing tags, use them, or create new ones. I wanted to manually tag the first batch anyway, but this is a clear next step for automation. \"Editor-in-Chief\" (Fact-Checking): My initial attempt at an EIC agent with web search tools often struggled with very new information or got stuck in loops. For now, giving the writers access to the continuously updated Supabase KB felt more reliable. I might revisit a separate EIC/review workflow later. Opinion Pieces: I plan to scrape Reddit comments from top posts (any flair) and have the LLM try to come up with opinion pieces (or unpopular opinion pieces!) based on what the community is talking about that week and post them gradually throughout the week. This would give the site both news and ‚Äúeditorials‚Äù. I‚Äôll likely just create a new workflow to manually run once a week as adding all of that logic to the current one would just look messy. Let me know what you think or any feedback you have. I'm still new to this and trying to learn as much as possible. Resources Supabase Setup Discord Setup ScrapeNinja Setup"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lw7gji/i_pay_you_build/",
          "author": "duuuuuuuudeSweeeet",
          "time": "2025-07-10T08:27:21+00:00",
          "body_text": "Not sure how everyone going about the payments I‚Äôve suggested escrow.com. Open to ideas though. Details below Work Brief ‚Äì Merged Outreach Workflow (Broken Links + Prospects) Last updated: 10 July 2025\nTimezone: Australia/Melbourne (UTC +10) ‚∏ª 0 ‚Äî What You Will Build Create one merged n8n workflow (Docker self-hosted) that:\n    1.  Discovers broken backlinks from sites I crawl with Screaming Frog.\n    2.  Processes hand-picked prospects that I type into a second sheet.\n    3.  Applies quality filters: Spam Score < 10 and Domain Rating ‚â• 30.\n    4.  Finds public email per domain with Hunter.io.\n    5.  Generates (or re-uses) a Google Doc outreach draft.\n    6.  Writes everything back to the correct Google Sheet.\n    7.  Logs errors cleanly.\n    8.  Ends with a live Zoom / Google Meet call where you prove the workflow works on my machine. ‚∏ª 1 ‚Äî Tools & Plans Tool    Purpose Plan Used   Notes\nn8n (Docker)    Low-code automation Free    Already running ‚Äì local host version 1.100.1 (let me know if this needs updating)\nScreaming Frog  Crawl sites ‚Üí export broken external links    Free    Manual crawl, 500-URL limit, no API\nGoogle Sheets   Input & output tracking Free    OAuth2\nGoogle Docs Outreach draft creation Free    OAuth2\nWayback Machine Archive snapshots   Free    No key\nSEO Review Tools    DR, Spam Score, Referring domains   Lite Plan   I‚Äôll supply API key\nHunter.io   Email discovery Free/Paid   I‚Äôll supply API key\nOpenAI / LLM    Draft article writing   Pay-as-you-go   Only if we automate writing ‚∏ª 2 ‚Äî Google Sheets Setup 2.1 Broken Backlinks Sheet ID: 1_PyF‚Ä¶KPlJ7I‚ÄÉName: Sheet1 A   B   C   D   E   F   G\nBroken URL  Title (H1)  Emails  Ref Domains Draft Link  Processed (TRUE/FALSE)  Error 2.2 Prospects Sheet ID: prospect-outreach-2025‚ÄÉName: Prospects A   B   C   D   E\nDomain  Page URL    Notes   Emails  Draft Link ‚∏ª 3 ‚Äî High-Level Flow (Single Workflow) Start\n   ‚îú‚îÄ Read Sheet1 (Broken URLs)  ‚ûú  Wayback ‚ü∂ H1 ‚ü∂ Dup-check ‚ü∂ (maybe) Doc\n   ‚îÇ                                 ‚îÇ\n   ‚îÇ                                 ‚îî‚îÄ Tag sheetOrigin=\"broken\"\n   ‚îî‚îÄ Read Sheet2 (Prospects)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                     ‚ñº\n        Shared Steps for EVERY domain\n        ‚îú‚îÄ DR + Spam filter  (SEO Review Tools)\n        ‚îú‚îÄ Hunter.io  (‚â§2 emails)\n        ‚îú‚îÄ If draftLink empty ‚Üí make Google Doc\n        ‚îî‚îÄ Update the correct sheet (using sheetOrigin flag) All downstream logic is reused; only the top two branches differ. ‚∏ª 4 ‚Äî Detailed Step-by-Step 4.1 Top Branch A ‚Äì Broken Backlink Intake\n    1.  CSV Import sub-flow\n    ‚Ä¢ Watches a folder (or manual trigger) for Screaming Frog CSV export.\n    ‚Ä¢ Appends each external 404 URL into Sheet1 A (skip duplicates).\n    2.  Read Sheet1 ‚Äì rows where Processed is blank.\n    3.  Wayback Machine ‚Üí get snapshot. Log error if none.\n    4.  Download HTML ‚Üí extract first <h1>.\n    5.  Duplicate test vs column B.\n    ‚Ä¢ If new H1 ‚Üí generate Google Doc article using my prompt; store link in column E.\n    ‚Ä¢ If existing H1 ‚Üí skip Doc creation; keep existing link for reuse.\n    6.  Lookup referring domains (SEO Review Tools bulk-backlinks endpoint).\n    7.  For each referring domain emit an item with:\n    ‚Ä¢ domain\n    ‚Ä¢ draftLink (new or existing)\n    ‚Ä¢ sheetOrigin = \"broken\"\n    ‚Ä¢ plus row index so we can write back later. ‚∏ª 4.2 Top Branch B ‚Äì Manual Prospect Intake\n    1.  Read Prospects sheet ‚Äì rows missing Emails and Draft Link.\n    2.  Emit item with:\n    ‚Ä¢ domain (column A)\n    ‚Ä¢ draftLink = \"\" (blank; will create later)\n    ‚Ä¢ sheetOrigin = \"prospect\"\n    ‚Ä¢ row index. ‚∏ª 4.3 Shared Steps (Both Branches) Action  Notes 1   SEO Review Tools ‚Äì get DR + Spam Score for domain.    Skip item if DR < 30 or Spam Score ‚â• 10.\n2   Hunter.io ‚Äì /domain-search?domain={{domain}}&limit=2&api_key={{‚Ä¶}}  Capture up to 2 emails.\n3   If draftLink empty ‚Üí create Google Doc draft (template uses Notes or H1). Else reuse existing link.\n4   Update sheet based on sheetOrigin: ‚Ä¢ If \"broken\" ‚Üí Sheet1 columns C/D/E/F. \n    ‚Ä¢ If \"prospect\" ‚Üí Prospects columns D/E. 5   Error handling ‚Äì write message to Error column (Sheet1 G) and skip item if fatal. ‚∏ª 5 ‚Äî Filtering Rules (Universal)\n    ‚Ä¢ Domain Rating ‚â• 30\n    ‚Ä¢ Spam Score < 10\n    ‚Ä¢ Apply before Hunter.io & Doc steps. ‚∏ª 6 ‚Äî API Placeholders Service Placeholder\nSEO Review Tools    {{SEO_REVIEW_API_KEY}}\nHunter.io   {{HUNTER_API_KEY}}\nWayback Machine (no key) ‚∏ª 7 ‚Äî Testing Checklist\n    1.  CSV import: drop a small Screaming Frog export ‚Üí confirm rows land in Sheet1.\n    2.  Add one dummy prospect row.\n    3.  Run workflow manually.\n    4.  Verify for both paths:\n    ‚Ä¢ DR/Spam filter works.\n    ‚Ä¢ New vs existing draft logic.\n    ‚Ä¢ Emails written back.\n    ‚Ä¢ Errors logged.\n    5.  Live Zoom/Meet hand-over ‚Äì developer screenshares on my PC and reruns the test. ‚∏ª 8 ‚Äî Deliverables Item    Description\nmerged-outreach-v1.json Single n8n export containing the whole flow\nCSV-import sub-flow (if separate)   Either embedded or standalone export\nDocumentation   This brief (Markdown & PDF if I ask)\n20‚Äì30 min Zoom/Meet session to show it working and set up with me Live proof it works on my setup ‚∏ª PAYMENT & ESCROW (Escrow.com) To keep things safe and simple for both of us we‚Äôll run the whole job through Escrow.com with milestone releases. Why Escrow.com?\n‚Ä¢ Low fees: only ~3 % per transaction.\n‚Ä¢ Fiverr fees: they take 20 % from you and another 5 % from me.\n‚Ä¢ Please take a moment to research Escrow.com and read user reviews to get comfortable with the platform. How it works\n    1.  I create an Escrow.com transaction listing each milestone and its payout.\n    2.  You accept the terms.\n    3.  I deposit the full amount up-front‚ÄîEscrow.com holds the funds.\n    4.  You deliver the current milestone. I have 1 day to test and approve.\n    5.  Once approved, Escrow.com releases that milestone‚Äôs payment to you.\n    6.  Repeat until the final live Zoom hand-over, then the last payment is released.\n    7.  If anything goes sideways, Escrow.com‚Äôs dispute process steps in. Suggested Milestones Deliverable % of total  Target Day (adjustable) 1   Core merged workflow: read sheets, DR/Spam filter, sheet write-backs    25 %    Day 3\n2   Integration steps: Hunter.io email lookup, Google Doc creation, error handling & logging    25 %    Day 6\n3   Final testing & live demo: you run the full workflow on my own machine while I follow along and execute each step, then we wrap up on Zoom  50 %    Day 9 Note: Target days are fully adjustable based on the timeline you propose‚Äîjust let me know and we‚Äôll update the Escrow terms accordingly. ‚∏ª 9 ‚Äî Credentials I‚Äôll provide temp credentials for everything after we agree on quote. ‚∏ª 10 ‚Äî Screaming Frog Free-Plan Notes\n    ‚Ä¢ 500-URL crawl limit ‚Äì run multiple crawls if needed.\n    ‚Ä¢ No API/scheduler ‚Äì I start crawls manually, export CSV.\n    ‚Ä¢ Free plan is sufficient; no paid license needed. ‚∏ª THE PROMPT FOR THE DRAFT DOC THATS CREATED:::::: You are an expert in SEO content and writing content that creates both user engagement and conversion, and is clear to Google‚Äôs algorithm. The website is a service business specializing in Electrical services . You have years of experience in writing content that is semantically relevant to target keywords. Your content should be accessible, engaging, and reflect your personal experiences or research. Include anecdotes or stories you‚Äôve heard from related businesses. Your writing style should be casual and conversational, as if you‚Äôre explaining things to a friend. You don‚Äôt use cliches or exaggerated language. You are very familiar with how Google‚Äôs algorithm interprets written content and how it knows what keyword to rank specific content for. Please write content for a page for an Electrical company in Melbourne. The target article length is 1,500 words. The target keyword for this page is WHATEVER THE H1 OR TITLE IS Incorporate a compelling hook that uses personal anecdotes or stories to enhance emotional resonance with the reader. As you re-write the content, ensure it remains concise and directly addresses the reader‚Äôs questions. Use storytelling techniques and adhere to Google‚Äôs quality guidelines to keep the content engaging. Employ metaphors and vivid descriptions where appropriate to enrich the narrative, and ensure the tone aligns with our previous analysis of our customers‚Äô preferred writing style. Focus on providing genuinely helpful information to electricians and customers. Include relevant context and references to specific neighbourhoods or areas in Melbourne. Use headings and subheadings to break up the text. Mention any relevant events or news related to electricians. Link to reputable local sources for additional information. Share any personal anecdotes or stories you‚Äôve heard about electricians. Aim for a conversational tone that engages the reader while providing valuable insights about electricians. Don‚Äôt use formal language. Avoid fluff. Write at the 4th-grade level. Keep in mind natural language processing as you write and make sure the structure of the sentences is as simple as possible. Here is a list of NLP words to use in the content: Avoid these words: embark, look no further, navigating, picture this, top-notch, unleash, unlock, unveil, we‚Äôve got you covered, transition, transitioning, crucial, delve, daunting, deep dive, dive in, realm, ensure, in conclusion, in summary, optimal, assessing, firstly, strive, striving, furthermore, moreover, comprehensive, we know, we understand, testament, captivating, eager, refreshing, edge of my seat, breath of fresh air, breath of fresh, to consider, it is important to consider, there are a few considerations, it‚Äôs essential to, vital, it‚Äôs important to note, it should be noted, to sum up, secondly, lastly, in terms of, with regard to, it‚Äôs worth mentioning, it‚Äôs interesting to note, significantly, notably, essentially, as such, therefore, thus, interestingly, in essence, noteworthy, bear in mind, it‚Äôs crucial to note, one might argue, it‚Äôs widely acknowledged, predominantly, from this perspective, in this context, this demonstrates, arguably, it‚Äôs common knowledge, undoubtedly, this raises the question, in a nutshell, unveiled. ‚∏ª End of Brief Copy everything above into the Fiverr order.\nLet‚Äôs book the verification Zoom once you say the workflow is ready to test."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated Website via Reddit Posts",
          "link": "https://old.reddit.com/r/n8n/comments/1l27yz1/introducing_my_ai_automation_workflow_with_n8n/",
          "author": "Connect_Cook_8034",
          "time": "2025-06-03T10:01:16+00:00",
          "body_text": "I have developed a powerful AI automation workflow using n8n that significantly enhances my agency's content generation and social media presence. This workflow automates the posting of content across LinkedIn, Reddit, and YouTube, saving me considerable time each week while maintaining professional standards. Here's a breakdown of the components and how it works. The workflow includes several key components:\n- Input: Video transcriptions are captured as the initial input, serving as the foundation for content creation.\n- n8n: The primary tool used is n8n, a no-code automation platform, which allows for seamless integration and workflow automation.\n- Content Generation: The automation features AI agents that assist in generating engaging content tailored for each platform‚ÄîLinkedIn, Reddit, and YouTube.\n- Posting Automation: Once the content is generated, the workflow automates the posting process on all three platforms, ensuring consistency and timeliness.\n- Engagement Measurement: Post automation, the workflow includes components for tracking engagement metrics, helping to optimize future content based on performance data. To rebuild this workflow:\n1. Set up n8n and create a new workflow.\n2. Utilize a video transcription service (API) to retrieve transcripts.\n3. Integrate AI content generation APIs tailored for LinkedIn, Reddit, and YouTube.\n4. Configure posting actions for each social media platform.\n5. Implement tracking nodes to monitor engagement on the posts. The output of the workflow is automated posts on LinkedIn, Reddit, and YouTube, complete with optimized content tailored to each platform. I have found this method not only streamlines my content production process but also enhances my agency's online presence and engagement rates. #n8n #agents #AI #automation"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I pay, you build",
          "link": "https://old.reddit.com/r/n8n/comments/1l6vum8/from_news_headline_to_instagram_post_in_2_minutes/",
          "author": "postigodiego",
          "time": "2025-06-09T04:31:21+00:00",
          "body_text": "Want to grow a news-based or content-driven Instagram account without spending hours writing, designing, or posting? I built a workflow that does it all in about 2 minutes. üßµ Use cases (with a few modifications, you can accomplish these) Run a daily news recap account that posts automatically and grows your audience Automate company social media posts across Instagram, LinkedIn, Bluesky, X Provide content as a service for clients with fast, consistent branding Turn educational or trending content into visuals for classrooms or creators ü§ñ How It Works You enter a topic via a form The workflow researches it using Perplexity (Sonar) It generates engaging post copy with an LLM Via the Canva API , it injects the content into your branded templates It posts to Instagram using the Facebook Graph API ‚úÖ How to Set It Up Download the source: https://pastebin.com/TKw3e5dV Import it into n8n Set up API access for: Facebook Graph API (Instagram) Canva (OAuth2) OpenRouter (Sonar + Gemini) Create Canva templates with these fields: page_number , title , body ‚ö†Ô∏è Heads-Up Setting up the Facebook Graph API, Canva OAuth, and OpenRouter keys can be tricky and time-consuming . Plus, customizing it to your exact use case might take hours. The good news? I've already set this up for over 10 clients , and I can help you get it running fast or customize it to your needs. üì© Contact me: [ n8n@sdomenech.me ]()"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring or Looking for Cofounder",
          "link": "https://old.reddit.com/r/n8n/comments/1kzaysv/i_built_a_workflow_to_scrape_virtually_any_news/",
          "author": "dudeson55",
          "time": "2025-05-30T18:00:38+00:00",
          "body_text": "I run a daily AI Newsletter called The Recap and a huge chunk of work we do each day is scraping the web for interesting news stories happening in the AI space. In order to avoid spending hours scrolling, we decided to automate this process by building this scraping pipeline that can hook into Google News feeds, blog pages from AI companies, and almost any other \"feed\" you can find on the internet. Once we have the scraping results saved for the day, we load the markdown for each story into another automation that prompts against this data and helps us pick out the best stories for the day. Here's how it works 1. Trigger / Inputs The workflow is build with multiple scheduled triggers that run on varying intervals depending on the news source. For instance, we may only want to check  feed for Open AI's research blog every few hours while we want to trigger our check more frequently for the 2. Sourcing Data For every news source we want to integrate with, we setup a new feed for that source inside rss.app. Their platform makes it super easy to plug in a url like the blog page of a company's website or give it a url that has articles filtered on Google News. Once we have each of those sources configured in rss.app, we connect it to our scheduled trigger and make a simple HTTP request to the url rss.app gives us to get a list of news story urls back. 3. Scraping Data For each url that is passed in from the rss.app feed, we then make an API request to the the Firecrawl /scrape endpoint to get back the content of the news article formatted completely in markdown. Firecrawl's API allows you to specify a paramter called onlyMainContent but we found this didn't work great in our testing. We'd often get junk back in the final markdown like copy from the sidebar or extra call to action copy in the final result. In order to get around this, we opted to actually to use their LLM extract feature and passed in our own prompt to get the main content markdown we needed (prompt is included in the n8n workflow download). 4. Persisting Scraped Data Once the API request to Firecrawl is finished, we simply write that output to a .md file and push it into the Google Drive folder we have configured. Extending this workflow With this workflow + rss.app approach to sourcing news data, you can hook-in as many data feeds as you would like and run it through a central scraping node. I also think for production use-cases it would be a good idea to set a unique identifier on each news article scraped from the web so you can first check if it was already saved to Google Drive. If you have any overlap in news stories from your feed(s), you are going to end up getting re-scraping the same articles over and over. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=2uwV4aUyGIg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Introducing My AI Automation Workflow with n8n for Social Media Posting!",
          "link": "https://old.reddit.com/r/n8n/comments/1jv9ikl/i_built_an_aipowered_social_media_machine_that/",
          "author": "hashpanak",
          "time": "2025-04-09T16:07:19+00:00",
          "body_text": "I automated social posting to 6 different platforms. Why? I wanted to build authority on my socials in the AI space, and I was putting it off for a while simply because I haven't got the time or the energy to do social media. But now I have leverage. and don't have to sign up for expensive tools. Use it for: - inbound lead-gen - growing authority on socials - set up for your clients I explain the setup here -> https://youtu.be/D1iKOayS270 I automated everything ‚Äîfrom research to posting‚Äîusing n8n ,  here's the blueprint, also see the images. This system generates viral-ready content (of course you can customise the style in the prompt), lets you approve it from your phone, and posts across LinkedIn, Instagram, X, Threads, YouTube, and more. How It Works This workflow turns raw topics into polished posts in 4 stages: AI-Powered Research & Content Creation Topic Deep Dive : Input a keyword (e.g., ‚ÄúSEO‚Äù), and AI scours Google, extracts trending subtopics, and builds a research doc. (goes through people also search for and sub topics) Multi-Platform Content : Generates LinkedIn posts, Twitter threads, Instagram reels scripts, and YouTube shorts scripts‚Äîall hyper-tailored to each platform‚Äôs style. Give examples to train in your style in the Agent prompts) Dynamic Scripts depending on the research found : For reels/shorts, get ready-to-record scripts like: ‚ÄúI wish someone told me this SEO secret sooner. Google doesn‚Äôt care about your website. Here‚Äôs why‚Ä¶‚Äù On-the-Go Review via Telegram Review and approve from your phone : Get notified on Telegram to review drafts. Approve, request edits, or upload video links directly from your phone ( So you can be productive even if you are out and about ) . Automated Publishing & Scheduling Independent Schedules in one System : Set different schedules for different platforms, add more platforms because of the modular nature of the workflow.  Go live at optimal times (e.g., LinkedIn every Tuesday at 2 PM). Reels and Shorts (TikTok upload is manual) : Upload Instagram reels/YouTube shorts by pasting a Google Drive link‚ÄîThe agent downloads and posts the video to shorts and reels, TikTok is not yet done, (if you know how to do it, let me know, there does not seem to be an API) Cross-Platform Sync One System, All Apps : Posts auto-sync to LinkedIn, X, Facebook, Threads, YouTube, and Instagram. Key Features - No-Code Setup : Built entirely in n8n ‚Äîno coding skills needed. - Free Tools : Uses Serper (2.5k free searches), and platform APIs. - Video Ready : Downloads filmed videos from Google Drive and posts as Instagram reels, and Youtube shorts (This filming bit is manual  but the agent will send you the script to your phone, and if it is easy enough you can record it then - currently looking at hooking in my Faceless video automation into this as well - but that is work in progress) - Customizable : Tweak prompts, schedules, or platforms in minutes. Why This Works Save 10+ Hours/Week : Saves a ton of time in research, and coming up with content ideas, just tweak or it will give you a lot of drafts, a single topic can produce up to 200 + pieces of content. Scale Your Brand : Post daily on 6+ platforms without lifting a finger. PS : This setup replaces tools like Hootsuite ($50+/mo) or Later ($30+/mo) with a free AI-powered machine. üî• Turn your social media into a 24/7 inbound lead-gen engine. üî• I explain the setup here -> https://youtu.be/D1iKOayS270 publishing schedule The whole system for social media review system short form video review system topic research sample content generated for linked in. Of course the tone and everything else is customisable. Topic cold email outreach X/Twitter and Threads"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lowna7/signed_up_48_hours_ago_already_built_3_workflows/",
          "author": "HeerShingala",
          "time": "2025-07-01T08:49:55+00:00",
          "body_text": "Hiiii I'm a full-time marketer, and I wanted to explore AI beyond using chatgpt or claude for work. Been playing around with n8n and I'm hooked!!! Flows I've built: - A Telegram bot that lets me post tweets via chat, then sends back a confirmation when it‚Äôs live. ‚Üí Nodes: Webhook, Twitter - A lead onboarding & nurture workflow: Collects form responses, drafts personalized emails using chatgpt, tags priority, sends out the emails, updates on Slack, updates google sheet CRM. Inspired by a Nate Herk tutorial on YouTube! ‚Üí Nodes: Google Sheets, Gmail, OpenAI, Slack - For my partner: a workflow that monitors multiple RSS feeds, filters content by date & keywords, and automatically populates a Google Sheet with relevant articles - saves hours of manual content curation. ‚Üí Nodes: Google Sheets, RSS Feeds, Merge & Filter I am on the 7th sky! It is so satisfying to debug and finally watch things work. Can't wait to start offering this to clients & making money, although I have no idea how to get started."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From news headline to Instagram post in 2 minutes, all automated! (CODE INCLUDED)",
          "link": "https://old.reddit.com/r/n8n/comments/1l9pff8/i_built_an_ai_system_that_scrapes_stories_off_the/",
          "author": "dudeson55",
          "time": "2025-06-12T15:31:02+00:00",
          "body_text": "So I built an AI newsletter that isn‚Äôt written by me ‚Äî it‚Äôs completely written by an n8n workflow that I built. Each day, the system scrapes close to 100 AI news stories off the internet ‚Üí saves the stories in a data lake as markdown file ‚Üí and then runs those through this n8n workflow to generate a final newsletter that gets sent out to the subscribers. I‚Äôve been iterating on the main prompts used in this workflow over the past 5 months and have got it to the point where it is handling 95% of the process for writing each edition of the newsletter. It currently automatically handles: Scraping news stories sourced all over the internet from Twitter / Reddit / HackerNews / AI Blogs / Google News Feeds Loading all of those stories up and having an \"AI Editor\" pick the top 3-4 we want to feature in the newsletter Taking the source material and actually writing each core newsletter segment Writing all of the supplementary sections like the intro + a \"Shortlist\" section that includes other AI story links Formatting all of that output as markdown so it is easy to copy into Beehiiv and schedule with a few clicks What started as an interesting pet project AI newsletter now has several thousand subscribers and has an open rate above 20% Data Ingestion Workflow Breakdown This is the foundation of the newsletter system as I wanted complete control of where the stories are getting sourced from and need the content of each story in an easy to consume format like markdown so I can easily prompt against it. I wrote a bit more about this automation on this reddit post but will cover the key parts again here: The approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each one: This is done by calling into a scrape_url sub-workflow that I built out. This uses the Firecrawl API /scrape endpoint to scrape the contents of the news story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. Newsletter Generator Workflow Breakdown This workflow is the big one that actually loads up all scraped news content, picks the top stories, and writes the full newsletter. 1. Trigger / Inputs I use an n8n form trigger that simply let‚Äôs me pick the date I want to generate the newsletter for I can optionally pass in the previous day‚Äôs newsletter text content which gets loaded into the prompts I build to write the story so I can avoid duplicated stories on back to back days. 2. Loading Scraped News Stories from the Data Lake Once the workflow is started, the first two sections are going to load up all of the news stories that were scraped over the course of the day. I do this by: Running a simple search operation on our S3 bucket prefixed by the date like: 2025-06-10/ (gives me all stories scraped on June 10th) Filtering these results to only give me back the markdown files that end in an .md extension (needed because I am also scraping and saving the raw HTML as well) Finally read each of these files and load the text content of each file and format it nicely so I can include that text in each prompt to later generate the newsletter. 3. AI Editor Prompt With all of that text content in hand, I move on to the AI Editor section of the automation responsible for picking out the top 3-4 stories for the day relevant to the audience. This prompt is very specific to what I‚Äôm going for with this specific content, so if you want to build something similar you should expect a lot of trial and error to get this to do what you want to. It's pretty beefy. Once the top stories are selected, that selection is shared in a slack channel using a \"Human in the loop\" approach where it will wait for me to approve the selected stories or provide feedback. For example, I may disagree with the top selected story on that day and I can type out in plain english to \"Look for another story in the top spot, I don't like it for XYZ reason\". The workflow will either look for my approval or take my feedback into consideration and try selecting the top stories again before continuing on. 4. Subject Line Prompt Once the top stories are approved, the automation moves on to a very similar step for writing the subject line. It will give me its top selected option and 3-5 alternatives for me to review. Once again this get's shared to slack, and I can approve the selected subject line or tell it to use a different one in plain english. 5. Write ‚ÄúCore‚Äù Newsletter Segments Next up, I move on to the part of the automation that is responsible for writing the \"core\" content of the newsletter. There's quite a bit going on here: The action inside this section of the workflow is to split out each of the stop news stories from before and start looping over them. This allows me to write each section one by one instead of needing a prompt to one-shot the entire thing. In my testing, I found this to follow my instructions / constraints in the prompt much better. For each top story selected, I have a list of \"content identifiers\" attached to it which corresponds to a file stored in the S3 bucket. Before I start writing, I go back to our S3 bucket and download each of these markdown files so the system is only looking at and passing in the relevant context when it comes time to prompt. The number of tokens used on the API calls to LLMs get very big when passing in all news stories to a prompt so this should be as focused as possible. With all of this context in hand, I then make the LLM call and run a mega-prompt that is setup to generate a single core newsletter section. The core newsletter sections follow a very structured format so this was relatively easier to prompt against (compared to picking out the top stories). If that is not the case for you, you may need to get a bit creative to vary the structure / final output. This process repeats until I have a newsletter section written out for each of the top selected stories for the day. You may have also noticed there is a branch here that goes off and will conditionally try to scrape more URLs. We do this to try and scrape more ‚Äúprimary source‚Äù materials from any news story we have loaded into context. Say Open AI releases a new model and the story we scraped was from Tech Crunch. It‚Äôs unlikely that tech crunch is going to give me all details necessary to really write something really good about the new model so I look to see if there‚Äôs a url/link included on the scraped page back to the Open AI blog or some other announcement post. In short, I just want to get as many primary sources as possible here and build up better context for the main prompt that writes the newsletter section. 6. Final Touches (Final Nodes / Sections) I have a prompt to generate an intro section for the newsletter based off all of the previously generated content I then have a prompt to generate a newsletter section called \"The Shortlist\" which creates a list of other AI stories that were interesting but didn't quite make the cut for top selected stories Lastly, I take the output from all previous node, format it as markdown, and then post it into an internal slack channel so I can copy this final output and paste it into the Beehiiv editor and schedule to send for the next morning. Workflow Link + Other Resources Github workflow links: AI News Story / Data Ingestion Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json Firecrawl Scrape Url Sub-Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json AI Newsletter Generator Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Nv5_LU0q1IY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m5qnxn/i_built_a_content_generator_ai_workflow_using_n8n/",
          "author": "Altruistic-Brother37",
          "time": "2025-07-21T18:22:06+00:00",
          "body_text": "I've created and given up on multiple website and revenue potentials. Creating a site on WordPress is easy and fun, but writing the content is boring and tiresome. Since I don‚Äôt have a team of writers or editors to help churn out daily blog posts, I built an n8n automation system that posts 10 content a day. It scrapes fresh topics, generates long form blog posts from RSS feeds, repurpose them using AI, and publishes directly to WordPress. Here‚Äôs how the system works: 1. Daily Trigger & Topic Ideation The workflow starts with a daily schedule trigger. It pulls fresh news content from multiple sources (mostly big tech news sites) and uses embeddings to group them into potential topics. For example, if multiple articles are trending around ‚ÄúAI music tools,‚Äù the workflow merges those into one blog worthy topic. 2. AI Powered Blog Generation Once the topic is selected, the workflow sends a custom GPT prompt to generate a 1000 - 1500 word blog post. It uses summary + original content merging, plus markdown formatting for readability. You can configure tone, length, call to actions, or SEO keywords depending on your niche. 3. WordPress Publishing The final content is cleaned, formatted, and then sent to WordPress via the API. It handles: Categories and tags Featured image (auto generated using Leonardo AI) Author and post status Slug and permalink structure This means new blog posts go live without you touching a thing. Costs The workflow runs entirely on free or open source tools. . Your only costs would be Open AI API calls + your WordPress server. Component Daily Usage Monthly Cost Estimate OpenAI 10 posts per day ~$6 Leonardo AI 10 images per day (15 credits each) ~$14 (9 base + 5 top-up) MongoDB Free up to 512 MB $0 WordPress Hosting and domain ~$1.25 Total ~$21/month Alternatively, you could use open / cheaper models via local API or and image generation via other models on Hugging Face Limitations I noticed that sometimes the word count is below 1,500. Usually around 700 to 800 words. Its not bad, but not ideal I've received a few feedback from people that sometimes the source RSS websites change the slug URL of their site and it causes errors Further Improvement (I'm working on) I want this flow to be able to generate images and tables within the blog Will be cool to repurpose the content for other sites like Twritter and Linkedin I think the current setup is too linear, passing information from one AI agent to another. Perhaps, a flow where there are 3 agents in a loop. The task master who creates the task, sections and reading journey The content writer who writes the content, drafts the tables, generates the images The Quality Checker who validates the SEO score and overall consistency. Provides feedback and passes the article if it meets certain quality thresholds Currently, it only works for one category per workflow. Right now, i hard coded AI, but you can change it to whatever you want. I notice that most blogs use 3 to 5 content pillars. Will add more topics so readers dont get bored GitHub repo with the full n8n workflow code: https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress Hopefully its useful and inspire you to light up that WP blog u abandoned."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a workflow to scrape (virtually) any news content into LLM-ready markdown (firecrawl + rss.app)",
          "link": "https://old.reddit.com/r/n8n/comments/1h09744/automated_shorts_and_reels_creation_with_n8n_full/",
          "author": "gochapachi1",
          "time": "2024-11-26T10:57:05+00:00",
          "body_text": "Hey everyone! First off, I want to sincerely apologize for the delay in sharing the video and the workflow I promised. Life got a little hectic, but I‚Äôm finally here with the step-by-step guide to automating your shorts and reels creation using n8n. In this video, I walk you through: ‚úÖ Setting up the workflow on n8n (no coding needed!) ‚úÖ Automating video templates and captions ‚úÖ Exporting and scheduling reels for your social media You can watch the full guide here: Youtube Link This workflow is 100% free and perfect for content creators, startups, or anyone looking to save time while growing their social media presence. Thanks for your patience, and I hope you find this helpful! Let me know in the comments if you have any questions, or feel free to share your thoughts on how you plan to use this workflow. Looking forward to hearing from you all! üôå Cheers,"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lfqrco/n8n_content_creation_automation_website_scroll/",
          "author": "canhelp",
          "time": "2025-06-20T00:40:34+00:00",
          "body_text": "Just wanted to share one of the app I built where Claude Code cooked an app end to end. All of this started with me watching Andrej karpathy video from YC. I was always looking for an app that can help me create content for tiktok,instagram,X where we have a automation that scrolls the website and stop at place on the website that are important and have a voice over. I wasn't sure if Claude code would be able to do it but for my luck it actually built the whole app. I started with something very rudimentary that : \"Just build me an app that will take a url and a time in seconds and it would automatically scroll the video from top to bottom over the period of time\". This I was confident it would do . Next to see how much I can push I asked it to take a screenshot identify the key points on the website using Gemini Vision api and then scroll to that section wait for a few seconds before it goes to the next section. Holy crap it actually built the working prototype. See this end to end flow get built in a day is quite crazy to think of. Also it helped me to create videos with different aspect ratio ü§Ø Next I want to built this whole flow very there is even a voice over when it stop to tell about what is happening at that frame. I know if I share a link my post will get deleted. So if you want to play around with this app. Please reply in the comment or dm me. For folks who generate content for tiktok / instagram/ short would something like this be useful if I exposed it as an api? Here is the workflow I am thinking User Enter : A website or saas The n8n : Scrapes that generated a script The API : Generate the background url scrolling through the website and pausing when anything important is detected Voice over when the scrolling pauses https://reddit.com/link/1lfqrco/video/03r880maaz7f1/player https://reddit.com/link/1lfqrco/video/je1yyzlaaz7f1/player Post -> Twitter/Instagram/Tiktok?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Social Media Machine That Writes, Reviews, & Posts Content 24/7 ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1i5mp/i_made_a_crawlee_server_built_specifically_for/",
          "author": "conor_is_my_name",
          "time": "2025-06-02T13:33:25+00:00",
          "body_text": "Hello Everyone! Today I'm sharing my latest n8n tool - a very performant dockerized version of the crawlee web scraping package. https://github.com/conor-is-my-name/crawlee-server Who is this for: Want to scrape every page on a website customize the fields & objects that you scrape you already have a database setup - default is postgres Scaled scraping - can run multiple containers for parallelism Who this is not for: you don't have a database - the scraper is too fast to return results to google sheets or n8n I've used this to scrape millions of web pages, and this setup is the baseline that I use for my competitor analysis and content generation work.  This template is all you need to get good at web scraping.  If you can learn how to modify the selectors in the code of this package, you can scrape 99% of websites. Simply run this docker container & update the IP address and Port number in the workflow - example n8n http node is already included. http://100.XX.XX.XX:####/start-crawl?url=https://paulgraham.com&maxResults=10 Parameters to pass from n8n: url & max results (don't pass max results if you want full site scraped) The baseline code that I'm sharing is configured as a generic web scraper most suitable for blogs and news articles.  You can modify what you want returned in the results.js file. sitehomepage, article_url, title, bodyText, datePublished, \narticlecategories, tags, keywords, author, featuredImage, comments I have also included an example for scraping a e-commerce site that runs on Woo Commerce in the n8n-nodes folder.  You can use that as a template to adjust to just about any site by changing the selectors used in the routes.js file. If you don't know how to do this, I highly recommend using Roo Code in VS Code.  It's as simple as copying the HTML from the page and asking Roo Code to pick the specific selectors you want. It will make the adjustments in the routes.js file for you.  But note that you will have to make sure your database also has all of the matching fields you want scraped. Example SQL is also included for initial database setup.  I recommend using this in conjunction with my n8n-autoscaling build which already comes with postgres installed. Instructions: Clone the repository Update passwords in the .env file to match your setup docker compose up -d update the IP address and port number in the n8n workflow to match the running containers Optional: The docker compose file has a Deploy section that comes commented out by default.  If you want to run multiple instances of this container you can make your adjustments here. You can modify scraper concurrency in the .env file.  I'd advise you to stay in the 3-5 range unless you know the site doesn't have rate limiting. As always, be sure to check out my other n8n specific GitHub repositories: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM I do expert n8n consulting, send me a message if you need help on a project."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Signed up 48 hours ago, already built 3 workflows. First-timer, no coding experience. n8n is AWESOME!!",
          "link": "https://old.reddit.com/r/n8n/comments/1lm8kv1/my_first_n8n_content_creation_automation/",
          "author": "yoav0307",
          "time": "2025-06-27T23:38:04+00:00",
          "body_text": "Hi guys, just built my first content creation automation using n8n. The idea is simple enough but I did it all by myself (with some help from chatGPT) It‚Äôs pretty straightforward but the special spice is a Supabase table with all my previous LinkedIn posts and a RAG that retrieves that last 3 to write like me. I wanted to also add the option to create drafts on LinkedIn using an http request node but wasn‚Äôt able to yet. What do you think? What else can I improve? the workflow is available here: https://limewire.com/d/VFj7C#cN2y2UGVPD"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jqdzs6/i_made_a_reusable_research_workflow_for_content/",
          "author": "davidgyori",
          "time": "2025-04-03T09:31:49+00:00",
          "body_text": "https://preview.redd.it/ccppg57je19f1.png?width=1284&format=png&auto=webp&s=064050247aa2427a04d2a699d5048104a51fc8f4 Got tired of manually creating content for LinkedIn, X, and Facebook, so I built this n8n workflow that finds trending topics and auto-posts AI-generated content. It's been running twice daily for weeks and engagement is actually better than my manual posts. How the magic works: Finds trending topics using Google Trends API (searches past 3 days) AI picks the best topic based on relevance + search volume growth Perplexity researches the chosen topic with current data GPT-4 creates platform-specific content (LinkedIn formatting, character limits, etc.) Posts simultaneously to X, LinkedIn, and Facebook Logs everything to Google Sheets for tracking The content quality is surprisingly good: Uses trending keywords for better reach Humanized writing (removes AI-isms and citations) Platform-specific formatting (LinkedIn gets professional tone, X gets punchy) Includes relevant hashtags and CTAs Posts twice daily at optimal times (6am & 6pm) Tech stack: n8n for workflow orchestration Google Trends via SerpAPI for topic discovery Perplexity AI for research and current data OpenAI GPT-4 for content generation Social platform APIs for posting Google Sheets for content tracking The workflow runs completely hands-off. I just check the analytics weekly to see what's performing best. Way more consistent than trying to come up with content ideas manually."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI system that scrapes stories off the internet and generates a daily newsletter (now at 10,000 subscribers)",
          "link": "https://old.reddit.com/r/n8n/comments/1lk0jp2/n8n_ai_content_factory_that_researches_and_post/",
          "author": "zeolite",
          "time": "2025-06-25T08:51:19+00:00",
          "body_text": "I‚Äôve always believed the best automations have a little personality. In our team, we‚Äôre not just cranking out scheduled posts; we‚Äôre using AI in n8n to turn daily inspirations into timely content, repurposing conversations into fresh social snippets, and letting AI suggest creative twists on trending topics. It‚Äôs made our workflow genuinely more fun! Are you also blending AI with n8n in a way that makes your content feel more relatable? Or Have you ever had a moment where an AI-generated post surprised you, made you smile, or felt genuinely human ?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m67jb2/how_you_all_are_using_ai_in_n8n_to_power_your/",
          "author": "Wash-Fair",
          "time": "2025-07-22T07:18:54+00:00",
          "body_text": "So yesterday I did a post on an upwork job scrapper, tagged it under the 'no code included' flair then a looser, people seemed to like it others commented 'Finally something that helps', people came to my DM and right now I have several people am working with helping them, and I wouldn't' call them clients because they are paying nothing. But among the many positive comments a single person has the altitude to say the negative and that gets the Mods to remove the post, after I had already done the follow up post explaining the post. And it seems Mods are robots or I don't know cause its like they also couldn't read the comments, see the interaction between other people and me they just got directed by a silly guy who I tried to check his profile and its like his work is to post negative comments on thriving posts, an enemy of progress. So my question is, what the hell is the use of 'No Code included' Flair?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a Content Generator AI workflow using n8n that takes daily news update from Big News sites and repurposes them for WordPress Blog Articles",
          "link": "https://old.reddit.com/r/n8n/comments/1llld36/what_is_the_use_of_the_flair_no_code_included_am/",
          "author": "DebougerSam",
          "time": "2025-06-27T04:54:56+00:00",
          "body_text": "I created an automated workflow integrated with Notion that transforms recorded meetings (via tl;dv) into content ready to be published on social media. The system works as follows: üé• 1. Automated Recording and Summarization All meetings are recorded and automatically summarized into a Notion database, organized by client and project. ü§ñ 2. Idea Generation with AI Agents For each new recorded meeting, three content agents (bots) are activated: LinkedIn ‚Äì Focused on authority and B2B relationship building. Instagram ‚Äì Focused on visual identity and emotional connection. Reels/Shorts ‚Äì Generates video cuts and highlights with precise suggestions for reels. Each agent uses custom prompts to generate up to 5 post ideas with captions , totaling up to 15 suggestions per meeting . üñºÔ∏è 3. AI Image Generation The validated ideas are turned into realistic images using AI. On average, 4 to 6 iterations per image are needed to reach an acceptable visual result. üìÜ 4. Planning and Scheduling via Notion Once validated, the posts are scheduled directly in Notion, with date, channel, and content type. üöÄ 5. Automatic Publishing Every day at 10 AM , an automation scans the database and publishes the scheduled content to the specified social media platforms. https://preview.redd.it/jnsl3surjvaf1.png?width=1264&format=png&auto=webp&s=72a5de46014c6e72aa0f109be99ddcf22ba74ab5 https://preview.redd.it/9ctiz3tsjvaf1.png?width=661&format=png&auto=webp&s=3facd249a50b35eda1e7b54b59f906f5b1b93ef7"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lrlbdi/i_automated_my_content_creation_from_client/",
          "author": "piothur",
          "time": "2025-07-04T15:19:15+00:00",
          "body_text": "Hey everyone! üëã\n\nI've been working on a FREE project that solves a common challenge many of us face with n8n: tracking long-running and asynchronous tasks. I'm excited to share the n8n Task Manager - a complete orchestration solution built entirely with n8n workflows!\n\nüéØ What Problem Does It Solve?\n\nIf you've ever needed to:\n- Track ML model training jobs that take hours\n- Monitor video rendering or time consuming processing tasks\n- Manage API calls to services that work asynchronously (Kling, ElevenLabs, etc.)\n- Keep tabs on data pipeline executions\n- Handle webhook callbacks from external services\n\nThen this Task Manager is for you!\n\nüöÄ Key Features:\n\n- 100% n8n workflows - No external code needed\n- Automatic polling - Checks task status every 2 minutes\n- Real-time monitoring - React frontend with live updates\n- Database backed - Uses Supabase (free tier works!)\n- Slack alerts - Get notified when tasks fail\n- API endpoints - Create, update, and query tasks via webhooks\n- Batch processing - Handles multiple tasks efficiently\n\nüì¶ What You Get:\n\n1. 4 Core n8n Workflows:\n¬† ¬†- Task Creation (POST webhook)\n¬† ¬†- Task Monitor (Scheduled polling)\n¬† ¬†- Status Query (GET endpoint)\n¬† ¬†- Task Update (Callback handler)\n\n2. React Monitoring Dashboard:\n¬† ¬†- Real-time task status\n¬† ¬†- Media preview (images, videos, audio)\n¬† ¬†- Running time tracking\n\n3. 5 Demo Workflows - Complete AI creative automation:\n¬† ¬†- OpenAI image generation\n¬† ¬†- Kling video animation\n¬† ¬†- ElevenLabs text-to-speech\n¬† ¬†- FAL Tavus lipsync\n¬† ¬†- Full orchestration example\n\nüõ†Ô∏è How to Get Started:\n\n1. Clone the repo: https://github.com/lvalics/Task_Manager_N8N\n2. Set up Supabase (5 minutes, free account)\n3. Import n8n workflows (drag & drop JSON files)\n4. Configure credentials (Supabase connection)\n5. Start tracking tasks!\n\nüí° Real-World Use Cases:\n\n- AI Content Pipeline: Generate image ‚Üí animate ‚Üí add voice ‚Üí create lipsync\n- Data Processing: Track ETL jobs, report generation, batch processing\n- Media Processing: Monitor video encoding, image optimization, audio transcription\n- API Orchestration: Manage multi-step API workflows with different services\n\nüì∫ See It In Action:\n\nI've created a full tutorial video showing the system in action: [\\[YouTube Link\\]](\nhttps://www.youtube.com/watch?v=PckWZW2fhwQ\n)\n\nü§ù Contributing:\n\nThis is open source! I'd love to see:\n- New task type implementations\n- Additional monitoring features\n- Integration examples\n- Bug reports and improvements\n\nGitHub: https://github.com/lvalics/Task_Manager_N8N\n\nüôè Feedback Welcome!\n\nI built this to solve my own problems with async task management, but I'm sure many of you have similar challenges. What features would you like to see? How are you currently handling long-running tasks in n8n?\n\nDrop a comment here or open an issue on GitHub. Let's make n8n task management better together!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated Shorts and Reels Creation with N8N ‚Äì Full Workflow & Video Guide!",
          "link": "https://old.reddit.com/r/n8n/comments/1l0k5dh/opensource_task_manager_for_n8n_track_longrunning/",
          "author": "Silly_Chapter6498",
          "time": "2025-06-01T08:38:51+00:00",
          "body_text": "I wanted to share a workflow I've been refining. I was tired of manually finding content for a niche site I'm running, so I built a bot with N8N to do it for me. It automatically fetches news articles on a specific topic and posts them to my Ghost blog. The end result is a site that stays fresh with relevant content on autopilot. Figured some of you might find this useful for your own projects. Here's the stack: Data Source: LumenFeed API (Full disclosure, this is my project. The free tier gives 10k requests/month which is plenty for this). Automation: N8N (self-hosted) De-duplication: Redis (to make sure I don't post the same article twice) CMS: Ghost (but works with WordPress or any CMS with an API) The Step-by-Step Workflow: Here‚Äôs the basic logic, node by node. (1) Setup the API Key: First, grab a free API key from LumenFeed. In N8N, create a new \"Header Auth\" credential. Name: X-API-Key Value: [Your_LumenFeed_API_Key] (2) HTTP Request Node (Get the News): This node calls the API. URL: https://client.postgoo.com/api/v1/articles Authentication: Use the Header Auth credential you just made. Query Parameters: This is where you define what you want. For example, to get 10 articles with \"crypto\" in the title: q:¬†crypto query_by:¬†title language:¬†en per_page:¬†10 (3) Code Node (Clean up the Data): The API returns articles in a¬†data¬†array. This simple JS snippet pulls that array out for easier handling. return $node[\"HTTP Request\"].json[\"data\"]; (4) Redis \"Get\" Node (Check for Duplicates): Before we do anything else, we check if we've seen this article's URL before. Operation: Get Key: {{ $json.source_link }} (5) IF Node (Is it a New Article?): This node checks the output of the Redis node. If the¬†value¬†is empty, it's a new article and we continue. If not, we stop. Condition: {{ $node[\"Redis\"].json.value }}¬†->¬†Is Empty (6) Publishing to Ghost/WordPress: If the article is new, we send it to our CMS. In your Ghost/WordPress node, you map the fields: Title: {{ $json.title }} Content: {{ $json.content_excerpt }} Featured Image: {{ $json.image_url }} (7) Redis \"Set\" Node (Save the New Article): This is the final step for each¬†new¬†article. We add its URL to Redis so it won't get processed again. Operation: Set Key: {{ $json.source_link }} Value: true That's the core of it! You just set the Schedule Trigger to run every few hours and you're good to go. Happy to answer any questions about the setup in the comments! For those who prefer video or a more detailed write-up with all the screenshots: Full Video Guide: https://youtu.be/BNkxt48plFg Detailed Blog Post: https://medium.com/@contact_34329/how-to-build-an-automated-news-aggregator-with-n8n-and-lumenfeed-api-step-by-step-guide-c63b61d4c0ce"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "N8n Content Creation Automation - Website scroll and record.",
          "link": "https://old.reddit.com/r/n8n/comments/1l9qdde/build_your_own_news_aggregator_with_this_simple/",
          "author": "Witty_Passage_3845",
          "time": "2025-06-12T16:08:42+00:00",
          "body_text": "I created a powerful AI automation workflow using n8n that generates social media content and maximizes efficiency in posting to platforms like LinkedIn, Reddit, and YouTube. Here‚Äôs a breakdown of how I built this optimized process: Introduction to the Workflow : I started with an overview of the automation capabilities of n8n to understand how it fits into AI content creation. Leveraging n8n for Content Generation : I utilized n8n‚Äôs no-code platform to streamline the process, allowing for easy integration with various applications. Airtable Integration : I demonstrated how Airtable serves as a central hub for managing content, establishing an organized method for generating and tracking posts. Quality Control : Ensuring quality content creation was integral, where I set up checks to maintain high standards before posting. YouTube Descriptions and Titles : I optimized the content generation for YouTube by crafting engaging descriptions and titles that attract viewer attention. Crafting LinkedIn Posts : I focused on creating professional and engaging LinkedIn posts tailored specifically to target audiences. Guidelines for Reddit Engagement : I outlined strategies for crafting posts that foster engagement within Reddit communities. Airtable Functions Utilization : I explored how different Airtable functions enhance content generation and organization. Triggering Workflows : Setting triggers in Airtable facilitated automatic workflow starts based on pre-defined criteria. YouTube Agent Prompt Details : I provided insights into detailed prompts configured for YouTube content creation. Finalizing Post Content : I went through the process of finalizing and reviewing content before it goes live. LinkedIn Specifics : Lastly, I shared strategies for optimizing posts specifically for LinkedIn. Reddit Strategies : I wrapped up with effective strategies for successful Reddit posts, ensuring engagement and visibility. This workflow combines n8n and Airtable, automating content creation across major platforms and saving time while increasing productivity. It‚Äôs ideal for content creators aiming to optimize their social media strategies and workflows! n8n #agents #AI #automation"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1kz3vpt/optimizing_social_media_content_creation_with_n8n/",
          "author": "Connect_Cook_8034",
          "time": "2025-05-30T13:12:38+00:00",
          "body_text": "Hey everyone! I'm looking to build (or find existing) automation workflows that can help me share insights about specific tech topics on Twitter/X and LinkedIn. What I'm trying to achieve: Target topics: DevOps, cloud technologies, software development best practices, etc. Automatically gather latest trends and industry insights Generate engaging content from this data Format as Twitter/X threads for that platform Create standard posts for LinkedIn Current challenges: Finding reliable sources for trend data Automating the content creation process Maintaining authentic, valuable posts (not just spam) Handling different format requirements per platform I'm familiar with tools like N8n and have some experience with social media APIs.¬†Has anyone built something similar? What approach would you recommend for: Data collection and trend analysis Content generation workflows Multi-platform posting automation Any existing workflows, tools, or architectural advice would be incredibly helpful. Thanks in advance!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I made a Crawlee Server built specifically for n8n workflows. Very fast web scraper used for deep crawls through every page on a website. I've used it to scrape millions of webpages. Full code included with link to GitHub & n8n workflow example included.",
          "link": "https://old.reddit.com/r/n8n/comments/1m1ltxy/help_wanted_automated_social_media_content/",
          "author": "MrJackz",
          "time": "2025-07-16T19:19:40+00:00",
          "body_text": "Tired of manually searching LinkedIn for company posts? I‚Äôve built an automated n8n workflow that scrapes all posts from any company using just its name! No login required! üîç What You‚Äôll Get: ‚úÖ Company Name ‚Äì Identify the brand behind the post ‚úÖ Post URL ‚Äì Direct link to each post ‚úÖ Comments & Likes ‚Äì Engagement insights ‚úÖ Post Caption ‚Äì Full content of the post ‚úÖ Published Date ‚Äì Know when it was posted SAMPLE SCRAPED DATA SAMPLE WEBSITE FOR THE SCRAPED DATA WEBSITE GLANCE üî• Test It for FREE! I‚Äôm giving out an API key with 100 free test runs so you can try it yourself. First come, first served! üñ•Ô∏è Live Demo & Portfolio: Try out the scraper right here üëâ n8nPro LIVE Testing üí° Want a FREE website for this Scraper to monetize it for your clients? I‚Äôll build it for FREE upon request! Just drop a message. ‚ö° CLICK HERE TO BUY THIS WORKFLOW NOW!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jd51b3/n8nworkflow_scrape_any_companys_linkedin_posts_in/",
          "author": "ProEditor69",
          "time": "2025-03-17T04:59:04+00:00",
          "body_text": null
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Keyword Research and Clustering Machine That Automates SEO Content Strategy ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1kahpiu/i_built_an_aipowered_keyword_research_and/",
          "author": "hashpanak",
          "time": "2025-04-29T06:55:11+00:00",
          "body_text": "I automated keyword research and clustering for SEO and content planning. Why? I was tired of spending hours manually digging through keywords and organizing them into clusters, only to end up with a chaotic mess. Now, I‚Äôve got a system that does it all‚Äîresearches keywords, builds clusters, and prioritizes opportunities‚Äîusing n8n and AI. It‚Äôs a game-changer for anyone looking to streamline their SEO strategy without breaking the bank. Use it for: Creating topic clusters and using it as a lead magnet for acquiring new clients. Using generated pillars and pages for social media content ideas from what people are actually searching for. Automating keyword research and clustering Saving time while uncovering high-impact topics I explain the setup here -> https://youtu.be/twFg_L9_07E I automated everything‚Äîfrom keyword discovery to clustering‚Äîusing n8n. Here‚Äôs the blueprint. This system pulls keyword data, organizes it into pillar pages and subpages, and even stores it neatly in Google Sheets, all while running on free or low-cost tools. How It Works This workflow turns a single seed keyword into a full SEO strategy in 4 stages: Automated Keyword Research (with human in the loop) Seed Keyword Start: Drop in a keyword (e.g., ‚Äún8n‚Äù or ‚ÄúSEO‚Äù), and the system uses the Google Ads API (or Data for SEO) to fetch related keywords, search volumes, competition, and trends. Replace it with other APIs if you so wish like \"DataForSEO\" Iterative Discovery: It keeps digging, researching related keywords to build a massive, relevant list‚Äîthink hundreds of terms in minutes. AI-Powered Clustering Smart Grouping: An AI model (like GPT-4.1 or Gemini) analyzes the keywords based on user intent (informational, transactional, navigational), search trends, and competition. Pillar & Subpage Structure: It creates thematic clusters‚Äîpillar pages for broad topics and subpages for specifics‚Äîthen ranks them by growth potential and ranking ease. Google Sheets Integration Central Hub: All keywords and clusters land in Google Sheets, ready for you to review or tweak. Manual Control: Spot a dud keyword? Delete it. Need to adjust a cluster? It‚Äôs all right there. n8n Automation No-Code Workflow: Built in n8n, this system runs on autopilot with scheduled triggers or manual kicks. Scalable Processing: Looping mechanisms handle huge keyword sets without breaking a sweat. Key Features No-Code Setup: Built entirely in n8n‚Äîno coding skills needed. Free/Low-Cost Tools: Uses Google Ads API (free tier) or Data for SEO ($1 signup credit and cheap alternative to Ahrefs or SEMRush). Scalable: Tackles thousands of keywords with batch processing. Customizable: Tweak AI prompts or schedules to match your needs. Why This Works Save 10+ Hours/Week: No more manual keyword grinding‚Äîautomation does the heavy lifting. Smarter SEO: AI prioritizes clusters with real data, targeting topics that rank and convert. Scale Effortlessly: Perfect for big content strategies without the overwhelm. Example: Feed it ‚Äún8n,‚Äù and it pulls keywords like ‚Äún8n automation,‚Äù ‚Äún8n workflows,‚Äù and ‚Äún8n integrations.‚Äù The AI then builds clusters like ‚Äún8n Automation Tutorials‚Äù (pillar) with subpages like ‚ÄúConnecting n8n to Salesforce‚Äù‚Äîcomplete with primary/secondary keywords and ranking priorities. PS: This replaces tools like Ahrefs ($99+/mo) or SEMrush ($129+/mo) with a free AI-powered system. üî• Turn your SEO strategy into a 24/7 content-planning engine. üî• I explain the setup here -> https://youtu.be/twFg_L9_07E https://preview.redd.it/bxjl5j430qxe1.png?width=1704&format=png&auto=webp&s=53e7bc67c1f76f46a3fbdc76b2acdef1bbc4ac51 https://preview.redd.it/xr50mgn40qxe1.png?width=2344&format=png&auto=webp&s=04768c89f7bc0947eac2fe1a1976c30634273a38 https://preview.redd.it/sobyfvw50qxe1.png?width=2716&format=png&auto=webp&s=bb7994483791ff33a21ec3e44aa766e2d6560388 https://preview.redd.it/r8zawm170qxe1.png?width=2886&format=png&auto=webp&s=a3f5803b70120d2c34de4d8547b264a2e84ca1ae https://reddit.com/link/1kahpiu/video/zz4xrkxg0qxe1/player"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lj6m11/3_steps_to_n8n_vps_hosting_configuration_code/",
          "author": "Tom-Miller",
          "time": "2025-06-24T09:30:37+00:00",
          "body_text": "I documented the exact 3-step setup , including: Docker Compose config Nginx reverse proxy (with a ready-to-use conf file) Free SSL via Certbot It‚Äôs beginner-friendly, no fluff, and should take under 30 minutes. If you‚Äôre looking to move off n8n.io or avoid execution/workflow limits, this might help."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "3 Steps to N8N VPS Hosting - Configuration Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jwnsfb/canva_content_creation_with_n8n/",
          "author": "deadadventure",
          "time": "2025-04-11T11:42:11+00:00",
          "body_text": "Hey everyone, loads were asking me how the content creation between canva and n8n was done, so here's my workflow prototype that I will be implementing over at gomars.co.uk , currently in beta but feel free to sign up. If you are a reseller, feel free to contact me and we can work together. Feel free to ask me questions about it!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1l0gm98/try_this_podcast_generation_workflow_i_built/",
          "author": "teezworkspace",
          "time": "2025-06-01T04:49:03+00:00",
          "body_text": "Hey everyone, I built out this workflow in n8n to help me intake the highest quality AI content in the most digestible format for myself; audio. In short, the RSS Feed scrapes three (could be more if you want) of the most reputable sources in the AI space, goes through a Code node for scoring (looks for the highest quality content: whitepapers, research papers, etc) and calls AutoContentAPI (NOT free, but a NotebookLM alternative nonetheless) via HTTP Request and generates podcasts on the respective material and sends it to me via Telegram and Gmail, and updates my Google Drive as well. Provided below is a screenshot and the downloadable JSON in case anyone would like to try it. Feel free to DM me if you have any questions. I'm also not too familiar with how to share files on Reddit so the option I settled on was placing the JSON in this code block, hopefully that works? Again, feel free to DM me if you'd like to try it and I should be able to share it to you directly as downloadable JSON for you to import into n8n. {\n  \"name\": \"AI Podcast Generation (AutoContentAPI)\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"triggerTimes\": {\n          \"item\": [\n            {}\n          ]\n        }\n      },\n      \"name\": \"Schedule: Weekly Learning Run\",\n      \"type\": \"n8n-nodes-base.cron\",\n      \"typeVersion\": 1,\n      \"position\": [\n        -1820,\n        -200\n      ],\n      \"id\": \"7a78b92e-d75b-4cab-bf0c-6a9fd41c5683\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.url }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.rssFeedRead\",\n      \"typeVersion\": 1.1,\n      \"position\": [\n        -920,\n        -180\n      ],\n      \"id\": \"2a012472-2e03-451c-80d7-202d159c3959\",\n      \"name\": \"RSS Read\",\n      \"onError\": \"continueRegularOutput\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"return [\\n  { json: { url: \\\"https://huggingface.co/blog/feed\\\" } },\\n  { json: { url: \\\"https://machinelearningmastery.com/blog/feed/\\\" } },\\n  { json: { url: \\\"https://blog.tensorflow.org/feeds/posts/default\\\" } }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -1620,\n        -200\n      ],\n      \"id\": \"758b3629-43b5-4330-a1a0-2c1aabdfdf1e\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"const keywords = [\\n  \\\"whitepaper\\\", \\\"research\\\", \\\"study\\\", \\\"publication\\\", \\\"paper\\\", \\\"preprint\\\", \\\"abstract\\\",\\n  \\\"benchmark\\\", \\\"evaluation\\\", \\\"methodology\\\", \\\"experiment\\\", \\\"analysis\\\", \\\"dataset\\\",\\n  \\\"LLM\\\", \\\"GPT\\\", \\\"transformer\\\", \\\"language model\\\", \\\"fine-tuning\\\", \\\"pretraining\\\"\\n];\\n\\nconst now = new Date();\\nconst weekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000);\\nconst monthStart = new Date(now.getFullYear(), now.getMonth(), 1);\\nconst seenLinks = new Set();\\n\\n// Domains not supported by AutoContentAPI on free tier\\nconst blockedDomains = [\\n  \\\"arxiv.org\\\",\\n  \\\"ieeexplore.ieee.org\\\",\\n  \\\"springer.com\\\",\\n  \\\"sciencedirect.com\\\",\\n  \\\"dl.acm.org\\\"\\n];\\n\\n// Score and parse\\nlet scored = items.map(item => {\\n  const title = (item.json.title || \\\"\\\").toLowerCase();\\n  const description = (item.json.description || item.json.contentSnippet || item.json.content || \\\"\\\").toLowerCase();\\n  const link = item.json.link || item.json.url || \\\"\\\";\\n  const pubDateStr = item.json.pubDate || item.json.date || item.json.isoDate || \\\"\\\";\\n  const pubDate = pubDateStr && !isNaN(Date.parse(pubDateStr)) ? new Date(pubDateStr) : null;\\n\\n  let score = 0;\\n  keywords.forEach(keyword => {\\n    if (title.includes(keyword)) score += 2;\\n    if (description.includes(keyword)) score += 1;\\n  });\\n\\n  return {\\n    json: {\\n      title: item.json.title,\\n      link,\\n      pubDate: pubDateStr,\\n      pubDateObject: pubDate,\\n      content: item.json.content || item.json.contentSnippet || \\\"\\\",\\n      score\\n    }\\n  };\\n});\\n\\n// Filter: only allow whitelisted, non-duplicate, recent items\\nlet filtered = scored.filter(item =>\\n  item.json.score >= 2 &&\\n  item.json.pubDateObject instanceof Date &&\\n  !isNaN(item.json.pubDateObject) &&\\n  item.json.link &&\\n  !seenLinks.has(item.json.link) &&\\n  !blockedDomains.some(domain => item.json.link.includes(domain)) &&\\n  seenLinks.add(item.json.link)\\n);\\n\\n// Prioritize items from the last 7 days\\nlet pastWeek = filtered.filter(item => item.json.pubDateObject >= weekAgo);\\n\\n// If none found, fall back to items from this calendar month\\nif (pastWeek.length === 0) {\\n  pastWeek = filtered.filter(item =>\\n    item.json.pubDateObject >= monthStart && item.json.pubDateObject <= now\\n  );\\n}\\n\\n// Sort by score descending\\npastWeek.sort((a, b) => b.json.score - a.json.score);\\n\\n// Return top 3\\nreturn pastWeek.slice(0, 3);\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -700,\n        -180\n      ],\n      \"id\": \"3ffafffd-f20a-4197-a09c-b08dca6099a6\",\n      \"name\": \"Whitepaper Filter\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"0e2fb51a-8995-4b8d-bb41-ea78cf5c1904\",\n              \"name\": \"url\",\n              \"value\": \"={{ $json.url }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -1120,\n        -180\n      ],\n      \"id\": \"d0115844-b5fb-489c-83fe-4d2fbd11b7b9\",\n      \"name\": \"Edit Fields\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"ca3acbb3-9375-4335-b8b2-a951e72dff76\",\n              \"name\": \"request_id\",\n              \"value\": \"={{ $json.request_id }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        120,\n        -160\n      ],\n      \"id\": \"06ef9efc-88b3-470a-b7dd-b615e7700d09\",\n      \"name\": \"Extract Request ID\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"=https://api.autocontentapi.com/content/status/{{$json[\\\"request_id\\\"]}}\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer 5b62e1aa-54d0-4319-81e8-93320d9a58ef\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        320,\n        -160\n      ],\n      \"id\": \"50db4ed9-e412-48bd-b41f-1a764be41c74\",\n      \"name\": \"GET Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://api.autocontentapi.com/Content/Create\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer YOUR<API>KEY\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"raw\",\n        \"rawContentType\": \"application/json\",\n        \"body\": \"={{ \\n  JSON.stringify({\\n    resources: [\\n      {\\n        content: $json[\\\"link\\\"],\\n        type: \\\"website\\\"\\n      }\\n    ],\\n    text: \\\"Create a podcast summary of this article in a conversational, engaging tone.\\\",\\n    outputType: \\\"audio\\\"\\n  })\\n}}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -140,\n        -160\n      ],\n      \"id\": \"8ae2fffa-03ab-4053-9db0-388de34b5287\",\n      \"name\": \"Generate Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"9f869aa6-11f0-4664-8d16-d06a6ec52c9f\",\n              \"leftValue\": \"={{ $json.status }}\",\n              \"rightValue\": 100,\n              \"operator\": {\n                \"type\": \"number\",\n                \"operation\": \"equals\"\n              }\n            }\n          ],\n          \"combinator\": \"or\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        520,\n        -160\n      ],\n      \"id\": \"2785e08c-f859-4fa2-b752-9f114e6617bc\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"teezworkspace@gmail.com\",\n        \"subject\": \"={{ $json.audio_title }}\",\n        \"message\": \"={{ $json.audio_title }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"attachmentsUi\": {\n            \"attachmentsBinary\": [\n              {\n                \"property\": \"audio\"\n              }\n            ]\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        1080,\n        80\n      ],\n      \"id\": \"f07b9a91-aa2d-43a9-9095-41497180454f\",\n      \"name\": \"Send Audio to Email\",\n      \"webhookId\": \"0ff65219-e34a-4ad4-b600-f7238569c92d\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"\",\n          \"name\": \"Terry's Gmail\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"inputDataFieldName\": \"audio\",\n        \"name\": \"={{ $json.audio_title }}\",\n        \"driveId\": {\n          \"__rl\": true,\n          \"value\": \"My Drive\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\"\n        },\n        \"folderId\": {\n          \"__rl\": true,\n          \"value\": \"1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Weekly AI Research Audio\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"typeVersion\": 3,\n      \"position\": [\n        1080,\n        -120\n      ],\n      \"id\": \"5d9eec4c-f596-48f0-a81e-5f1bc37a082b\",\n      \"name\": \"Upload Audio Folder\",\n      \"credentials\": {\n        \"googleDriveOAuth2Api\": {\n          \"id\": \"\",\n          \"name\": \"Terry Google Drive\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"sendAudio\",\n        \"chatId\": \"6018770135\",\n        \"binaryData\": true,\n        \"binaryPropertyName\": \"audio\",\n        \"additionalFields\": {\n          \"caption\": \"={{ $json.audio_title }}\",\n          \"title\": \"={{ $json.audio_title }}\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.telegram\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        1080,\n        -340\n      ],\n      \"id\": \"6f21e927-a79b-48f3-a5ff-8dd9d460916f\",\n      \"name\": \"Send Audio to Telegram\",\n      \"webhookId\": \"97f48ead-3e73-4928-a555-455722196acc\",\n      \"credentials\": {\n        \"telegramApi\": {\n          \"id\": \"\",\n          \"name\": \"AutoContentAPI Bot \"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"batchSize\": 15,\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -1380,\n        -200\n      ],\n      \"id\": \"fb9a4a7c-2aba-4a17-89e4-6e856bd23d0a\",\n      \"name\": \"URL Loop\"\n    },\n    {\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -480,\n        -180\n      ],\n      \"id\": \"9ce3486f-0bd6-45fa-bdcc-392c72bfff97\",\n      \"name\": \"Podcast Gen Loop\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.audio_url }}\",\n        \"options\": {\n          \"response\": {\n            \"response\": {\n              \"responseFormat\": \"file\",\n              \"outputPropertyName\": \"audio\"\n            }\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        760,\n        -180\n      ],\n      \"id\": \"0afdf799-a612-4a07-a2e5-c65b262ef12e\",\n      \"name\": \"Download Audio\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Schedule: Weekly Learning Run\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"RSS Read\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Whitepaper Filter\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Whitepaper Filter\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Edit Fields\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"RSS Read\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Request ID\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"GET Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Extract Request ID\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"GET Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Download Audio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upload Audio Folder\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"URL Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Edit Fields\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Podcast Gen Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Generate Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Download Audio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Audio to Telegram\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Upload Audio Folder\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Send Audio to Email\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": false,\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"57ddc431-4059-4b0e-92dc-325c7296ac9a\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"f9bd58af1591f515777c160d7518c3e5cf0ad788d4a4c3831380e58e9febdfa6\"\n  },\n  \"id\": \"Ece8XCZeyPq6R0Uv\",\n  \"tags\": []\n} https://preview.redd.it/7nryas97v84f1.png?width=2694&format=png&auto=webp&s=1bb4c966619c7cfd520ab6bc47c1dbbd439871e6"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Canva Content Creation with n8n",
          "link": "https://old.reddit.com/r/n8n/comments/1l4cy01/content_creation_and_n8n/",
          "author": "theSImessenger",
          "time": "2025-06-05T22:44:53+00:00",
          "body_text": "I'm going to go more into content creation in the next few weeks. What are some issues you have with the current content creators you follow? My issue with Nate Herk and Nick Saraev is that they focus too much on the technical side and too little on the marketing and sales aspect. I'll focus more on the things they won't talk about. The 'ugly' parts of having a business in the first place, some automation things like they do and more focused on marketing and sales. What do you do when you lose your biggest 5 clients within 24 hours (which is something I would have loved to have seen a video of when it happened to me). There's more than enough content about n8n workflows and 'copy and sell this workflow for $25,000!'. Too little about the intricate details that one experiences when launching, growing and scaling. Probably because that 'sells' less. Any feedback is appreciated to guide me in my content strategy."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Try this podcast generation workflow I built using n8n + AutoContentAPI!",
          "link": "https://old.reddit.com/r/n8n/comments/1jo48z2/n8n_automation_for_content_creation_for_companys/",
          "author": "jk_120104",
          "time": "2025-03-31T14:17:04+00:00",
          "body_text": "I would like to create or I need someone who can help or create a n8n automation that searches for content ideas for companies from the internet / news or also searches what the other companies in my industry have posted what is relevant based on the targeted audience and on the goal. Then you can choose which ideas are good and also add your own ideas and then it will generate a post idea -> that will be the caption text with hashtags depending on what platform you want to post on (it should be possible to get your own style and guidelines in there to make sure everything is safe) then it will also generate an image and or another infographic especially with the new structure from Chatgpt / Imagemodell that is so good at generating text in images. And then the final post will be scheduled on a tool like feedhive to get approval before it is uploaded in between there should also be an option to place the logo in the image and get a review / feedback from a human before scheduling so the human could make an imput that the workflow will change. And also in the future, if this works great, it would be nice to get some videos generated with voice and stuff. This would be very helpful for companies in my niche and also for others I think. On top of that it would be nice if there would also be a workflow that is more or less the same but for ads on meta for example and that helps you get ideas based on the product and the marketing angel plus the goal that you can provide in advance in quick questions asked so the model gets what you want. And I would have another great add on for this tool which is even more for rules check on guidelines for advertising which is very important in my field. Everything should not be that complicated when i do everything manually it works so yeah let me know what you think about it and would be nice to hear some opinions or from anyone who wants to build and also can build this."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jkz9au/n8nworkflow_automate_your_linkedin_content/",
          "author": "ProEditor69",
          "time": "2025-03-27T08:31:55+00:00",
          "body_text": "Tired of manually curating LinkedIn posts? My ultimate n8n workflow does all the heavy lifting‚Äî scraping, rewriting, and enhancing LinkedIn content with AI so you can grow your audience effortlessly! üíº‚ú® üî• Features: ‚úÖ Scrape Multiple LinkedIn Creator Profiles ‚Äì Extract high-quality posts in bulk. üîì No LinkedIn Login Required ‚Äì No need to risk your personal account for scraping! ü§ñ AI-Powered Post Rewriting ‚Äì Instantly transform content into fresh, engaging social media-ready posts. üé® AI Image Generator ‚Äì Creates stunning, relevant images to match the post. üõë Human Approval System ‚Äì Manually APPROVE/REJECT posts before publishing. üìä CSV Export ‚Äì Keep track of all generated content in a well-structured file. SAMPLE CSV [Included with purchase] SAMPLE EMAIL ALERT FOR APPROVAL SAMPLE EMAIL BY SENT BY THE WORKFLOW ‚úÖ Sample LINKEDLN Post Generated by the Workflow: SAMPLE LINKEDLN POST LINK üí∞ Save time, automate your LinkedIn content, and boost engagement ‚Äîall with one powerful workflow! üîó Get this workflow here: CLICK HERE TO BUY NOW"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Content Creation and n8n",
          "link": "https://old.reddit.com/r/n8n/comments/1i8v10a/need_help_workflow_automations_needed_within_the/",
          "author": "Shelf-Made",
          "time": "2025-01-24T13:10:41+00:00",
          "body_text": "Hi n8ners, I run a Content Marketing Agency with most of our clients in the B2B SaaS and services industries (Like supply chain, Regtech, Freight Tech and many others). We want to build automated workflows and integrations within content marketing and need to partner with AI Automation Specialists who may have had experience in marketing, copywriting, content research, or SEO in another life. Some examples of the types of things we want to build: Workflow: Take an interview from a tool like otter (transcript or straight from voice), look for markers that will help package the interview into a strategy document that curates the interview into 10 Ps of positioning, write the strategy into google doc (or specific nicely designed framework of some kind) Tool: We want to build a tool that knows everything about our client and then you give it a website or a prospect and based on what it knows about our client's capabilities, it reads the prospects website and then matches the prospects goals/plans/problems to the clients capabilities and then creates a table for a proposal. Chatbot: A chatbot (not sure where it can be deployed yet) that can help a shipper/freight/logistics worker to know what is the correct incoterm to use based on the seller and buyer agreements and another one that helps a shipper to determine what is the correct HS Code for a product that you're shipping. SEO Workflow: 1. Ahrefs keyword analysis ->create SEO content Brief based on analysis -> write SEO optimized blog google docs -> [Stop here to add human authenticity to article] -> agent to upload with correct H1/H2/H3 etc headlines to website backend. Writing Aid: Knows everything about a company/thought leader -> scrapes or reads news articles from a set or particular niche industry websites -> based on what it knows about company/thought leader writes short interesting snippets of company or thought leader's opinion about the news. deployed into excel spreadsheet for checking. Excel/Sheet should be able to hold multiple clients in one sheet maybe on each tab. automatically runs/fetches to write every 3 days with a capped number of 3 \"thoughts or musings per client.\" Competitor Analysis Tool: Our clients are always needing to watch for complaints around their competitor SaaSs and we end up having to manually look at platforms like G2 to find those so we can position our clients as a solution that doesn't have that problem. So the tool would be to follow a particular SaaS's subreddit or it's profile on G2 (although I think G2 blocks calls) pull everything into sheet and then do a sentiment analysis to mark them as a complaint or a fault and even go beyond to see how common that fault might be based on comments etc. This allows us to automate competitor analysis for our clients. I'm so sorry for such a long post. Hoping to meet some brilliant minds! Feel free to get in touch and send your rates and what you need for project briefs."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1ldvy4n/10_things_i_wish_i_knew_before_diving_into_ai/",
          "author": "Drogoff1489",
          "time": "2025-06-17T19:09:39+00:00",
          "body_text": "Been deep in the automation game for the past year - here's what actually matters vs. what everyone talks about: 1. Start stupidly simple Your first automation should take 10 minutes, not 10 hours. I wasted weeks on complex builds when a simple \"new email ‚Üí Slack notification\" would've taught me more. 2. Document your builds publicly Every automation you create is potential content. Screenshots, learnings, failures - it all becomes proof of expertise. I get more clients from sharing my process than from perfect demos. 3. Master the HTTP Request node first Seriously. Half the \"limitations\" people complain about disappear when you can build custom API calls. It's your Swiss Army knife for everything the built-in nodes can't handle. 4. Stop calling yourself an \"automation expert\" Everyone says that. Instead: \"I help [specific industry] eliminate [specific pain point].\" Specificity attracts premium clients who have that exact problem. 5. Your biggest wins come from saying no Turned down a $500 project last month because it wasn't aligned with my positioning. Client came back two weeks later with a $3K project that was perfect fit. Boundaries create value. 6. Error handling is where amateurs get exposed Everyone shows the happy path. Pros build for when APIs go down, data formats change, or users input garbage. Plan for chaos. 7. Share your failures, not just successes \"Here's how I broke a client's workflow and what I learned\" gets way more engagement than \"Look at this perfect automation.\" Vulnerability builds trust. 8. The money is in ongoing optimization, not one-time builds Clients pay once for setup, monthly for \"make it work better.\" Maintenance contracts beat project work every time. 9. Your network determines your net worth Other automators become referral sources, not competition. Help people in communities, share knowledge freely. Half my clients come from automator referrals now. 10. Build your own systems first Nothing proves automation expertise like having your own lead generation, content creation, and client onboarding automated. Practice what you preach. Bonus insight : The automators making real money talk about business outcomes, not technical features. \"Saved 15 hours/week\" beats \"Built a 47-node workflow\" every time. What's your biggest automation learning curve? Always curious what trips people up vs. what clicks immediately."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "N8N automation for Content Creation for Companys with guidelines that could turn in a SAAS",
          "link": "https://old.reddit.com/r/n8n/comments/1it9ybc/new_workflow_added_of_short_video_creation/",
          "author": "koncept25",
          "time": "2025-02-19T16:42:58+00:00",
          "body_text": "Hey everyone, After the amazing response to my last post, I‚Äôm excited to share another powerful workflow ‚Äîthis time for automating YouTube Shorts Or Instagram reel creation! (Faceless Channels) üí° This workflow wasn‚Äôt built by me but by Productive Dude , so full credit to him! üôå üìå Code & Video are available on the Notion website! üî• What‚Äôs Next? I‚Äôm also working on an AI agent for \"Research Anything\" by AI Foundation , so stay tuned for more automation! üîó Get the workflow & previous ones ‚Äì Link in the comment or check my profile ! ‚ö° More workflows coming this week! If you have ideas, submit them in the form on the website! Thank you! üöÄ AI Agent for YouTube Short and Instagram Reels"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1ksnvdm/power_up_your_n8n_workflows_with_diverse_content/",
          "author": "PostGoo",
          "time": "2025-05-22T11:02:28+00:00",
          "body_text": "I run an Instagram account that publishes short form videos each week that cover the top AI news stories. I used to monitor twitter to write these scripts by hand, but it ended up becoming a huge bottleneck and limited the number of videos that could go out each week. In order to solve this, I decided to automate this entire process by building a system that scrapes the top AI news stories off the internet each day (from Twitter / Reddit / Hackernews / other sources), saves it in our data lake, loads up that text content to pick out the top stories and write video scripts for each. This has saved a ton of manual work having to monitor news sources all day and let‚Äôs me plug the script into ElevenLabs / HeyGen to produce the audio + avatar portion of each video. One of the recent videos we made this way got over 1.8 million views on Instagram and I‚Äôm confident there will be more hits in the future. It‚Äôs pretty random on what will go viral or not, so my plan is to take enough ‚Äúshots on goal‚Äù and continue tuning this prompt to increase my changes of making each video go viral. Here‚Äôs the workflow breakdown 1. Data Ingestion and AI News Scraping The first part of this system is actually in a separate workflow I have setup and running in the background. I actually made another reddit post that covers this in detail so I‚Äôd suggestion you check that out for the full breakdown + how to set it up. I‚Äôll still touch the highlights on how it works here: The main approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. Right now, there are around ~13 news sources that I have setup to pull stories from every single day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each  story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. 2. Loading up and formatting the scraped news stories Once the data lake / news storage has plenty of scraped stories saved for the day, we are able to get into the main part of this automation. This kicks off off with a scheduled trigger that runs at 7pm each day and will: Search S3 bucket for all markdown files and tweets that were scraped for the day by using a prefix filter Download and extract text content from each markdown file Bundle everything into clean text blocks wrapped in XML tags for better LLM processing - This allows us to include important metadata with each story like the source it came from, links found on the page, and include engagement stats (for tweets). 3. Picking out the top stories Once everything is loaded and transformed into text, the automation moves on to executing a prompt that is responsible for picking out the top 3-5 stories suitable for an audience of AI enthusiasts and builder‚Äôs. The prompt is pretty big here and highly customized for my use case so you will need to make changes for this if you are going forward with implementing the automation itself. At a high level, this prompt will: Setup the main objective Provides a ‚Äúcuration framework‚Äù to follow over the list of news stories that we are passing int Outlines a process to follow while evaluating the stories Details the structured output format we are expecting in order to avoid getting bad data back ```jsx\n<objective>\nAnalyze the provided daily digest of AI news and select the top 3-5 stories most suitable for short-form video content. Your primary goal is to maximize audience engagement (likes, comments, shares, saves). The date for today's curation is {{ new Date(new Date($('schedule_trigger').item.json.timestamp).getTime() + (12 * 60 * 60 * 1000)).format(\"yyyy-MM-dd\", \"America/Chicago\") }} . Use this to prioritize the most recent and relevant news. You MUST avoid selecting stories that are more than 1 day in the past for this date.\n</objective> <curation_framework>\nTo identify winning stories, apply the following virality principles. A story must have a strong \"hook\" and fit into one of these categories: Impactful: A major breakthrough, industry-shifting event, or a significant new model release (e.g., \"OpenAI releases GPT-5,\" \"Google achieves AGI\"). Practical: A new tool, technique, or application that the audience can use now (e.g., \"This new AI removes backgrounds from video for free\"). Provocative: A story that sparks debate, covers industry drama, or explores an ethical controversy (e.g., \"AI art wins state fair, artists outraged\"). Astonishing: A \"wow-factor\" demonstration that is highly visual and easily understood (e.g., \"Watch this robot solve a Rubik's Cube in 0.5 seconds\"). Hard Filters (Ignore stories that are): * Ad-driven: Primarily promoting a paid course, webinar, or subscription service.\n* Purely Political: Lacks a strong, central AI or tech component.\n* Substanceless: Merely amusing without a deeper point or technological significance.\n</curation_framework> <hook_angle_framework>\nFor each selected story, create 2-3 compelling hook angles that could open a TikTok or Instagram Reel. Each hook should be designed to stop the scroll and immediately capture attention. Use these proven hook types: Hook Types: - Question Hook: Start with an intriguing question that makes viewers want to know the answer\n- Shock/Surprise Hook: Lead with the most surprising or counterintuitive element\n- Problem/Solution Hook: Present a common problem, then reveal the AI solution\n- Before/After Hook: Show the transformation or comparison\n- Breaking News Hook: Emphasize urgency and newsworthiness\n- Challenge/Test Hook: Position as something to try or challenge viewers\n- Conspiracy/Secret Hook: Frame as insider knowledge or hidden information\n- Personal Impact Hook: Connect directly to viewer's life or work Hook Guidelines: - Keep hooks under 10 words when possible\n- Use active voice and strong verbs\n- Include emotional triggers (curiosity, fear, excitement, surprise)\n- Avoid technical jargon - make it accessible\n- Consider adding numbers or specific claims for credibility\n</hook_angle_framework> <process>\n1. Ingest: Review the entire raw text content provided below.\n2. Deduplicate: Identify stories covering the same core event. Group these together, treating them as a single story. All associated links will be consolidated in the final output.\n3. Select & Rank: Apply the Curation Framework to select the 3-5 best stories. Rank them from most to least viral potential.\n4. Generate Hooks: For each selected story, create 2-3 compelling hook angles using the Hook Angle Framework .\n</process> <output_format>\nYour final output must be a single, valid JSON object and nothing else. Do not include any text, explanations, or markdown formatting like ` json before or after the JSON object. The JSON object must have a single root key, stories , which contains an array of story objects. Each story object must contain the following keys:\n- title (string): A catchy, viral-optimized title for the story.\n- summary (string): A concise, 1-2 sentence summary explaining the story's hook and why it's compelling for a social media audience.\n- hook_angles (array of objects): 2-3 hook angles for opening the video. Each hook object contains:\n  - hook (string): The actual hook text/opening line\n  - type (string): The type of hook being used (from the Hook Angle Framework)\n  - rationale (string): Brief explanation of why this hook works for this story\n- sources (array of strings): A list of all consolidated source URLs for the story. These MUST be extracted from the provided context. You may NOT include URLs here that were not found in the provided source context. The url you include in your output MUST be the exact verbatim url that was included in the source material. The value you output MUST be like a copy/paste operation. You MUST extract this url exactly as it appears in the source context, character for character. Treat this as a literal copy-paste operation into the designated output field. Accuracy here is paramount; the extracted value must be identical to the source value for downstream referencing to work. You are strictly forbidden from creating, guessing, modifying, shortening, or completing URLs. If a URL is incomplete or looks incorrect in the source, copy it exactly as it is. Users will click this URL; therefore, it must precisely match the source to potentially function as intended. You cannot make a mistake here.\n``` After I get the top 3-5 stories picked out from this prompt, I share those results in slack so I have an easy to follow trail of stories for each news day. 4. Loop to generate each script For each of the selected top stories, I then continue to the final part of this workflow which is responsible for actually writing the TikTok / IG Reel video scripts. Instead of trying to 1-shot this and generate them all at once, I am iterating over each selected story and writing them one by one. Each of the selected stories will go through a process like this: Start by additional sources from the story URLs to get more context and primary source material Feeds the full story context into a viral script writing prompt Generates multiple different hook options for me to later pick from Creates two different 50-60 second scripts optimized for talking-head style videos (so I can pick out when one is most compelling) Uses examples of previously successful scripts to maintain consistent style and format Shares each completed script in Slack for me to review before passing off to the video editor. Script Writing Prompt ```jsx\nYou are a viral short-form video scriptwriter for David Roberts, host of \"The Recap.\" Follow the workflow below each run to produce two 50-60-second scripts (140-160 words). Before you write your final output, I want you to closely review each of the provided REFERENCE_SCRIPTS and think deeploy about what makes them great. Each script that you output must be considered a great script. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 1 ‚Äì Ideate ‚Ä¢ Generate five distinct hook sentences (‚â§ 12 words each) drawn from the STORY_CONTEXT. STEP 2 ‚Äì Reflect & Choose ‚Ä¢ Compare hooks for stopping power, clarity, curiosity. ‚Ä¢ Select the two strongest hooks (label TOP HOOK 1 and TOP HOOK 2). ‚Ä¢ Do not reveal the reflection‚Äîonly output the winners. STEP 3 ‚Äì Write Two Scripts For each top hook, craft one flowing script ‚âà 55 seconds (140-160 words). Structure (no internal labels): ‚Äì Open with the chosen hook. ‚Äì One-sentence explainer. ‚Äì 5-7 rapid wow-facts / numbers / analogies. ‚Äì 2-3 sentences on why it matters or possible risk. ‚Äì Final line = a single CTA ‚Ä¢ Ask viewers to comment with a forward-looking question or ‚Ä¢ Invite them to follow The Recap for more AI updates. Style: confident insider, plain English, light attitude; active voice, present tense; mostly ‚â§ 12-word sentences; explain unavoidable jargon in ‚â§ 3 words. OPTIONAL POWER-UPS (use when natural) ‚Ä¢ Authority bump ‚Äì Cite a notable person or org early for credibility. ‚Ä¢ Hook spice ‚Äì Pair an eye-opening number with a bold consequence. ‚Ä¢ Then-vs-Now snapshot ‚Äì Contrast past vs present to dramatize change. ‚Ä¢ Stat escalation ‚Äì List comparable figures in rising or falling order. ‚Ä¢ Real-world fallout ‚Äì Include 1-3 niche impact stats to ground the story. ‚Ä¢ Zoom-out line ‚Äì Add one sentence framing the story as a systemic shift. ‚Ä¢ CTA variety ‚Äì If using a comment CTA, pose a provocative question tied to stakes. ‚Ä¢ Rhythm check ‚Äì Sprinkle a few 3-5-word sentences for punch. OUTPUT FORMAT  (return exactly this‚Äîno extra commentary, no hashtags) HOOK OPTIONS ‚Ä¢ Hook 1 ‚Ä¢ Hook 2 ‚Ä¢ Hook 3 ‚Ä¢ Hook 4 ‚Ä¢ Hook 5 TOP HOOK 1 SCRIPT [finished 140-160-word script] TOP HOOK 2 SCRIPT [finished 140-160-word script] REFERENCE_SCRIPTS <Pass in example scripts that you want to follow and the news content loaded from before>\n``` 5. Extending this workflow to automate further So right now my process for creating the final video is semi-automated with human in the loop step that involves us copying the output of this automation into other tools like HeyGen to generate the talking avatar using the final script and then handing that over to my video editor to add in the b-roll footage that appears on the top part of each short form video. My plan is to automate this further over time by adding another human-in-the-loop step at the end to pick out the script we want to go forward with ‚Üí Using another prompt that will be responsible for coming up with good b-roll ideas at certain timestamps in the script ‚Üí use a videogen model to generate that b-roll ‚Üí finally stitching it all together with json2video. Depending on your workflow and other constraints, It is really up to you how far you want to automate each of these steps. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=7WsmUlbyjMM The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "[n8n-workflow] Automate Your LinkedIn Content Creation with AI ‚Äì No Login Requiredüî•",
          "link": "https://old.reddit.com/r/n8n/comments/1loafvx/i_built_this_ai_automation_to_write_viral/",
          "author": "dudeson55",
          "time": "2025-06-30T15:26:51+00:00",
          "body_text": "Sorry for this, I am brand new so forgive any ignorance or misunderstanding of the rules that I have. I am struggling with a N8N workflow that I know is practically possible, but seemingly beyond my comprehension. I am looking for help on a job, and would be willing to pay if the price point was reasonable for what my company would invest in this workflow should it be practical. The workflow would involve using ChatGPT and other AI Application APIs to process video/audio files on local SSD drives, label/categorize/sort them, sync and clip talking head interviews, clip best b-roll, layout talking audio clips in a story format that makes sense, generate AI Music that is copyright free, generate any animation or AI based visuals needed, generic on scree graphics, and process 3D models of products that can be rendered into video demo's like how to's of using products, all of which should be tied into a locally saved XML file I can open in Premiere. On top of all of that, I need the workflow to be self learning and capable of also scraping the internet for information on competitors and our social media metrics so it can be informed decisions on what videos work, what videos don't, edits that should be made, better music to create, etc. I don't need it to do all of the work of creating my videos, but I film a lot each week and if I could spend more time doing revisions and more creative video edits, and less time making some of the tedious cuts or models I would love it. As I said before, I know these are all doable but it's beyond me how I can tie them all together and make them work side by side. Any help, or anyone who would be able to do such work for hire would be greatly appreciated (to be blunt and clear, there is no explicit job offer but I would be happy to accept bids for this jobs should it price within my budget at work - I am trying to create some sort of proposal to share in regards to what this would take so I can get approval for the help I need, AI Agents are not immediately relevant to my company beyond what it could do for me). I have also diagrammed a rough workflow idea that I can share as well. I also should add I already have a locally hosted N8N through Docker with accounts for postman, a server on Digital Ocean, and a connection through Ngrok. Thanks for any help or guidance in advance."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Template",
          "link": "https://old.reddit.com/r/n8n/comments/1k3idx1/looking_for_help_setting_up_local_content/",
          "author": "NevcoVideoCreator",
          "time": "2025-04-20T08:55:17+00:00",
          "body_text": "Made it for my class project 2 months prior. Idk how to get clients. Everything works amazingly and I am impressed. Anyways here's the read me file from my GitHub: # n8n-Project This project is relating agentic sales workflow and business automation. # n8n Agentic Sales Workflow Automation ## Overview This project demonstrates a sophisticated agentic sales workflow built using n8n. It integrates AI-powered intent recognition, automated market trend analysis from Amazon, a custom sales forecasting model, and proactive low stock alerts. The system is designed to act as an intelligent assistant for sales and inventory management, providing actionable insights and automating key reporting tasks. ## Features *   **AI-Powered Chatbot:** *   Handles user queries via webhook or n8n's chat trigger. *   Utilizes a Groq LLM via Langchain nodes for fast intent recognition (Market Trend, Sales Forecast, Low Stock Alert, or Unknown). *   Provides immediate acknowledgment and routes requests to the appropriate sub-workflow. *   **Automated Market Trend Analysis:** *   Scrapes product data (e.g., electronics) from Amazon. *   Processes scraped HTML to extract product details using custom JavaScript logic. *   Leverages a Groq LLM to analyze product listings, identify top categories, average prices, pricing trends, and generate market insights. *   Automatically generates and sends an email report of the market analysis. *   Saves the analysis to a Google Sheet for record-keeping and further use. *   **Advanced Sales Forecasting:** *   Generates sample historical sales data (can be replaced with actual data source). *   Merges historical data with fresh market insights from the Market Trend Analysis pipeline. *   Enhances sales data based on identified market trends. *   Preprocesses data and prepares it for model training (feature engineering, scaling). *   Trains a custom Gradient Descent Linear Regression model (implemented in JavaScript within an n8n Function node) to predict future sales. *   Evaluates model performance (MSE, RMSE, R¬≤). *   Generates future sales forecasts. *   Formats forecast data for visualization and reporting. *   Distributes forecast reports via email, saves to Google Sheets, and can send data to an external dashboard via webhook. *   **Proactive Low Stock Alerts:** *   Triggered by chat intent. *   Checks a simulated inventory (can be adapted to a real inventory system). *   Identifies items with stock levels below a defined threshold. *   Provides an alert summary back to the chat interface. *   Sends an email notification for low stock items, including recommendations and urgency. *   **Modular Workflow Design:** *   Clearly defined sub-pipelines for each core function, managed by a central Switch node. *   Utilizes n8n's Sticky Notes (as seen in the original `n8n_project_combined_final_version.json` file) for excellent visual documentation of workflow sections. ## Workflow Breakdown The workflow is organized into several interconnected pipelines, as detailed by the Sticky Notes in the n8n canvas: **Chatbot Intent Processing:** Handles incoming messages, normalizes input, and routes based on LLM-identified user intent. **Market Trend Analysis & Reporting:** Scrapes product data, uses an LLM to analyze trends, and generates/distributes reports. This can run independently or feed into the sales forecast. **Sales Data Preparation:** Generates historical sales data, merges it with market insights, and preprocesses it for model training. **Sales Forecasting Model:** Splits data, trains a regression model, predicts future sales, and evaluates performance. **Forecast Output & Distribution:** Formats forecast data, saves to Google Sheets, sends to a dashboard, and emails reports. **Low Stock Alert System:** Checks inventory based on chat intent, responds in chat, and emails alerts. ## Technologies Used *   **n8n:** Core workflow automation platform. *   **Groq LLM:** For fast and efficient AI tasks (intent recognition, market analysis). *   **@n8n/n8n-nodes-langchain:** For seamless integration with LLMs. *   **JavaScript:** Extensively used in n8n Function/Code nodes for: *   Custom web scraping logic for Amazon product details. *   Data processing and transformation pipelines. *   Implementation of the Gradient Descent Linear Regression model. *   Business logic for alerts, report generation, and input normalization. *   **Web Scraping:** n8n's HTTP Request & HTML Extract nodes for Amazon data. *   **Google Sheets:** For data storage (market analysis, sales forecasts). *   **SMTP:** For sending email notifications and reports. *   **Webhook:** For chat interaction and dashboard integration. ## Setup & Configuration **Import Workflow:** Import the `n8n_project_combined_final_version.json` file into your n8n instance. **Credentials:** Configure the necessary credentials in n8n for: *   Groq API (for `Groq Chat Model` nodes: IDs `f3e1caa8...` and `be65cfaf...`). *   SMTP (for `EmailSend` nodes: IDs `f198e912...`, `5fa0580d...`, `c0407d2c...`). *   Google Sheets OAuth2 (for `GoogleSheets` nodes: IDs `387d3ad8...`, `5fae2bf2...`). **Node Configuration (Verify/Update):** *   **Webhooks:** *   `Chatbot Webhook` (ID `b580128b...`): Note its path (`sales-chat`) for external triggers. *   `Send to Dashboard` (ID `fa379d39...`): Currently uses a placeholder Zapier URL. Update with your actual dashboard webhook. *   **Google Sheets Nodes:** *   `Save Market Analysis` (ID `387d3ad8...`): Spreadsheet ID `1dcFVMUN4LNqxWKzkVNxsTycZIn-pXvsJCSrc4KlnPqE`, Sheet Name `gid=0` (Sheet1). *   `Write Forecast to Google Sheet1` (ID `5fae2bf2...`): Spreadsheet ID `1GEhcjhEL4B96yU91kTunQdYgD71pcXLv-6CarXe9ylA`, Sheet Name `Sheet1`. *   **Email Nodes:** Update `FromEmail`, `ToEmail`, `CCEmail` addresses. *   **`Generate Market Email Body` (ID `aee77175...`):** The `googleSheetUrl` variable in the JS code points to the Market Analysis sheet. *   **`Generate Forecast Email HTML` (ID `385b7b00...`):** The `forecastSheetUrl` variable in the JS code points to the Sales Forecasts sheet. *   **`Fetch Amazon Search Results` / `Fetch Amazon Search Results1` (IDs `e2cfb0b9...`, `a362ba64...`):** Amazon URLs are hardcoded (e.g., `k=electronics`). Modify for different searches. *Disclaimer: Amazon's scraping policies can change and may impact functionality.* ## How to Use *   **Chat Interaction:** *   Send a POST request to the `Chatbot Webhook` endpoint (e.g., `YOUR_N8N_URL/webhook/sales-chat`). *   Or, use the `When chat message received` trigger (ID `dc91fe83...`) with n8n's chat interface. *   Example prompts: *   \"What are the current market trends for electronics?\" *   \"Generate a sales forecast.\" *   \"Are there any low stock items?\" *   **Manual Trigger:** The `Start` node (ID `bc2d6054...`) can manually initiate the market trend and sales forecasting pipelines, likely for testing. ## Project Structure & Logic Highlights *   **Agentic Behavior:** The workflow understands user intent and autonomously executes complex, multi-step tasks. *   **Custom ML Model:** A Gradient Descent Linear Regression model is implemented from scratch in JavaScript within an n8n Function node, showcasing n8n's flexibility. *   **Dynamic Data Integration:** Sales forecasts intelligently incorporate real-time market analysis scraped from Amazon. *   **Intent-Driven Architecture:** A central Switch node (`aa6d732e...`) elegantly routes requests to specialized sub-workflows based on LLM-classified intent. *   **Comprehensive Multi-Channel Reporting:** Insights and alerts are delivered via chat, email, and Google Sheets. *   **Logical Flow Control:** The workflow demonstrates sophisticated connection logic, such as the \"Twin Paradox\" (Sales Forecast triggering Market Trends for data enrichment) and the \"Necessary Evil\" (an intentional error path to control pipeline execution flow for the `Merge With Forecast` node). --- P.S. Can someone guide me how to get clients?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need help: workflow automations needed within the realm of content marketing",
          "link": "https://old.reddit.com/r/n8n/comments/1lzz02o/how_much_is_my_workflow_worth/",
          "author": "Nocare420",
          "time": "2025-07-14T21:22:57+00:00",
          "body_text": "With the new ChatGPT-4o image generator, I've been considering whether I could move away from actively using LinkedIn. The constant need to create organic content and post daily to gain followers or connect with people feels like a significant time investment. Providing free content and value every day is demanding, and I'm looking for a more efficient approach.\nMy idea is to use Gemini 2.5 for writing the text and ChatGPT-4o for generating images. I have a template in mind, and I'd like a workflow where the AI generates a LinkedIn post based on this template, sends it to me for approval, and only posts if I give it the go-ahead. However, my current setup doesn't integrate with ChatGPT-4o, and I'm unsure how to create this workflow. (And i would like to add a node  where i send them like every week or every month a linkedin post or linkedin profile to inspire from it and post similar like him)\nAny help or suggestions on how to integrate these tools and automate this process would be greatly appreciated)\nP.S. I found a helpful YouTube channel https://youtu.be/o_oSYl6gSO8?si=rj7hAaFtimFn4iIk that inspired this idea, and the template I'm referring to can be found there.\"\nIf i did find  a way how this is created i will post the template next post :)"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "10 things I wish I knew before diving into AI automation (after building 100+ workflows)",
          "link": "https://old.reddit.com/r/n8n/comments/1jkex9o/linkedin_post_automation_or_any_content_creation/",
          "author": "Xx_zineddine_xX",
          "time": "2025-03-26T15:47:40+00:00",
          "body_text": "Hey fellow automation nerds! üëã I just built a powerful n8n workflow that automates product research and UGC ad script generation, and I had to share it with you all! üî• What It Does: 1Ô∏è‚É£ Scrapes Top Product Review Videos ‚Äì Fetches trending YouTube videos reviewing a specific product. 2Ô∏è‚É£ Extracts Insights ‚Äì Analyzes the transcripts to identify key selling points, pros/cons, and emotional triggers. 3Ô∏è‚É£ AI-Powered Script Generation ‚Äì Feeds the insights into AI to generate an authentic, engaging UGC ad script. https://preview.redd.it/ca1xq19m4aoe1.jpg?width=1080&format=pjpg&auto=webp&s=cb2a775f6ee211e9f0b13a5050f6604c0ac1aea1 https://preview.redd.it/rrl5fpfn4aoe1.jpg?width=1080&format=pjpg&auto=webp&s=c37a6133d7209e5f24b2bb9502334d105467fa5b üí° Why This is Game-Changing: ‚úÖ Saves hours of manual research & scriptwriting. ‚úÖ Generates data-driven ad content based on real consumer reactions. ‚úÖ Helps marketers & brands create compelling UGC-style ads effortlessly. If you're in e-commerce, dropshipping, or running paid ads , this could be a game-changer! Thinking of adding auto-video generation next. üëâ You can check my architectur e and design your own workflow. If you need a ready-to-use workflo w, grab it from the link below! üîóüî•Hey fellow automation nerds! üëã I just built a powerful n8n workflow that automates product research and UGC ad script generation, and I had to share it with you all! üî• What It Does: 1Ô∏è‚É£ Scrapes Top Product Review Videos ‚Äì Fetches trending YouTube videos reviewing a specific product. 2Ô∏è‚É£ Extracts Insights ‚Äì Analyzes the transcripts to identify key selling points, pros/cons, and emotional triggers. 3Ô∏è‚É£ AI-Powered Script Generation ‚Äì Feeds the insights into AI to generate an authentic, engaging UGC ad script. You can check my architecture and design your own workflow. If you need a ready-to-use workflow , grab it from the link below! üîóüî• üëâ GET THE AUTOMATION"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1j9n1zv/n8nworkflow_automating_ugc_ad_script_creation/",
          "author": "ProEditor69",
          "time": "2025-03-12T15:46:02+00:00",
          "body_text": "Creating User-Generated Content (UGC) videos for Amazon products manually? Ain‚Äôt nobody got time for that! üòÖ Here‚Äôs how I automated the process to generate UGC-style AI product videos from any Amazon lin k in seconds! üî• https://preview.redd.it/51efmzhiusne1.png?width=787&format=png&auto=webp&s=931fbf47342a032ac1460c24584434cca0ff39c0 üîπ How It Works: 1Ô∏è‚É£ Paste an Amazon Product Link in your Telegram chat üõí 2Ô∏è‚É£ Add your own Title,description, affiliate links üì¶ 3Ô∏è‚É£ Auto-Generates a UGC-Style Video ÔøΩ ÔøΩ 4Ô∏è‚É£ Instant Download or Direct Upload to Socials ÔøΩ ÔøΩ ‚ö° Why This is a Game-Changer? ‚úÖ No need to record videos manually! ‚úÖ Perfect for affiliate marketers, brands & content creators üéØ ‚úÖ AI-generated captions, voiceovers & smooth animations ‚úÖ Scales UGC video production instantly For COSTING & budget required for this automation, check it out here."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "üöÄ New Workflow added of Short Video Creation üé•",
          "link": "https://old.reddit.com/r/n8n/comments/1j7rod6/n8n_workflow_automate_ugc_video_creation_from_any/",
          "author": "ProEditor69",
          "time": "2025-03-10T05:38:26+00:00",
          "body_text": "after going through over 1,000 n8n templates , I‚Äôm convinced most of them are either half-baked, broken, or riddled with bugs. So many look good on GitHub or in the n8n library, but once you import them‚Ä¶ missing nodes, failed integrations, or completely outdated logic. It‚Äôs like people just export random experiments and call them ‚Äúproduction-ready.‚Äù Anyway, after way too many months of testing, debugging, and cursing at my screen, I found a few actually solid workflows worth using especially if you're doing lead gen, enrichment, or routing work. Firstly here are OG n8n Templates these are Awesome and works enescingoz/awesome-n8n-templates ‚Äì A well-curated repo with real-world workflows, including one for lead scoring and CRM integration. https://github.com/wassupjay/n8n-free-templates Bonus : How I Vet GitHub Repos Automatically After getting burned a few times, I stopped trusting stars or even READMEs. So now, I run a quick reputation check before using any GitHub template. Specifically: Is it active? Are people talking about it on Reddit or X ? Are there better forks or open issues? Does it look like a side project someone forgot about? To automate this, I ask BhindiAI to scan Reddit and X for the most relevant discussions about a repo, summarize the general vibe (positive, meh, sketchy), and extract key takeaways. It then logs that info repo link + commentary + score straight into my Google Sheet. That way I‚Äôm not wasting time setting up stuff the internet already knows is broken. Reddit and X Discussions are helpful to know what is a bs better than github stars. The worst part? Some templates had security vulnerabilities that weren't obvious until I dug deeper. One workflow was storing API keys in plain text comments, and another was making unencrypted calls to external services. I now have a strict checklist I run through before implementing anything in production. I've also started forking promising templates immediately and maintaining my own versions. Too many times I've seen maintainers abandon projects or push breaking changes without warning. Now I control my own destiny with these workflows. The community aspect is real though - I've found some of my best templates through Reddit and random Twitter threads where people share their actual working setups. The official n8n community forums are hit or miss, but when you find someone who's actually using these workflows at scale, their insights are gold."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Power Up Your n8n Workflows with Diverse Content: LumenFeed.com Offers 10k Free API Requests/Month!",
          "link": "https://old.reddit.com/r/n8n/comments/1ltsocy/i_have_reviewed_over_1000_n8n_templates_heres_how/",
          "author": "Valuable_Simple3860",
          "time": "2025-07-07T12:19:18+00:00",
          "body_text": "I run an AI Tools directory and a large growth opportunity for us is writing \"roundup\" articles that highlight the best AI tools in a particular software category such as \"Best AI Image Editors\", \"Best AI Sales Tools\" , and many more. The problem is, this content takes a ton of time to research, write, and build out rankings for each tool. With the major improvements that have rolled out to \"Deep Research\" tools, we decide to see if we could leverage the output of these deep research reports, feed it into our own prompts, and scale out the process of creating great content. This automation is what we were able to build and I'm happy to report this process has allowed us to significantly scale up our content creation process. I'm pretty amazed at how much depth Chat GPT's deep research tool is able to provide and I think in many cases it is able to provide better insights than if we were doing this by hand. Here's the automation breakdown 1. Trigger / Inputs We use a simple form trigger for this automation that takes in some details specific to the structure of our website such as the slug for the category and the target audience for this article We also accept an optional parameter here for any tools that we know we want to be included in the \"Best Tools by Category\" roundup in order to give us a bit more control of the final output. 2. Building Necessary Context The next few nodes in here our very specific to our own website. We get started by making an HTTP request into our own API to get details about the category we are going to write about. After that, we make another HTTP request to our own API to get a list of all tools that are inside the category we are going to write about. Depending on the type of article you are going to write, this initial \"build context\" step is likely going to look very different for you so be sure you start with the end vision in mind of what you are trying to create and then work backwards from there to build the context you need. 3. Building The Deep Research Prompt The next step here is to take all of that context we just gathered and bundle it together into a prompt that we will feed into a deep research tool like Chat GPT deep research or Gemini Deep research. In our case, we build a prompt that focuses on telling the deep research tool to focus on picking out the top 6-8 tools in the provided category with a strong focus on social signals and real reviews left by users of the tool. Once again, this is going to be something that you will need to customize for your own use case. You should expect to spend a lot of time iterating on this prompt to get it in a good spot. I think we probably went through 30+ iterations of this prompt until we were happy with the deep research output. 4. Human-In-The-Loop Step Once the prompt is generated, we pause the automation with a sendAndWait slack message that will post the full prompt in slack. We then copy that prompt into Chat GPT deep research and let that run until we get the deep research report back After the deep research process finishes, we click the \"Add Result\" button in slack which will let us paste in the output of deep research into a form and continue with the automation. The Chat GPT deep research output is just so good so we opted to go this way. 5. Generating Article Content With the deep research report content now inside our n8n execution, we are then able to proceed with generation our article. We opted to take an approach where we build this section by section (like lego block) instead of trying to one-shot the entire article. We found this to be more effective and result in better output. Here's how we split it up: Intro paragraph and article meta description get written first in two separate prompts After that, we write another paragraph that details the ranking criteria that was used to pick out the top tools and how they were evaluated We then move on to writing the main content for each of the selected tools. We have a single prompt and a format we follow for each tool, so we loop over the selected tools and generate this content for each. Lastly, we take the output for each of these sections and pass that into a final prompt that is responsible for writing a conclusion. 6. Pushing To Our CMS In the final section of this automation, we take the output of each of the nodes from before and bundle it up into into a format that can easily get inserted into our CMS. We use Sanity CMS for hosting all of our content which doesn't have an out of the box API that we can just push content too, so we opted to create our own API endpoint to handle this for us. If you are hosting your content on something like Webflow / WordPress / another CMS, you should be able to replace the node here with an HTTP request that publishes your content to replicate this behavior. Workflow Link + Other Resources Github workflow link for this automation: https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json YouTube video that walks through this workflow step-by-step: https://youtu.be/uDrkgEuEOBA Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l81jzd/i_built_an_ai_automation_that_writes_seooptimized/",
          "author": "dudeson55",
          "time": "2025-06-10T15:23:31+00:00",
          "body_text": "https://preview.redd.it/jdoutiemdawe1.png?width=3252&format=png&auto=webp&s=339ce57b75c783169d5cf0206578c0e06c27c931 Hey everyone! I wanted to share something I've built that I'm actually proud of - a fully operational chatbot system for my Airbnb property in the Philippines (located in an amazing surf destination). And let me be crystal clear right away: I have absolutely nothing to sell here. No courses, no templates, no consulting services, no \"join my Discord\" BS. Unlike the flood of posts here that showcase flashy-looking but ultimately useless \"theoretical\" workflows (you know the ones - pretty diagrams that would break instantly in production), this is a real, functioning system handling actual guest inquiries every day. And the kicker? I had absolutely zero coding experience when I started building this. What I've created: A multi-channel AI chatbot system that handles: Instagram DMs Facebook Messenger Direct chat interface It intelligently: Classifies guest inquiries (booking questions, transportation needs, weather/surf conditions, etc.) Routes to specialized AI agents Checks live property availability Generates booking quotes with clickable links Knows when to escalate to humans Remembers conversation context Answers in whatever language the guest uses System Architecture Overview System Components The system consists of four interconnected workflows: Message Receiver: Captures messages from Instagram, Messenger, and n8n chat interfaces Message Processor: Manages message queuing and processing Router: Analyzes messages and routes them to specialized agents Booking Agent: Handles booking inquiries with real-time availability checks Message Flow 1. Capturing User Messages The Message Receiver captures inputs from three channels: Instagram webhook Facebook Messenger webhook Direct n8n chat interface Messages are processed, stored in a PostgreSQL database in a message_queue table, and flagged as unprocessed. 2. Message Processing The Message Processor does not simply run on schedule, but operates with an intelligent processing system: The main workflow processes messages immediately After processing, it checks if new messages arrived during processing time This prevents duplicate responses when users send multiple consecutive messages A scheduled hourly check runs as a backup to catch any missed messages Messages are grouped by session_id for contextual handling 3. Intent Classification & Routing The Router uses different OpenAI models based on the specific needs: GPT-4.1 for complex classification tasks GPT-4o and GPT-4o Mini for different specialized agents Classification categories include: BOOKING_AND_RATES, TRANSPORTATION_AND_EQUIPMENT, WEATHER_AND_SURF, DESTINATION_INFO, INFLUENCER, PARTNERSHIPS, MIXED/OTHER The system maintains conversation context through a session_state database that tracks: Active conversation flows Previous categories User-provided booking information 4. Specialized Agents Based on classification, messages are routed to specialized AI agents: Booking Agent: Integrated with Hospitable API to check live availability and generate quotes Transportation Agent: Uses RAG with vector databases to answer transport questions Weather Agent: Can call live weather and surf forecast APIs General Agent: Handles general inquiries with RAG access to property information Influencer Agent: Handles collaboration requests with appropriate templates Partnership Agent: Manages business inquiries 5. Response Generation & Safety All responses go through a safety check workflow before being sent: Checks for special requests requiring human intervention Flags guest complaints Identifies high-risk questions about security or property access Prevents gratitude loops (when users just say \"thank you\") Processes responses to ensure proper formatting for Instagram/Messenger 6. Response Delivery Responses are sent back to users via: Instagram API Messenger API with appropriate message types (text or button templates for booking links) Technical Implementation Details Vector Databases: Supabase Vector Store for property information retrieval Memory Management: Custom PostgreSQL chat history storage instead of n8n memory nodes This avoids duplicate entries and incorrect message attribution problems MCP node connected to Mem0Tool for storing user memories in a vector database LLM Models: Uses a combination of GPT-4.1 and GPT-4o Mini for different tasks Tools & APIs: Integrates with Hospitable for booking, weather APIs, and surf condition APIs Failsafes: Error handling, retry mechanisms, and fallback options Advanced Features Booking Flow Management: Detects when users enter/exit booking conversations Maintains booking context across multiple messages Generates custom booking links through Hospitable API Context-Aware Responses: Distinguishes between inquirers and confirmed guests Provides appropriate level of detail based on booking status Topic Switching: Detects when users change topics Preserves context from previous discussions Multi-Language Support: Can respond in whatever language the guest uses The system effectively creates a comprehensive digital concierge experience that can handle most guest inquiries autonomously while knowing when to escalate to human staff. Why I built it: Because I could! Could come in handy when I have more properties in the future but as of now it's honestly fine to answer 5 to 10 enquiries a day. Why am I posting this: I'm honestly sick of seeing posts here that are basically \"Look at these 3 nodes I connected together with zero error handling or practical functionality - now buy my $497 course or hire me as a consultant!\" This sub deserves better. Half the \"automation gurus\" posting here couldn't handle a production workflow if their life depended on it. This is just me sharing what's possible when you push n8n to its limits, aren't afraid to google stuff obsessively, and actually care about building something that WORKS in the real world with real people using it. Happy to answer any questions about how specific parts work if you're building something similar! Also feel free to DM me if you want to try the bot, won't post it here because I won't spend 10's of ‚Ç¨ on you knobheads if this post picks up! https://preview.redd.it/1ftsbfemdawe1.png?width=2398&format=png&auto=webp&s=2e8c18728ed47103d13c0a225ad88897fc51220a https://preview.redd.it/3kw8bfemdawe1.png?width=1410&format=png&auto=webp&s=d2dba7bafcf8130d8f36dd78eaf911a07b47c3fa https://preview.redd.it/j5s42femdawe1.png?width=3068&format=png&auto=webp&s=32a02a8f85bfadd55fc2a07227b0b6fc19b257d4 EDIT: Since many of you are DMing me about resources and help, I thought I'd clarify how I approached this: I built this system primarily with the help of Claude 3.7 and ChatGPT. While YouTube tutorials and posts in this sub provided initial inspiration about what's possible with n8n, I found the most success by not copying others' approaches. My best advice: Start with your specific needs, not someone else's solution . Explain your requirements thoroughly to your AI assistant of choice to get a foundational understanding. Trust your critical thinking . Even the best AI models (we're nowhere near AGI) make logical errors and suggest nonsensical implementations. Your human judgment is crucial for detecting when the AI is leading you astray. Iterate relentlessly . My workflow went through dozens of versions before reaching its current state. Each failure taught me something valuable. I would not be helping anyone by giving my full workflow's JSON file so no need to ask for it. Teach a man to fish... kinda thing hehe Break problems into smaller chunks . When I got stuck, I'd focus on solving just one piece of functionality at a time. Following tutorials can give you a starting foundation, but the most rewarding (and effective) path is creating something tailored precisely to your unique requirements. For those asking about specific implementation details - I'm happy to answer questions about particular components in the comments!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built this AI Automation to write viral TikTok/IG video scripts (got over 1.8 million views on Instagram)",
          "link": "https://old.reddit.com/r/n8n/comments/1k4u0c4/i_built_a_comprehensive_instagram_messenger/",
          "author": "oneisallxt3",
          "time": "2025-04-22T01:09:10+00:00",
          "body_text": "I've charged multiple clients $600 just to set this system up for them using their own credentials. This workflow automates the full social media content lifecycle by ingesting content from Telegram and RSS feeds, researching topics with AI, generating platform-specific social copy with images, and publishing approved content to multiple platforms after human review. This n8n workflow creates an automated social media content pipeline that handles multiple stages of content generation, management, and publishing. The workflow is comprehensive and consists of four main functional areas: 1. Content Ingestion (Input Sources) Telegram Message Processing Accepts incoming messages from Telegram through a webhook trigger Processes two types of content: Text messages (analyzed to identify topics or research needs) Audio messages (transcribed using OpenAI's transcription service) Identifies if a text message contains a URL (article link) and handles it differently than plain text queries Article Processing When a URL is detected, it fetches the article content Uses OpenAI to summarize the article's main points and extract the headline Stores the summary and URL in Airtable RSS Feed Monitoring Periodically checks configured RSS feeds (KnowTechie, AI Business, Futurism) Filters for recent content (past 3 days) Fetches full article content for each RSS item Summarizes articles using OpenAI Stores article summaries in Airtable for social media content creation 2. Research and Topic Analysis Analyzes incoming messages to identify the main topic Creates research prompts for deeper exploration Uses Perplexity AI for advanced research on identified topics With GPT-4o (with internet search capabilities), it can research current topics Summarizes research results for easier consumption Stores all research data in Airtable for content creation 3. AI Content Generation An AI Agent system that processes content waiting for social posts Retrieves writing prompts for different social platforms from Airtable Generates platform-specific content for: Twitter/X LinkedIn Facebook Instagram Blog posts Creates custom image prompts when images are needed Uses Flux AI (via GoAPI) to generate images based on prompts Updates Airtable records with generated content and changes status to \"Needs Approval\" Sends notification via Telegram when new content is ready for approval 4. Social Media Publishing Monitors Airtable for content with \"Approved\" status Determines which social platforms are targeted for each piece of content Handles both text-only and image posts Publishing capabilities include: Twitter/X (text and image posts using Twitter API) LinkedIn (personal and organization posts) Placeholder support for Facebook, Instagram, and blog posting Updates records in Airtable to \"Posted\" status with posting date Manages OAuth authentication for different platforms Data Storage and Management Uses Airtable as the central database with tables for: Social media content (with fields for source, headlines, summaries, platform-specific copy) Writing prompts for different platforms Content status tracking (Waiting for Content ‚Üí Needs Approval ‚Üí Approved ‚Üí Posted) Maintains connections between original source content and published social media posts Technical Features Uses multiple AI models (GPT-4o mini, GPT-4o with search, Claude, Perplexity) Implements OAuth authentication for social media platforms Handles both synchronous and asynchronous operations (with wait states) Uses structured output parsing to standardize AI outputs Implements branching logic for different content types and platforms Features extensive error handling and retry mechanisms This workflow essentially creates an end-to-end social media content pipeline that ingests content from multiple sources, enriches it with AI-powered research, generates platform-specific social media copy, creates images when needed, and publishes to various platforms after human approval."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jlf53k/i_built_a_fully_automated_social_media_content/",
          "author": "Drogoff1489",
          "time": "2025-03-27T22:03:44+00:00",
          "body_text": "This is the 2nd in this week from the same person. This workflow again is stolen from a Tweet: https://x.com/DataChaz/status/1936736973361930476 And I see the same in a LinkedIn post from different person on same date here: https://www.linkedin.com/posts/sofianbettayeb_breaking-this-ai-workflow-just-replaced-activity-7342808928625287170-4_tl And here: https://www.linkedin.com/posts/aaroh-jain_%F0%9D%97%A7%F0%9D%97%B5%F0%9D%97%B6%F0%9D%98%80-%F0%9D%97%94%F0%9D%97%9C-%F0%9D%97%94%F0%9D%97%B4%F0%9D%97%B2%F0%9D%97%BB%F0%9D%98%81-%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%BD%F0%9D%97%B9%F0%9D%97%AE%F0%9D%97%B0%F0%9D%97%B2%F0%9D%98%80-activity-7341831229710712832-kmWd Request: Don't call it \"I built...\" call it \"I discovered..\" and respectfully tag the original creator."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking for Help Setting Up Local Content Creation N8N Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1lw07x6/stolen_workflows_again/",
          "author": "hncvj",
          "time": "2025-07-10T01:27:54+00:00",
          "body_text": "N8N is hot right now. \nI fear though that there's a wave of false optimism fueling it. My 2 cents...\nOn one hand there are a lot of beginners, high in the idea of internet and agency money. They're all juiced up on Nate and Nick videos and might even think they're pretty good because they got a template or two working. On the other, there's the OGs, the pros who've been here all along and in so many cases predate ai and no-code. Maybe they're even a bit dismissive, looking down a bit on the newbies ;) The fresh interest is good because it's bringing talent into the industry and that is needed BUT there's a big gap between hopes, dreams, yeti videos and The Commercial Reality.\nThere seems to be a missing middle between cute 700 node workflows and ones that work reliably at scale. Youtube is great because the workflow porn is fun and gets people interested but getting from there to commercial deployment is a big step...\n- error handling\n- debugging\n- commercial volumes of data\n- handling rate limits\n- privacy and security\n- etc,\nYou never see training on this stuff unless you really look. \nThe missing middle is education/ content that gets people from \"that's so cool\" to \"I know exactly what my limitations are in this scenario in terms of both tech and ability\".\nGetting from irrational exuberance to legit professional. \nRight now, most people think they are good at this but have no idea what they don't know but they are starting \"agencies\". Some will find a way, others are going to make a mess and ruin businesses. Any advice/feedback from seasoned pros?\nAny newer pros who learned hard lessons under fire?\nAny feedback from up and comers or complete beginners?\nIs there are market for this more practical professional development or intermediate training? Full disclosure: I'm a relative n8n newcomer with enough grey hair to understand my limitations. I'll consider putting something together if a) I'm on the right track and b) I can bring together people to teach and learn\nInterested?\nAm I reading the situation right? Insights?\nIf I'm off base, let me know."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1jm0vfb/your_n8n_learning_journey_just_got_easier_i_built/",
          "author": "feliche93",
          "time": "2025-03-28T17:42:53+00:00",
          "body_text": "Hi everyone, I work in workflow automation and needed to deploy n8n for process automation and AI agent creation. While n8n's official docs are solid for basic setups, I wanted something production-ready from day one without the usual hosting costs. After spending time figuring out the optimal Google Cloud free tier configuration, I managed to get a complete setup running on an e2-micro instance that includes: SSL certificates with auto-renewal Nginx reverse proxy with security headers Automated daily backups Health monitoring and auto-restart Proper systemd service configuration UFW firewall and fail2ban protection The trickiest parts were getting the Nginx proxy configuration right for n8n's WebSocket connections, configuring Let's Encrypt for automatic SSL renewal, and setting up monitoring that actually works within the 1GB RAM constraints of the free tier. Total deployment time is about 45 minutes, and it's been rock solid for months handling my automation workflows. The whole thing runs within Google Cloud's Always Free limits, so genuinely zero hosting costs. Has anyone else tackled production n8n deployments on budget constraints? What approaches have worked for you? I'm curious about other people's experiences with self-hosting automation tools vs. using SaaS platforms. Here is the doc link: https://scientyficworld.org/how-to-setup-n8n-on-google-cloud/"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How much is my workflow worth?",
          "link": "https://old.reddit.com/r/n8n/comments/1lqj8eg/n8n_ticking_timebomb/",
          "author": "generalistai",
          "time": "2025-07-03T07:13:41+00:00",
          "body_text": null
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lgqmf5/i_created_a_complete_productionready_guide_for/",
          "author": "Another_Noob_69",
          "time": "2025-06-21T07:15:10+00:00",
          "body_text": null
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I analysed 2,000+ n8n workflows and this is what I learned",
          "link": "https://old.reddit.com/r/n8n/comments/1l1f6n8/i_analysed_2000_n8n_workflows_and_this_is_what_i/",
          "author": "Majestic-Fix-3857",
          "time": "2025-06-02T11:05:59+00:00",
          "body_text": "So I downloaded 2,050 public n8n workflows and then used claude opus 4 to help me vibe code my way through a detailed analysis. I used cursor as my code running tool, ran the claude scripts over the 2,000 JSON files, created a report, and then summarised into the below actionable doc Here is a video walkthrough of me visually going over the insights + also exploring the recommendations on the n8n canvas: https://youtu.be/BvBa_npD4Og Or if you just wanna read, here is the claude actionable report (hope you legends enjoy and find useful) -- n8n Workflow Best Practices Guide Learnings from Analyzing 2,000+ Production Workflows This guide is based on insights gathered from analyzing 2,050 production n8n workflows containing 29,363 nodes. It highlights common patterns, critical issues, and best practices for building robust, secure, and maintainable automation workflows. üìä Executive Summary Our analysis revealed critical gaps in error handling (97% of workflows lack it), security vulnerabilities (320 public webhooks without auth), and efficiency issues (7% contain unused nodes). This guide provides actionable recommendations to address these issues and build better workflows. Key Statistics: 2,050 workflows analyzed 29,363 total nodes 14.3 average nodes per workflow 97% lack error handling 472 security vulnerabilities found 34.7% are AI/ML workflows üö® Critical Issue #1: Error Handling (97% Gap) The Problem Only 62 out of 2,050 workflows (3%) have any error handling mechanism. This means when things fail, workflows silently break without notification or recovery. Best Practices 1. Always Use Error Triggers // Add an Error Trigger node at the beginning of every workflow\n// Connect it to a notification system (Email, Slack, etc.)\nError Trigger ‚Üí Format Error Message ‚Üí Send Notification 2. Implement Node-Level Error Handling For critical nodes (HTTP requests, database operations, API calls): Enable \"Continue On Fail\" for non-critical operations Add retry logic with exponential backoff Set appropriate timeout values 3. Error Handling Template Start ‚Üí Error Trigger ‚Üí Error Handler\n  ‚Üì\nMain Workflow Logic\n  ‚Üì\nCritical Operation (with retry: 3, delay: 1000ms)\n  ‚Üì\nSuccess Path / Error Path 4. Monitoring Pattern Log all errors to a centralized system Include workflow name, node name, error message, and timestamp Set up alerts for repeated failures üîí Critical Issue #2: Security Vulnerabilities The Problems 320 public webhooks without authentication 152 unsecure HTTP calls 3 workflows with hardcoded secrets Security Best Practices 1. Webhook Security // Always enable authentication on webhooks\nWebhook Settings:\n  - Authentication: Header Auth / Basic Auth\n  - Use HTTPS only\n  - Implement IP whitelisting where possible\n  - Add rate limiting 2. Secure API Communications Never use HTTP - always use HTTPS Store credentials in n8n's credential system, never hardcode Use OAuth2 when available (694 workflows do this correctly) Implement API key rotation policies 3. Authentication Methods (from most to least secure) OAuth2 - Use for major integrations API Keys - Store securely, rotate regularly Basic Auth - Only when necessary, always over HTTPS No Auth - Never for public endpoints 4. Secret Management Checklist [ ] No hardcoded API keys in Code/Function nodes [ ] All credentials stored in n8n credential manager [ ] Regular credential audit and rotation [ ] Environment-specific credentials (dev/staging/prod) üéØ Critical Issue #3: Workflow Efficiency The Problems 144 workflows with unused nodes (264 total unused nodes) 133 workflows with API calls inside loops 175 workflows with redundant transformations Efficiency Best Practices 1. Clean Architecture Input ‚Üí Validate ‚Üí Transform ‚Üí Process ‚Üí Output\n         ‚Üì (fail)\n      Error Handler 2. Avoid Common Anti-Patterns ‚ùå Bad: API in Loop Loop ‚Üí HTTP Request ‚Üí Process Each ‚úÖ Good: Batch Processing Collect Items ‚Üí Single HTTP Request (batch) ‚Üí Process Results 3. Node Optimization Remove unused nodes (7% of workflows have them) Combine multiple Set nodes into one Use Code node for complex transformations instead of chaining Set nodes Cache API responses when possible 4. Performance Guidelines Average workflow should complete in < 10 seconds Use Split In Batches for large datasets Implement parallel processing where possible (only 4.8% currently do) Add progress logging for long-running workflows ü§ñ AI/ML Workflow Best Practices (34.7% of workflows) Common Patterns Observed 346 agent-based workflows 267 multi-model workflows 201 with memory systems 0 with vector databases (RAG pattern opportunity) AI Workflow Best Practices 1. Prompt Engineering // Structure prompts with clear sections\nconst prompt = `\nSystem: ${systemContext}\nContext: ${relevantData}\nTask: ${specificTask}\nFormat: ${outputFormat}\n`; 2. Cost Optimization Use GPT-3.5 for simple tasks, GPT-4 for complex reasoning Implement caching for repeated queries Batch similar requests Monitor token usage 3. Agent Workflow Pattern Trigger ‚Üí Context Builder ‚Üí Agent (with tools) ‚Üí Output Parser ‚Üí Response\n                                ‚Üì\n                          Memory System 4. Error Handling for AI Handle rate limits gracefully Implement fallback models Validate AI outputs Log prompts and responses for debugging üìã Workflow Organization Best Practices The Problem 74.7% of workflows categorized as \"general\" Poor documentation and organization Organization Best Practices 1. Naming Conventions [Category]_[Function]_[Version]\nExamples:\n- Sales_LeadScoring_v2\n- HR_OnboardingAutomation_v1\n- DataSync_Salesforce_Daily_v3 2. Tagging Strategy Essential tags to use: Environment: prod, staging, dev Category: sales, hr, finance, it-ops Frequency: real-time, hourly, daily, weekly Status: active, testing, deprecated 3. Documentation with Sticky Notes The #1 most used node (7,024 times) - use it well: Document complex logic Explain business rules Note dependencies Include contact information 4. Workflow Structure üìù Sticky Note: Workflow Overview\n    ‚Üì\n‚öôÔ∏è Configuration & Setup\n    ‚Üì\nüîÑ Main Process Logic\n    ‚Üì\n‚úÖ Success Handling | ‚ùå Error Handling\n    ‚Üì\nüìä Logging & Monitoring üîÑ Common Node Sequences (Best Patterns) Based on the most frequent node connections: 1. Data Transformation Pattern Set ‚Üí HTTP Request (379 occurrences) Best for: Preparing data before API calls 2. Chained API Pattern HTTP Request ‚Üí HTTP Request (350 occurrences) Best for: Sequential API operations (auth ‚Üí action) 3. Conditional Processing If ‚Üí Set (267 occurrences)\nSwitch ‚Üí Set (245 occurrences) Best for: Data routing based on conditions 4. Data Aggregation Set ‚Üí Merge (229 occurrences) Best for: Combining multiple data sources üõ°Ô∏è Security Checklist for Every Workflow Before Deployment [ ] No hardcoded credentials [ ] All webhooks have authentication [ ] All external calls use HTTPS [ ] Sensitive data is encrypted [ ] Access controls are implemented [ ] Error messages don't expose sensitive info Regular Audits [ ] Review webhook authentication monthly [ ] Rotate API keys quarterly [ ] Check for unused credentials [ ] Verify HTTPS usage [ ] Review access logs üìà Optimization Opportunities 1. For Complex Workflows (17.5%) Break into sub-workflows Use Execute Workflow node Implement proper error boundaries Add performance monitoring 2. For Slow Workflows Identify bottlenecks (usually API calls) Implement caching Use batch operations Add parallel processing 3. For Maintenance Remove unused nodes (found in 7% of workflows) Consolidate redundant operations Update deprecated node versions Document business logic üéØ Top 10 Actionable Recommendations Implement Error Handling - Add Error Trigger to all production workflows Secure Webhooks - Enable authentication on all 320 public webhooks Use HTTPS - Migrate 152 HTTP calls to HTTPS Clean Workflows - Remove 264 unused nodes Batch API Calls - Refactor 133 workflows with APIs in loops Add Monitoring - Implement centralized logging Document Workflows - Use Sticky Notes effectively Categorize Properly - Move from 74.7% \"general\" to specific categories Implement Retry Logic - Add to all critical operations Regular Audits - Monthly security and performance reviews üöÄ Quick Start Templates 1. Error-Handled Webhook Workflow Webhook (with auth) ‚Üí Validate Input ‚Üí Process ‚Üí Success Response\n         ‚Üì                    ‚Üì (error)\n   Error Trigger ‚Üê Error Formatter ‚Üê Error Response 2. Secure API Integration Schedule Trigger ‚Üí Get Credentials ‚Üí HTTPS Request (with retry) ‚Üí Process Data\n                                            ‚Üì (fail)\n                                     Error Handler ‚Üí Notification 3. AI Workflow with Error Handling Trigger ‚Üí Build Context ‚Üí AI Agent ‚Üí Validate Output ‚Üí Use Result\n    ‚Üì            ‚Üì             ‚Üì            ‚Üì\nError Handler ‚Üê Rate Limit ‚Üê Timeout ‚Üê Invalid Output üìö Resources and Next Steps Create Workflow Templates - Build standard templates with error handling Security Audit Tool - Scan all workflows for vulnerabilities Performance Dashboard - Monitor execution times and failures Training Program - Educate team on best practices Governance Policy - Establish workflow development standards üéâ Success Metrics After implementing these practices, aim for: < 5% workflows without error handling 0 public webhooks without authentication 0 HTTP calls (all HTTPS) < 3% workflows with unused nodes > 90% properly categorized workflows < 10s average execution time This guide is based on real-world analysis of 2,050 production workflows. Implement these practices to build more reliable, secure, and maintainable n8n automations."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1l3fcpm/the_best_2025_ultimate_collaborative_api_list_for/",
          "author": "tektektugen",
          "time": "2025-06-04T19:56:01+00:00",
          "body_text": "Hey automation wizards! Let‚Äôs create a MASSIVE resource of the best APIs (free & paid) for n8n ‚Äî organized by use-case: Marketing, SEO, Content, DevOps, Data, CRM, SDR/Sales, Analytics, AI, and more. Reply with your favorite APIs, upvote what you love, and share real-life use cases! üìà Marketing, SEO & Content Creation [Semrush API]() ‚Äî SEO analytics, keyword research OpenAI GPT API ‚Äî Content & social caption generation [Ahrefs API]() ‚Äî Backlink & SEO analysis [Surfer SEO API]() ‚Äî On-page SEO recommendations [Google Search Console API]() ‚Äî Search analytics data [StoryChief API]() ‚Äî Multi-channel content publishing üó£Ô∏è Social Media & Influencer Twitter API (X) ‚Äî Tweets, analytics, DMs Meta Graph API (Instagram/Facebook) ‚Äî Posts, stories, insights LinkedIn API ‚Äî Posts, connections, company data Reddit API ‚Äî Posts, comments, moderation [YouTube Data API]() ‚Äî Videos, playlists, channels ü§ñ DevOps, Monitoring & CI/CD GitHub API ‚Äî Repos, issues, actions GitLab API ‚Äî CI/CD, repos, merge requests [Jenkins API]() ‚Äî Build pipeline automation [PagerDuty API]() ‚Äî Incident & alert management [UptimeRobot API]() ‚Äî Website uptime monitoring [New Relic API]() ‚Äî Performance & error tracking üîÑ Data Processing, Cleaning & Enrichment [Clearbit API]() ‚Äî Company & contact enrichment [FullContact API]() ‚Äî Identity resolution & enrichment OpenRefine API (Refine Server) ‚Äî Data cleaning (open source) [Diffbot API]() ‚Äî Web data extraction [DataForSEO API]() ‚Äî Keyword, SERP, and backlink data [Hunter.io API]() ‚Äî Find & verify email addresses üìá CRM & Customer Data HubSpot API ‚Äî CRM, contacts, deals Salesforce API ‚Äî Enterprise CRM & automation [Pipedrive API]() ‚Äî Sales pipeline management [Zoho CRM API]() ‚Äî Sales automation & leads [Freshsales API]() ‚Äî CRM & sales automation üìû SDR / Sales Prospecting [Lusha API]() ‚Äî Contact & prospect data enrichment [Cognism API]() ‚Äî B2B lead generation & enrichment [AeroLeads API]() ‚Äî Find emails & phone numbers [Voila Norbert API]() ‚Äî Email verification üß† AI, Automation, & NLP OpenAI API ‚Äî GPT-3/4, DALL-E, Whisper, etc. [Hugging Face API]() ‚Äî Text, image, audio models [Deepgram API]() ‚Äî Speech-to-text [Google Cloud Vision API]() ‚Äî Image recognition [IBM Watson API]() ‚Äî NLP, tone analysis, language tools [Clarifai API]() ‚Äî Image/video recognition with AI üõ†Ô∏è Dev Tools & Security [Snyk API]() ‚Äî Code vulnerability scanning [Rapid7 API]() ‚Äî Security analytics & automation [CircleCI API]() ‚Äî CI/CD pipeline automation [Travis CI API]() ‚Äî Build & deployment automation üìä Analytics & Product Insights [Mixpanel API]() ‚Äî User & event analytics [Amplitude API]() ‚Äî Product & behavioral analytics [Google Analytics Data API]() ‚Äî Website & app analytics [Hotjar API]() ‚Äî User feedback & session insights üí∞ Payments, Finance & E-commerce Stripe API ‚Äî Payments, subscriptions [PayPal API]() ‚Äî Digital payments [Square API]() ‚Äî POS, payments, inventory [Shopify API]() ‚Äî E-commerce automation üì¶ Hubs & API Marketplaces RapidAPI ‚Äî The world‚Äôs largest API hub [API Layer]() ‚Äî High-quality, freemium APIs API List ‚Äî Directory of public/private APIs Public APIs ‚Äî Free APIs by topic Awesome-APIs (GitHub) ‚Äî Community API list [APIs.guru]() ‚Äî OpenAPI directory üéÅ Bonus: Miscellaneous & Fun APIs [OpenWeatherMap]() ‚Äî Global weather data [Giphy API]() ‚Äî GIF search [Unsplash API]() ‚Äî Free HD images IPify ‚Äî Get public IP address JokeAPI ‚Äî Random jokes generator What‚Äôs YOUR favorite API or hub? Share your killer integrations, real-world automations, or even n8n workflow screenshots! Let‚Äôs help each other discover new APIs for every niche ‚Äî add your picks below üëá and upvote your favorites!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "The Best 2025 Ultimate !!Collaborative!! API List for n8n",
          "link": "https://old.reddit.com/r/n8n/comments/1gu1lb8/automated_shorts_and_reels_creation_with_n8n100/",
          "author": "gochapachi1",
          "time": "2024-11-18T10:14:30+00:00",
          "body_text": "Hey everyone, my last video on automated brand creatives on n8n got very good response with 1k+ views, so I decided to take it up a notch and make an automated workflow to create reels using n8n and after many stupid hurdles finally a way to create automated brand reels that too for free and no code.\nLet me know if you guys are interested."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1m788m8/6_months_as_freelancer_in_workflow_automation_my/",
          "author": "detera",
          "time": "2025-07-23T12:43:26+00:00",
          "body_text": "After six months of creating client projects ranging from ‚Ç¨500 to ‚Ç¨5,000, I have realized that it is still too early to implement automation workflows with AI that will bring a truly relevant strategic advantage. We are constantly bombarded with phrases like ‚ÄúI replaced my team with this workflow‚Äù or ‚ÄúI created a new tool that from X generates Y‚Äù and so on. The reality is that 95% of implementations built with AI, to date, have a very serious problem related to consistency of results. Products based on integrations with AI - except for content creation tools or conversational chats (Telegram, WhatsApp, etc.), which despite being often frowned upon by end users justify the investment - look great on paper, but often turn out to be disastrous in production. I cannot make predictions about the future, not least because time and again we have been surprised by the technological capabilities that emerge every day. But today, in most cases, AI remains only ‚Äúsparkling AI,‚Äù where implementation is complex and the real benefits are still too limited. There are exceptions ofc"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated shorts and Reels creation with n8n(100% free).",
          "link": "https://old.reddit.com/r/n8n/comments/1ktsuz5/new_policy_on_rn8n_no_more_dm_me_for_workflows/",
          "author": "DepartureNo2745",
          "time": "2025-05-23T19:52:08+00:00",
          "body_text": "Mods will begin removing any post farming for emails.  This is an update to Rule #2 This includes posts like: comment here for DM DM me for workflow requires email signup for template anything similar Please help the mod team out by reporting these. This does not mean you need to share your code on every post.  We are specifically targeting the newest wave of spam that has been cropping up. Any feedback on this please leave a comment and we can refine the guidance moving forward."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "6 months as freelancer in workflow automation, my takes",
          "link": "https://old.reddit.com/r/n8n/comments/1kp05jl/20_useful_focused_tools_that_work_seamlessly_with/",
          "author": "skygetsit",
          "time": "2025-05-17T18:45:40+00:00",
          "body_text": "Wanted to share a list of tools that I keep reusing with n8n that integrate well either via webhooks or APIs. I‚Äôve intentionally skipped the usual ones like Notion, Airtable etc. and here I wanted to mention more focused, often single-purpose tools that do their job well and plug neatly into automations. Form Alternative ‚Äì Typeform alternative with no response limit forms, no-code & embed anywhere, send response data via webhook Kadoa ‚Äì no-code web scraping tool with API. Works great for dynamic data extraction in recurring workflows Cohere ‚Äì text embedding and classification API, often cheaper/faster than OpenAI. Can power tagging, sentiment analysis, or semantic search in n8n pipelines Zenscrape ‚Äì affordable proxy-based scraper API for clean data extraction. Works well in cron jobs with n8n Whalesync ‚Äì syncs data between tools (e.g., Postgres ‚Üí Webflow). n8n can trigger syncs or validate before/after states Inngest ‚Äì event-driven backend functions with great webhook support. Can act as a middle layer for complex automation logic Vectara ‚Äì fully-managed RAG platform with powerful embedding, indexing, and querying APIs. Great for plug-and-play semantic search and context injection without managing your own vector DB Baserow ‚Äì open-source Airtable alternative with REST API and webhook support. Pairs great with n8n for internal tools Loops.so ‚Äì email automation platform with developer-friendly API. Cleaner and simpler than big ESPs for dev workflows NoCodeAPI ‚Äì lets you connect to third-party APIs (like Google Sheets, Instagram, etc.) without writing backend logic. Works great as an intermediary for n8n if you want to avoid authentication hassle Baseflow ‚Äì backend workflows for SaaS products, including feature flags, webhooks, and user segmentation. Useful for product-led automation Hybiscus ‚Äì API for generating charts (bar, pie, line, etc.) from raw JSON. Great for visual summaries in dashboards or reporting workflows Langfuse ‚Äì observability tool for LLM-based workflows. Combine with n8n to monitor prompt usage, flag errors, or log metrics from AI steps Tinybird ‚Äì ingest event data and query it with SQL over an API. Think real-time dashboards or anomaly detection with n8n feeding events in Highlight.io ‚Äì open source session replay and observability platform. You can pipe front-end errors or user session events into n8n for alerts, logging, or follow-up actions Fathom Analytics ‚Äì privacy-focused, GDPR-compliant website analytics. Their API lets you pull traffic stats, goal conversions, or referrer insights for reports or automations (e.g. alerting on traffic drops) CurrencyAPI ‚Äì real-time and historical currency exchange rates via JSON API. Plug into financial or pricing automation flows Instantly ‚Äì cold outreach tool with webhook support. Sync campaign stats into Google Sheets or send reply triggers into your CRM using n8n Parabola ‚Äì visual dataflow tool, good for batch processing. You can offload complex CSV/JSON transformations here and connect via webhook or API in n8n Firecrawl.dev ‚Äì headless browser crawler that handles JavaScript-heavy sites. Great for clean content extraction in automation workflows PocketBase - Lightweight open-source backend with built-in auth, file storage, and API. Great for small projects and works well with n8n via webhook or HTTP request node. Hoppscotch - Lightweight Postman alternative for manually testing API endpoints. Useful when you want to isolate whether the issue is with n8n or the API itself. Hope this list is handy!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1knbcr3/after_weeks_of_testing_i_finally_built_a_voice/",
          "author": "Smooth-Carpenter8426",
          "time": "2025-05-15T15:39:35+00:00",
          "body_text": "After testing tons of APIs, debugging for days, and tweaking flows like a madman, I finally built a fully working AI Voice Agent. üìû It calls real phone numbers. üó£Ô∏è It talks like a human using Vapi + OpenAI. ‚úÖ It qualifies leads , collects emails, and logs everything in Google Sheets and Slack No fancy UI, just pure automation with n8n , Twilio , and Vapi doing all the heavy lifting. I‚Äôve already tested it on 100+ leads and it works like a charm. Open to any feedback, suggestions, or ideas üòÑ I shared more details on my profile!Check it out if you‚Äôre curious! #BuildWithVapi https://preview.redd.it/5lubgqq7uy0f1.png?width=2942&format=png&auto=webp&s=3487f70c231839b3fccf07eef06b4adc00d7a7aa https://preview.redd.it/4vuu41g9uy0f1.png?width=2932&format=png&auto=webp&s=2f890906b6dcaf4f69185b8ee559bebe9c366c9c https://preview.redd.it/ma9q16g9uy0f1.png?width=2118&format=png&auto=webp&s=d2cd802800f6acc0645484f6449129b85df34d7c https://preview.redd.it/s90d44g9uy0f1.png?width=2940&format=png&auto=webp&s=e13f449cfb3170d1e3a5002ecb91225054583a4a https://preview.redd.it/e0lfw0g9uy0f1.png?width=2940&format=png&auto=webp&s=c33b4a2559e4230ddb9d85dab0f7d5b1f76bdb1c"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "NEW POLICY on r/n8n - No more DM me for workflows",
          "link": "https://old.reddit.com/r/n8n/comments/1kuspxb/share_your_workflow_find_your_next_workflow_dont/",
          "author": "nusquama",
          "time": "2025-05-25T03:06:55+00:00",
          "body_text": "Find yours, create yours, and share it ! https://n8nworkflows.xyz/"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "20+ Useful, Focused Tools That Work Seamlessly with n8n",
          "link": "https://old.reddit.com/r/n8n/comments/1lszo8j/my_n8n_workflows_site_update_find_quality/",
          "author": "nusquama",
          "time": "2025-07-06T12:08:57+00:00",
          "body_text": "Hi I created n8n.workflows to help you easily discover top n8n workflows‚Äîover 3000 options ! Check out templates like: AI RAG AI Chatbot Try it out and let me know what you think!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m0o40i/i_built_an_ai_workflow_that_analyzes_longform/",
          "author": "dudeson55",
          "time": "2025-07-15T17:31:23+00:00",
          "body_text": "Clipping youtube videos and twitch VODs into tiktoks/reels/shorts is a super common practice for content creators and major brands where they take their long form video content like podcasts and video streams then turn it into many different video clips that later get posted and shared on TikTok + IG Reels. Since I don‚Äôt have an entire team of editors to work on creating these video clips for me, I decided to build an automation that does the heavy lifting for me. This is what I was able to come up with: Here's how the automation works 1. Workflow Trigger / Inputs The workflow starts with a simple form trigger that accepts a YouTube video URL. In your system, you could automate this further by setting up an RSS feed for your youtube channel or podcast. 2. Initial Video Processing Request Once the URL is submitted, the workflow makes an HTTP POST request to the Vizard API to start processing the video: The request includes the YouTube video URL and processing parameters like max_clip_number - IMO the defaults actually work pretty well here so I‚Äôd leave most alone to let their system analyze for the most viral moments in the video By default, it will also add in captions. If you want to customize the style of the video / keep captions consistent with your brand you can also specify a template id in your request The API returns a project ID and initial status code that we'll use to poll for results after the video analysis completes 3. Polling Loop for Processing Status Since video processing can take significant time (especially for longer videos), the workflow uses a simple polling system which will loop over: A simple Wait node pauses execution for 10 seconds between status checks (analyzing long form videos will take a fair bit of time so this will check many times) An HTTP GET request checks the processing status using the project ID from the initial request If the status code is 1000 (still processing), the workflow loops back to wait and check again When the status reaches 2000 (completed), the workflow continues to the next section 4. Filtering and Processing Results Once the video analysis/processing is complete, I get all the video clip results back in the response and I‚Äôm able to continue with further processing. The response I get back from this include a virality score of 1/10 based on the clips potential. Clips are filtered based on virality score - I only keep clips with a score of 9 or higher In my testing, this reduces a lot of the noise / worthless clips from the output After those videos get filtered, I then share a summary message in slack with the title, virality score, and download link for each clip You can also take this further and auto-generate a social media caption + pickout ideal hashtags to use based on the content of the video and where you plan to post it. If you want to auto-post, you would use another tool like blotato to publish to each social media platform you need I personally really like using slack to review all the clips because it centralizes all clips into a single spot for me to review before posting. Costs I‚Äôm currently just on the ‚ÄúCreator‚Äù plan for Vizard which costs $29 / month for 600 upload minutes (of source YouTube material). This fits my needs for the content that I create but if you are running a larger scale clipping operation or working with multiple brands that cost is going to scale up linearly for the minutes of source material you use. Workflow Link + Other Resources YouTube video that walks through this workflow node-by-node: https://www.youtube.com/watch?v=Yb-mZmvHh-I The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "After weeks of testing, I finally built a Voice Agent that does sales calls for me",
          "link": "https://old.reddit.com/r/n8n/comments/1l2txx6/the_automation_space_is_getting_crowded/",
          "author": "Drogoff1489",
          "time": "2025-06-04T02:05:34+00:00",
          "body_text": "Hey everyone! My name‚Äôs Duncan. great to be part of this community. I got fired from my 9-5 a few weeks ago but had already fallen in love with automation. BUT it feels like everyone and their mom is jumping into automation right now. Free tutorials everywhere, copy-paste workflows, people promising to \"10x your business with AI.\" But I keep seeing the same thing over and over: people learn the technical stuff, build some workflows, then struggle to actually make money. I spent 10+ years in marketing for Apple, PlayStation, Nissan, Charles Schwab. Now I'm obsessed with automation. About 2 weeks ago I realized my marketing background is actually what sets me apart. There are thousands of people who can build a Zapier workflow. But how many understand who they want to work with, what problems they're uniquely positioned to solve, and how to attract clients instead of chasing them? The technical ability is honestly whatever at this point. The real value is understanding yourself, your unique background, and building a personal brand around that. When you position yourself as an expert in YOUR niche (not just \"automation guy #47\"), you attract inbound leads, connect with clients deeper, and can charge higher rates while hand-picking your projects. Creating YouTube content has been huge for my business. The clients my channel attracts already know who I am and what I'm about, and legit reach out to me because ‚Äúthey liked my vibe‚Äù. Everyone's learning the technical stuff. Feels like the opportunity is becoming known for solving specific problems for specific people. My DMs are always open if anyone wants to chat about getting over that initial content creation fear or building a personal brand that actually attracts the right opportunities. Would love to meet more of you! *P.S. - What's been working for you in this space? Are you finding it harder to stand out as more people jump in?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1kcytxm/best_host_for_selfhosted_n8n/",
          "author": "Unlikely-Bread6988",
          "time": "2025-05-02T11:41:38+00:00",
          "body_text": "I want to know what the best server is for self-host N8N (One that most devs know, SSH, can add postgres etc). I have sliplane atm. I'm making complex automations, and it's too hard to isolate why things don't work. I'm not a dev so have to reduce dimensionality with a server that isn't a factor. I am not doing scale. So main factor for me is ease of fixing  (easily tell what version using etc). EDIT GUIDE; WANT A FREE N8N SETUP? I setup Docker Desktop with Coudflared tunnel to self host N8N. glstr wrote ‚Äú Why not host on your own system at home using docker desktop? Super easy and if you use Cloudflared tunnel you can use all the webhooks no problem. Best price - free. This means you can have N8N totally free with no SaaS and hosting¬† costs. ‚Äù It took f‚Äôn ages to setup, but as a non-dev, I was able to do it. I wrote this prompt which I put into https://aistudio.google.com/ (gemini) and it was super duper supportive taking me through step by step. Wanted to share this in case others want to make this setup for n8n too. But also this is totally free, and I know some people have zero funds so mostly thinking of you. If you are a dev and can improve, please do so (I‚Äôm not). REQUIREMENT -¬†¬†Computer- If you self host you ned to be able to run stuff and leave your computer on whilst you run things (not like with saas) -¬†Cloudflare- free (you may need your DNS there, I forget) -¬† Install Docker Desktop (Can be pain in ass to install- need to change BIOS if doesn‚Äôt work) -¬†Notepad++ (free)- you need to make a few files on your drive -¬†¬†Powershell (free) ‚Äì you have to write a lot of code prompts (Make sure you say you are using powershell if prompts don‚Äôt work) -¬† Maybe other software but figure it out NOTE -  I setup gmail SMTP to be able to add members too (You need to setup email if you want to someone to get an invite). My prompt deals with this but you might need to make a separate prompt - I am on Windows 11 64bit - Prompts at the end are a bit irrelevant. I may not have done ideal security but I was happy to be done üòâ -  I tried to optimise for everything so you will setup postgres database in process CAUTION -¬†¬†¬†¬†¬†¬†¬†¬†¬† Major BS happened for me (Not in the guide) to install Docker. I had to update my BIOS to enable virtualisation etc. So this guide doesn‚Äôt include all the potential BS you might have to deal with installing Docker. -¬†¬†¬†¬†¬†¬†¬†¬†¬† If you use my prompt, SAVE the chat. 3x I didn‚Äôt save my chat and I had to make Gemini triangulate back to the step I was on. My prompt will make Gemini take you step by step‚Ä¶ but you will press ‚Äòback‚Äô by accident and lose the steps (unless you save the chat!!) So paste this prompt into Gemini and follow it step by step (paste images of what you're doing if you are stuck) Revised \"Ideal Prompt\" incorporating your feedback and my review: \"I am a non-developer aiming for an ideal, robust, and secure self-hosted n8n setup from the start. I'll be using my Windows 11 64-bit machine with Docker Desktop. I want to make n8n publicly accessible via https://n8n.alexanderjarvis.com using a Cloudflare Tunnel. I plan to invite other users, so user management and email notifications are important. Please provide a comprehensive, step-by-step guide, assuming I will perform each step and confirm with you. The guide must cover: I. Prerequisites & Initial Setup: 1. Verifying Docker Desktop is correctly installed and running on Windows 11. 2. Creating a dedicated folder structure for this project (e.g.,¬†C:\\n8n-setup). 3. Guidance on creating a¬†.env¬†file to store sensitive credentials (like database passwords, SMTP passwords) to be used by¬†docker-compose.yml. II. Docker Compose Configuration (docker-compose.yml): 1. Creating a¬†docker-compose.yml¬†file for: * n8n Service: * Using the¬†n8nio/n8n:latest¬†image. * Exposing port¬†5678¬†locally. * Essential Environment Variables: * Database connection details for PostgreSQL (pointing to the Postgres service). *¬†N8N_HOST=0.0.0.0 *¬†N8N_PUBLIC_URL= https://n8n.alexanderjarvis.com¬†(or¬†WEBHOOK\\_URL) * Timezone (e.g.,¬†GENERIC_TIMEZONE=America/New_York). *¬†NODE_OPTIONS=--max_old_space_size=4096¬†(for 4GB RAM allocation to Node.js). *¬†N8N_USER_MANAGEMENT_DISABLED=false¬†(to enable n8n's built-in user management). * SMTP Configuration for Gmail: All necessary¬†N8N_EMAIL_MODE,¬†N8N_SMTP_*¬†variables, including advice on using a Gmail \"App Password\". * (Optional but Recommended) N8N_BASIC_AUTH_ACTIVE=true,¬†N8N_BASIC_AUTH_USER,¬†N8N_BASIC_AUTH_PASSWORD¬†for an additional security layer. * PostgreSQL Service: * Using a specific image like¬†postgres:15. * Environment variables for¬†POSTGRES_USER,¬†POSTGRES_PASSWORD,¬†POSTGRES_DB. * Volume for persistent PostgreSQL data (postgres_data). * Health check for PostgreSQL. 2. Named Docker Volumes: Defining¬†n8n_data¬†(for¬†/home/node/.n8n) and¬†postgres_data. III. Launching n8n & Initial Configuration: 1. Running¬†docker-compose up -d. 2. Verifying both containers (n8n, postgres) are running successfully (docker ps). 3. Checking logs for any errors (docker-compose logs n8n¬†and¬†docker-compose logs postgres). 4. Accessing n8n locally at¬†http://localhost:5678. 5. Setting up the n8n Owner/Admin account through the n8n web interface on first launch. 6. Briefly testing SMTP by inviting a test user or using a workflow that sends an email. IV. Cloudflare Tunnel Setup: 1. Cloudflare account prerequisites (active account, alexanderjarvis.com added as a site). 2. Installing and authenticating the¬†cloudflared¬†daemon/service on Windows 11. 3. Creating a Cloudflare Tunnel (preferably via the Cloudflare Zero Trust dashboard for ease of use). 4. Configuring the Tunnel: * Public Hostname:¬†n8n.alexanderjarvis.com. * Service: Pointing to¬†http://localhost:5678. 5. Verifying n8n is accessible via https://n8n.alexanderjarvis.com . 6. Ensuring HTTPS is enforced by Cloudflare. V. User Management in n8n: 1. How to invite new users to n8n using their email addresses. 2. Brief overview of user roles/permissions in n8n (if applicable). VI. Maintenance & Best Practices: 1. How to safely update n8n and PostgreSQL Docker images. 2. Backup strategy: * How to use¬†pg_dump¬†for the PostgreSQL database. * How to back up the¬†n8n_data¬†Docker volume. 3. Basic PostgreSQL maintenance (mention¬†VACUUM,¬†ANALYZE¬†for awareness). VII. Troubleshooting: * Common issues and how to diagnose them using Docker logs. Please explain technical terms simply and ensure all passwords/secrets are handled securely (e.g., via the¬†.env¬†file rather than hardcoded in¬†docker-compose.yml). I will confirm completion of each major step.\""
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Share your workflow ! Find your next workflow ! Don't buy it !",
          "link": "https://old.reddit.com/r/n8n/comments/1llq2o1/i_built_an_ai_thats_smarter_than_most_real_estate/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-27T10:03:15+00:00",
          "body_text": "Your real estate agent spends hours pulling comps, calculating market trends, and writing up reports. I'm going to show you the n8n blueprint for an AI that does it all automatically‚Äîfrom scraping a listing to generating a full investment analysis and emailing it to your team. This isn't a simple, single-prompt bot. This is a real, multi-stage AI agent. Here‚Äôs the 4-part architecture to build it. Module 1: The Data Collector (Scraping & Storing)\nThis module's job is to gather the raw data. The Workflow: Use an HTTP Request node to fetch data from a real estate URL (like Zillow, Redfin, etc.). Then, use n8n's built-in \"HTML Extract\" node or a \"Code\" node to parse the key information you need: price, square footage, address, property type, etc. The Output: Use the Google Sheets node to append this structured data into a new row. Over time, you'll build a powerful dataset of property listings. Module 2: The Number Cruncher (Data Analysis)\nThis module does the objective math. The Workflow: This is the most complex part. For true analysis, you need to calculate averages, medians, and trends from all the data in your Google Sheet. The most robust way to do this in n8n is with the Code node. You can run a small Python script using the Pandas library to perform all these calculations. The Output: The output of this node isn't a recommendation; it's a clean set of statistics: average price, average price/sqft, number of recent sales, etc. Module 3: The AI Analyst (Insight Generation)\nThis module takes the numbers and finds the meaning. Don't use one giant prompt; use a chain of specific AI calls. AI Call #1 (Market Condition): Feed the stats from Module 2 to an AI Node. Prompt: \"Given these market stats, determine if it is currently a buyer's or seller's market and briefly explain why.\" AI Call #2 (Investment Opportunities): Feed the raw property list and the calculated average price/sqft to another AI Node. Prompt: \"From this list of properties, identify the top 3 with the best investment potential based on a low price per square foot compared to the average.\" AI Call #3 (Final Report): Combine all the previous insights and stats and feed them to a final AI Node. Prompt: \"Synthesize all the following information into a single, comprehensive real estate market analysis report.\" Module 4: The Communicator (Email Automation)\nThis module drafts and sends your weekly report. The Workflow: Take the final report generated by the AI Analyst. Feed it to one last AI Node with the prompt: \"You are a professional real estate analyst. Based on the following report, draft a professional weekly summary email for my team. Use clear headers and bullet points, and include a subject line like 'This week's Real Estate Market Insights'.\" The Output: Send the AI-generated email content using the Gmail or another email node. By architecting your workflow in these distinct modules, you can build an incredibly powerful AI agent that provides real, data-driven insights, moving far beyond what a simple chatbot can do. What's the first data source you'd plug into a real estate agent like this?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lunl3h/stealing_someone_elses_project_and_claiming_it_as/",
          "author": "hncvj",
          "time": "2025-07-08T12:43:31+00:00",
          "body_text": "Saw a post here today about an \"I built an AI that runs a viral ASMR channel hands-free.\" Bold claim. Except‚Ä¶ it's not their work. The entire idea, workflow, and execution is taken straight from Nate Herk' YouTube channel and his free S kool community, without a single credit or change. Original here: https://youtu.be/RvuPRX-b7Lc?si=a2iHUF3pqPWDdhAN What's worse? They didn't even bother innovating or acknowledging. Just slapped a \"Workflow - Code Not Included\" banner and called it theirs. This trend of stealing real creators' work for fake clout needs to stop. It's not \"inspired,\" it's not \"repurposed\", it's a direct plagiarism, plain and simple. It's my humble request, respect the creators. Don't build your karma on someone else's grind. What do you guys think?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "My n8n Workflows Site ( update )- Find Quality Automations Easily!",
          "link": "https://old.reddit.com/r/n8n/comments/1kgxgo4/i_made_a_docker_compose_for_n8n_queue_mode_with/",
          "author": "conor_is_my_name",
          "time": "2025-05-07T13:32:53+00:00",
          "body_text": "UPDATE: Check the 2nd branch if you want to use cloudflared. TLDR:  Put simply, this is the pro level install that you have been looking for, even if you aren't a power user (yet). I can't be the only one who has struggled with queue mode (the documentation is terrible), but I finally nailed it.  Please take this code and use it so no one else has to suffer through what I did building it.  This version is better in every way than the regular install.  Just leave me a GitHub star. https://github.com/conor-is-my-name/n8n-autoscaling First off, who is this for? Anyone who wants to run n8n either locally or on a single server of any size (ram should be 2gb+, but I'd recommend  8gb+ if using with the other containers linked at the bottom, the scrapers are ram hogs) You want simple setup Desire higher parallel throughput (it won't make single jobs faster) Why is queue mode great? No execution limit bottlenecks scales up and scales down based on load if a worker fails, the jobs gets reassigned Whats inside: A Docker-based autoscaling solution for n8n workflow automation platform. Dynamically scales worker containers based on Redis queue length. No need to deal with k8s or any other container scaling provider, a simple script runs it all and is easily configurable. Includes Puppeteer and Chrome built-in for pro level scraping directly from the n8n code node.  It makes it so much easier to do advanced scraping compared to using the community nodes.  Just paste your puppeteer script in a regular code node and you are rolling.  Use this in conjunction with my Headful Chrome Docker that is linked at the bottom for great results on tricky websites. Everything installs and configures automatically, only prerequisite is having docker installed.  Works on all platforms, but the puppeteer install requires some dependency tweaks if you are using a ARM cpu.  (an AI will know what to do for the dependency changes) Install instructions: Windows or Mac: Install the docker desktop app. Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d Linux: Follow the instructions for the Docker Convenience Script . Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d That's it.  (But remember to change the passwords) Default settings are for 50 simultaneous workflow executions.  See GitHub page for instructions on changing the worker count and concurrency. A tip for those who are in the process of leveling up their n8n game: move away from google sheets and airtable - they are slow and unstable embrace Postgres - with AI its really easy, just ask it what to do and how to set up the tables Tested on a Netcup 8 core 16gb Root VPS - RS 2000 G11.  Easily ran hundreds of simultaneous executions.  Lower end hardware should work fine too, but you might want to limit the number of worker instances to something that makes sense for your own hardware. If this post inspires you to get a server, use this link .  Or don't, just run this locally for free. I do n8n consulting, send me a message if you need help on a project. check out my other n8n specific GitHub repos: Extremely fast google maps scraper - this one is a masterpiece web scraper server using crawlee for deep scraping - I've scraped millions of pages using this Headful Chrome Docker with Puppeteer for precise web scraping and persistent sessions - for tricky websites and those requiring logins"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Servers, Hosting, & Tech Stuff",
          "link": "https://old.reddit.com/r/n8n/comments/1ln8txv/got_called_out_for_a_3k_n8n_build_heres_the_real/",
          "author": "EquipmentTall6735",
          "time": "2025-06-29T07:30:36+00:00",
          "body_text": "Hey folks, I recently shared a post here about an HR automation agent I built using n8n, and mentioned it was sold for ‚Ç¨3000.\nIt got some strong reactions ‚Äî some called it fake, messy, and flagged the fact that it showed a free n8n Cloud account. Fair enough. I realized the way I presented it lacked full context.\nSo I wanted to write a clean, transparent version here. üëã Who I am: I'm a solo founder running an AI automation agency since August 2024.\nWe help recruiters, coaches, solar teams, and other operationally heavy businesses automate repetitive workflows using tools like n8n, LLMs, Make, etc. I‚Äôm not a developer by background. I focus more on: understanding the business logic structuring the workflows and making them usable for the client I work with a dev who handles technical setup and handover.\nMost of our clients come from LinkedIn (not Reddit), and I‚Äôm not posting here to pitch anything. What actually happened in that project: The screenshot in the old post was from a free n8n Cloud trial, taken by my dev while testing The final delivery was hosted on Railway (clean, secure, and fully functional) The scope included: Process mapping AI prompt logic Sheets + Slack integration Workflow config & testing 1:1 delivery & documentation The client didn‚Äôt pay ‚Ç¨3K just for a workflow ‚Äî it was a packaged MVP that solved a real ops problem. Why I originally posted: To be honest, I wanted to share something that might inspire someone else in this community.\nBut I should‚Äôve explained better ‚Äî because on Reddit, framing matters. I get that now, and I appreciate the feedback. Even the harsh stuff made me reflect. Final Note: If you‚Äôre early in your n8n or no-code journey ‚Äî don‚Äôt wait for perfection.\nMessy, half-built workflows can still solve real problems and create value.\nJust be honest, keep improving, and let the market validate your work ‚Äî not Reddit threads. Thanks to everyone here who gave honest feedback.\nI‚Äôm here to learn, build, and hopefully share better from now on üôå"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that analyzes long-form YouTube videos and generates short form clips optimized for TikTok / IG Reels / YT Shorts",
          "link": "https://old.reddit.com/r/n8n/comments/1lkbx2b/i_built_this_ai_automation_that_generates_viral/",
          "author": "dudeson55",
          "time": "2025-06-25T17:28:56+00:00",
          "body_text": "There‚Äôs been a huge trend of Bigfoot / Yeti vlog videos exploding across IG and TikTok all created with Veo 3 and I wanted to see if I could replicate and automate the full process of: Taking a simple idea as input Generate an entire story around that simple idea Turn that into a Veo 3 prompt Finally generate those videos inside n8n using FAL. Had a lot of fun building this and am pretty happy with final output . Here‚Äôs the workflow breakdown. 1. Input / Trigger The input and trigger for this workflow is a simple Form Trigger that has a single text field. What goes into here is a simple idea for for what bigfoot will be doing that will later get turned into a fully fleshed-out story. It doesn‚Äôt need any crazy detail, but just needs something the story can be anchored around. Here‚Äôs an example of one of the ones I used earlier to give you a better idea: jsx\nBigfoot discovers a world war 2 plane crash while on a hike through the deep forest that he hasn't explored yet 2. The Narrative Writer Prompt The next main node of this automation is what I call the ‚Äúnarrative writer‚Äù. Its function is very similar to a storyboard artist where it will accept the basic ideas as input and will generate an outline for each clip that needs to be generated for the story. Since Veo 3 has a hard limit of 8 seconds per video generation, that was a constraint I had to define here. So after this runs, I get an outline that splits up the story into 8 distinct clips that are each 8 seconds long. I also added in extra constraints here like what I want bigfoots personality to be like on camera to help guide the dialog and I also specified that I want the first out of the 8 clips to always be an introduction to the video. Here‚Äôs the full prompt I am using: ```jsx Role: You are a creative director specializing in short-form, character-driven video content. Goal: Generate a storyboard outline for a short vlog based on a user-provided concept. The output must strictly adhere to the Persona, Creative Mandate, and Output Specification defined below. [Persona: Bigfoot the Vlogger] Identity: A gentle giant named \"Sam,\" who is an endlessly curious and optimistic explorer. His vibe is that of a friendly, slightly clumsy, outdoorsy influencer discovering the human world for the first time. Voice & Tone: Consistently jolly, heartwarming, and filled with childlike wonder. He is easily impressed and finds joy in small details. His language is simple, and he might gently misuse human slang. PG-rated, but occasional mild exasperation like \"geez\" or \"oh, nuts\" is authentic. His dialog and lines MUST be based around the \"Outdoor Boys\" YouTube channel and he must speak like the main character from that Channel. Avoid super generic language. Physicality: An 8-foot male with shaggy, cedar-brown fur ( #6d6048 ) and faint moss specks. His silhouette is soft and \"huggable\" due to fluffy fur on his cheeks and shoulders. Features soft, medium-amber eyes, rounded cheeks, a broad nose, and short, blunt lower canines visible when he smiles. He holds a simple selfie stick at all times. [Creative Mandate] Visual Style: All scenes are shot 16:9 from a selfie-stick perspective held by Bigfoot. The style must feel like authentic, slightly shaky \"found footage.\" The camera is always on him, not his POV. Narrative Goal: The primary objective is to create audience affection. Each scene must showcase Bigfoot's charm through his gentle humor, endearing discoveries, or moments of vulnerability. The 8-scene arc must have a satisfying and heartwarming payoff. [Output Specification] Structure: Provide a storyboard with exactly 8 sequential scenes, formatted as shown below. Introduction Rule: Scene 1 must be a direct-to-camera introduction. In it, Bigfoot should enthusiastically greet his viewers (e.g., \"Hey everybody!\" or \"Hi friends!\") and briefly state the goal or adventure for the vlog, based on the user's concept. Duration: Each scene represents 8 seconds of footage. Content per Scene: For each scene, provide a single, descriptive paragraph. This paragraph must seamlessly weave together the visual action, Bigfoot's expressions, and his spoken dialogue. Each scene you create should be part of a cohesive story. Example Formats: SCENE 1 (0:00-0:08): Sam the Bigfoot grins warmly into his selfie stick, the background slightly out of focus. He waves a large, furry hand and says, \"Hey friends, Sam here! Today, we're going to try and build a brand new shelter out of... well, whatever we can find! Wish me luck!\" SCENE 3 (0:32-0:40): Sam holds up a rusty, bent bicycle wheel, peering at it curiously with his head tilted. He tries to spin it, a look of concentration on his face. \"Hmm. This is a weird-looking rock. Very... holey. Not good for a wall, I don't think.\" Task Using the rules above, create the storyboard outline for the following concept: {{ $json['Bigfoot Video Idea'] }} ``` And here‚Äôs what the output looks like after this prompt runs: ```jsx\nTitle: Finding History in the Deep Woods - WWII Plane Discovery!\nConcept: Bigfoot discovers a world war 2 plane crash while on a hike through the deep forest Scene 1 | 0:00-0:08\nSam the Bigfoot grins warmly into his selfie stick, towering evergreens framing his fuzzy shoulders. He adjusts his grip on the stick with both hands and beams at the camera. \"Hey everybody, Sam here! Today we're heading way deep into the back country for some serious exploring. You never know what kind of treasures you might stumble across out here!\" Scene 2 | 0:08-0:16\nSam trudges through dense undergrowth, his selfie stick bouncing slightly as he navigates around massive fir trees. Moss hangs like curtains around him, and his amber eyes dart curiously from side to side. \"Man, this forest just keeps getting thicker and thicker. Perfect day for it though - nice and cool, birds are singing. This is what I call the good life, friends!\" Scene 3 | 0:16-0:24\nSam suddenly stops mid-stride, his eyes widening as he stares off-camera. The selfie stick trembles slightly in his grip, showing his surprised expression clearly. \"Whoa, hold on a second here...\" He tilts his shaggy head to one side, his mouth forming a perfect 'O' of amazement. \"Guys, I think I'm seeing something pretty incredible through these trees.\" Scene 4 | 0:24-0:32\nSam approaches cautiously, pushing aside hanging branches with his free hand while keeping the camera steady. His expression shifts from wonder to respectful awe as he gets closer to his discovery. \"Oh my goodness... friends, this is... this is an old airplane. Like, really old. Look at the size of this thing!\" His voice drops to a whisper filled with reverence. Scene 5 | 0:32-0:40\nSam extends the selfie stick to show himself standing next to the moss-covered wreckage of a WWII fighter plane, its metal frame twisted but still recognizable. His expression is one of deep respect and fascination. \"This has got to be from way back in the day - World War Two maybe? The forest has just been taking care of it all these years. Nature's got its own way of honoring history, doesn't it?\" Scene 6 | 0:40-0:48\nSam crouches down carefully, his camera capturing his gentle examination of some scattered debris. He doesn't touch anything, just observes with his hands clasped respectfully. \"You know what, guys? Someone's story ended right here, and that's... that's something worth remembering. This pilot was probably somebody's son, maybe somebody's dad.\" His usual cheerfulness is tempered with genuine thoughtfulness. Scene 7 | 0:48-0:56\nSam stands and takes a step back, his expression shifting from contemplation to gentle resolve. He looks directly into the camera with his characteristic warmth, but there's a new depth in his amber eyes. \"I think the right thing to do here is let the proper folks know about this. Some family out there might still be wondering what happened to their loved one.\" Scene 8 | 0:56-1:04\nSam gives the camera one final, heartfelt look as he begins to back away from the site, leaving it undisturbed. His trademark smile returns, but it's softer now, more meaningful. \"Sometimes the best adventures aren't about what you take with you - they're about what you leave behind and who you help along the way. Thanks for exploring with me today, friends. Until next time, this is Sam, reminding you to always respect the stories the forest shares with us.\"\n``` 3. The Scene Director Prompt The next step is to take this story outline and turn it into a real prompt that can get passed into Veo 3. If we just took the output from the outline and tried to create a video, we‚Äôd get all sorts of issues where the character would not be consistent across scenes, his voice would change, the camera used would change, and things like that. So the next step of this process is to build out a highly detailed script with all technical details necessary to give us a cohesive video across all 8 clips / scenes we need to generate. The prompt here is very large so I won‚Äôt include it here (it is included inside the workflow) but I will share the desired output we are going for. For every single 8 second clip we generate, we are creating something exactly like that will cover: Scene overview Scene description Technical specs like duration, aspect ratio, camera lens Details of the main subject (Bigfoot) Camera motion Lighting Atmosphere Sound FX Audio Bigfoot dialog Really the main goal here is to be as specific as possible so we can get consistent results across each and every scene we generate. ```jsx SCENE 4  ‚ñ∏  ‚ÄúTrail to the Lake‚Äù  ‚ñ∏  0 ‚Äì 8 s Selfie-stick POV. Bigfoot strolls through dense cedar woods toward a sun-sparkled lake in the distance. No spoken dialogue in this beat‚Äîjust ambient forest sound and foot-fall crunches. Keeps reference camera-shake, color grade, and the plush, lovable design. SCENE DESCRIPTION POV selfie-stick vlog: Bigfoot walks along a pine-needle path, ferns brushing both\nsides. Sunbeams flicker through the canopy. At the 6-second mark the shimmering\nsurface of a lake appears through the trees; Bigfoot subtly tilts the stick to\nhint at the destination. TECHNICAL SPECS ‚Ä¢ Duration‚ÄÉ8 s‚ÄÉ‚Ä¢ 29.97 fps‚ÄÉ‚Ä¢ 4 K UHD‚ÄÉ‚Ä¢ 16 : 9 horizontal ‚Ä¢ Lens‚ÄÉ24 mm eq, ∆í/2.8‚ÄÉ‚Ä¢ Shutter‚ÄÉ1/60 s (subtle motion-blur) ‚Ä¢ Hand-held wobble amplitude cloned from reference clip (small ¬±2¬∞ yaw/roll). SUBJECT DETAILS  (LOCK ACROSS ALL CUTS) ‚Ä¢ 8-ft male Bigfoot, cedar-brown shaggy fur #6d6048 with faint moss specks. ‚Ä¢ Fluffier cheek & shoulder fur ‚Üí plush, huggable silhouette. ‚Ä¢ Eyes: soft medium-amber, natural catch-lights only ‚Äî no glow or excess brightness. ‚Ä¢ Face: rounded cheeks, gentle smile crease; broad flat nose; short blunt lower canines. ‚Ä¢ Hands: dark leathery palms, 4-inch black claws; right paw grips 12-inch carbon selfie stick. ‚Ä¢ Friendly, lovable, gentle vibe. CAMERA MOTION 0 ‚Äì 2 s‚ÄÉStick angled toward Bigfoot‚Äôs chest/face as he steps onto path. 2 ‚Äì 6 s‚ÄÉSmooth forward walk; slight vertical bob; ferns brush lens edges. 6 ‚Äì 8 s‚ÄÉStick tilts ~20¬∞ left, revealing glinting lake through trees; light breeze ripples fur. LIGHTING & GRADE Late-morning sun stripes across trail; teal-olive mid-tones, warm highlights,\ngentle film grain, faint right-edge lens smudge (clone reference look). ATMOSPHERE FX ‚Ä¢ Dust motes / pollen drifting in sunbeams. ‚Ä¢ Occasional leaf flutter from breeze. AUDIO BED  (NO SPOKEN VOICE) Continuous forest ambience: songbirds, light wind, distant woodpecker;\nsoft foot-crunch on pine needles; faint lake-lap audible after 6 s. END FRAME Freeze at 7.8 s with lake shimmering through trees; insert one-frame white-noise\npop to preserve the series‚Äô hard-cut rhythm.\n``` 3. Human in the loop approval The middle section of this workflow is a human in the loop process where we send the details of the script to a slack channel we have setup and wait for a human to approve or deny it before we continue with the video generation. Because generation videos this way is so expensive ($6 per 8 seconds of video), we want to review this before before potentially being left with a bad video. 4. Generate the video with FAL API The final section of this automation is where actually take the scripts generated from before, iterate over each, and call in to FAL‚Äôs Veo 3 endpoint to queue up the video generation request and wait for it to generate. I have a simple polling loop setup to check its status every 10 seconds which will loop until the video is completely rendered. After that is done, the loop will move onto the next clip/scene it needs to generate until all 8 video clips are rendered. Each clip get‚Äôs uploaded to a Google Drive I have configured so my editor can jump in and stitch them together into a full video. If you wanted to extend this even further, you could likely use the json2video API to do that stitching yourself, but that ultimately depends on how far or not you want to automate. Notes on keeping costs down Like I mentioned above, the full cost of running this is currently very expensive. Through the FAL API it costs $6 for 8 seconds of video so this probably doesn‚Äôt make sense for everyone‚Äôs use case. If you want to keep costs down, you can still use this exact same workflow and drop the 3rd section that uses the FAL API. Each of the prompts that get generated for the full script can simply be copied and pasted into Gemini or Flow to generate a video of the same quality but it will be much cheaper to do so. Workflow Link + Other Resources Github n8n workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=C65c8itWvf4 Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l1ta88/sharing_first_version/",
          "author": "dlt-namaste",
          "time": "2025-06-02T20:49:32+00:00",
          "body_text": "üîß [SHARING] First version of my complete workflow to generate a weekly marketing schedule based on a single theme Hey folks,\nThis is the first version of a workflow I‚Äôm using to generate my company‚Äôs entire content production based on a single weekly campaign theme. üëâ The flow starts by reading a theme from a specific tab in Google Sheets. From that point on: A main trigger (manual or scheduled) kicks off the process. n8n fetches the theme and triggers several sub-workflows, including: ‚úÖ WhatsApp content generation (multiple message models)\n‚úÖ Email creation with text + image (using ChatGPT + DALL¬∑E/OpenAI Image API)\n‚úÖ Blog posts with automatic SEO and publishing to WordPress\n‚úÖ Trello card creation with links to all generated documents\n‚úÖ All outputs go through formatting in Google Docs, Gmail integration, and automatic storage üß† The architecture is structured by content type: WhatsApp (emotional and sales-oriented messages) Email marketing (two versions with visuals and CTAs) Blog (fully structured with SEO) LinkedIn (adapted for technical or institutional tone) Pitch/VSL (long-form persuasive format) üß© All nodes are designed to receive the same theme input and return the result to the central workflow. A Merge node syncs the outputs before the flow finishes and frees up the theme for the next cycle. üì£ If anyone has questions, suggestions, or wants to review the logic, I‚Äôm open to feedback and improvements! n8n #workflow #automation #opensource #chatgpt #marketingautomation #trello #gmail #googleapps"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "The automation space is getting CROWDED!",
          "link": "https://old.reddit.com/r/n8n/comments/1la87n8/i_built_a_fullstack_ai_content_factory_with_n8n/",
          "author": "Glass-Ad-6146",
          "time": "2025-06-13T05:38:24+00:00",
          "body_text": "Hey folks, First we use a couple of Agents from Flowise and prep all text plus image prompts for media pipeline part After months of hacking, iterating, and way too many late-night ‚Äúwhat if we automate this too?‚Äù sessions, I‚Äôm stoked to share our latest project: a full-stack, multi-agent content production system built on n8n, OpenAI, Flowise, and a bunch of other bleeding-edge tools. This isn‚Äôt just another ‚Äúscrape and post‚Äù bot. Think of it as a digital assembly line‚Äîone that can plan, research, write, edit, generate images, publish, and even handle feedback‚Äîall orchestrated by a network of specialized AI agents and automation nodes. And yes, I‚Äôm giving away the whole playbook (canvas, tutorial, and resource pack) for free at the end. What Does This Actually Do? At its core, this system is a content production powerhouse that can: Take in a single prompt or topic Spin up a full research and content plan (think: outlines, angles, SEO keywords) Assign tasks to specialized agents (e.g., ‚Äúresearcher,‚Äù ‚Äúwriter,‚Äù ‚Äúeditor,‚Äù ‚Äúimage creator‚Äù) Generate long-form articles, social posts, and stunning images‚Äîautomatically Review, refine, and even re-prompt itself if something‚Äôs off Publish everywhere from WordPress to social media, or just drop assets in your cloud storage All of this runs on a single orchestrated n8n canvas, where every step is modular and remixable. The High-Level Workflow (How the Magic Happens) Media Pipeline with FAL Developer Cloud Models + OpenAI gpt-image-1 in base 64 that we send to AWS 1. The Kickoff: Everything starts with a ‚Äúmain prompt‚Äù or assignment. You can trigger this with a webhook, a form, or even schedule it to run on a content calendar. 2. Content Planning & Research: The system fires up a research agent (using Flowise + OpenAI) to fetch real-time web data, analyze trending topics, and profile the ideal content persona. It then builds a detailed outline and keyword map, pulling in SEO and ‚ÄúPeople Also Ask‚Äù data. 3. Multi-Agent Task Assignment: Here‚Äôs where it gets wild: the orchestrator splits the job into subtasks‚Äîlike research, drafting, editing, and image generation. Each is routed to a dedicated agent (LLM, API, or even a human-in-the-loop if needed). Research nodes pull fresh context from the web Drafting nodes generate humanized, non-AI-sounding copy Editorial nodes check for tone, clarity, and even add CTAs Image agents create hyper-realistic visuals (with prompt engineering and multiple AI models) 4. Quality Control & Feedback Loops: If any output doesn‚Äôt hit the mark, the system can auto-reprompt, escalate to a human for review, or even run A/B tests on different drafts. Feedback is logged and used to improve future runs. 5. Multi-Channel Publishing: Once the final assets are ready, the system can publish to your CMS, send to email, post on socials, or just drop everything in a cloud folder for your team. 6. Resource Pack & Full Transparency: Every run generates a full resource pack‚Äîdrafts, images, SEO data, and even the logs‚Äîso you can audit, remix, and learn from every campaign. Why Build All This? We use Agents and 3rd party service to compile media content Honestly? Because content ops are a pain. Scaling high-quality, multi-format content without burning out your team (or yourself) is brutal. We wanted a system that‚Äôs flexible, transparent, and easy to upgrade as new tools drop‚Äîwithout getting locked into a single vendor or platform. Plus, building this in n8n means you can remix, fork, or extend any part of the workflow. Want to swap in a new LLM? Add a feedback node? Trigger from Discord? Go for it. Want to Build Your Own? Here‚Äôs Everything You Need (Free): üõ†Ô∏è FREE Resource Pack (Canvas, Prompts, Docs): build.tesseract.nexus USE CODE L7PY90Q üéì Full n8n Masterclass (Step-by-Step Course): academy.tesseract.nexus/course/n8n-full-stack-masterclass-2025-building-generative-multimedia-production-system üì∫ FREE Full YouTube Tutorial (Workflow Walkthrough): youtu.be/BjbwStzgkkU No paywall, no catch‚Äîjust sharing what we‚Äôve learned and hoping it helps more builders level up. Curious about multi-agent orchestration, prompt engineering, or how we handle error recovery? Want to see the actual n8n JSON or discuss how to fork this for your own use case? Drop your questions below or DM me. Let‚Äôs build smarter, not harder. üöÄ ‚Äî Vadim (Tesseract Nexus / AutoAgentFlow) TL;DR: We built a modular, multi-agent content production system with n8n, LLMs, and agent orchestration‚Äînow open source and fully documented. Free canvas, full course, and YouTube walkthrough linked above."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lw54l2/viral_reels_script_generator_workflow_n8n/",
          "author": "Melodic_Incident_716",
          "time": "2025-07-10T05:53:02+00:00",
          "body_text": "TLDR: I created this viral reels trend workflow on n8n and I am honestly surprised how accurate it is and that it is actually giving me good results on my youtube channel. Long: So I have been learning n8n in the last 3 months and I have been building my own content as well, and it was a real pain in identifying what content to make. So i thought let me try to automate what i usually do. Picture 1: I setup a workflow which gets the latest 100 posts on r/technology and then from there I use an AI agent to process information and get the list of top trending topics based on upvotes comments etc. Now post that i connected with Google DERP API to get the latest trends and validate these results with these trends and give me 3 ideas a day. I then setup the automation to send these detiala along with a script on how i should make the reel. This prompt for the script was pretty complex and reqiured a lot of rework ( I needed the AI to provide me a hook starting line, with ideas for the video, then overall script , the format of the reel and also a good closing line). While making this i was thinking great in theory but would it really worked and to my surprise it was pretty fascinating and i tried it and i actually got really decent results. Considering i am on a mission to build in public i am willing to share the json here based on the interest."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Best host for self-hosted N8N",
          "link": "https://old.reddit.com/r/n8n/comments/1k15s3m/is_learning_n8n_time_well_spent/",
          "author": "Accurate-Jump-9679",
          "time": "2025-04-17T06:01:23+00:00",
          "body_text": "Over the last couple months, I've invested a lot of time learning to make workflows in n8n, mostly centered on research processes and content creation. It has reached a stage where I'm getting pretty useful results. However, I just finally got trial access to Manus and I've also checked out Google Deep Research. It's not difficult to imagine that the same type of workflows can soon be set up much more simply with natural language on these autonomous AI agent frameworks + tasked to run on a schedule. Also, tasks like scraping are tricky to set up properly in n8n, whereas these agents seem to breeze through all kinds of websites with ease. These results are impressive and leave me wondering if the stuff I'm doing will soon be obsolete."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1jlslg1/i_built_an_aipowered_lead_gen_machine_that/",
          "author": "hashpanak",
          "time": "2025-03-28T11:10:53+00:00",
          "body_text": "I‚Äôve not charged clients $500+ to set up this system, I am a coder, but I do get clients for my SaaS and MVP services this way. I have turned that into an n8n template, what I do with code. You probably are way better at selling this and so you can sell it to your clients. This fully automated pipeline scrapes leads, qualifies/disqualifies them with AI, and sends tailored cold emails at scale‚Äîwhile letting you review everything before hitting ‚Äúsend.‚Äù How It Works This workflow automates lead generation, qualification, and outreach in 4 stages: 1. Lead Collection (Scraping) Telegram Integration : Trigger workflows via Telegram messages (e.g., ‚ÄúFind SaaS companies under 100 employees‚Äù ). AI-Powered Apollo Search : An AI agent generates targeted Apollo URLs to scrape decision-makers (founders, CTOs, marketing VPs) based on your ideal customer profile. Apify Scraper : Automatically exports up to 50k leads (free $5 credits included) with LinkedIn/Twitter profiles, emails, and company data. Google Sheets Sync : All leads populate a spreadsheet with status tracking (sent/disqualified). 2. AI Qualification Auto-Disqualification Rules : Instantly filters out mismatched leads (e.g., companies that don't fit any of the offers you provide). LinkedIn & Website Scraping : Pulls data to assess lead relevance using Serper APIs. AI Decision Agent : Uses GPT-4o to analyze scraped data and decide if a lead is worth pursuing, with reasons (e.g., ‚ÄúDisqualified: Competes directly with your services‚Äù ). 3. Hyper-Personalized Outreach Dynamic Email Generator : Creates unique emails for each lead using: Company website/LinkedIn insights Target with multiple custom offers from a single lead list (e.g., Some may benefit from your automation others from your seo services etc) More columns added means more context about the lead. Train in your style Resend Integration : Sends emails from your domain (avoids spam folders) with open/click tracking. Or simply upload the lead list to Instantly if you want to use your own email service. 4. Follow-Up & Tracking Automated Status Updates : Marks emails as ‚Äúsent‚Äù or ‚Äúdisqualified‚Äù in Google Sheets. Scalable Sequences : Ready-to-add nodes for follow-ups, swtich google sheet with your favorite CRM if you prefer that. Key Features No-Code Setup : Fully built in n8n . Free Tools : Uses Apify ($5 free credits), Serper (2500 free searches), and Resend (100 free emails/day). Customizable Rules : Tweak disqualification logic, email templates, and scraping parameters in minutes. Human-in-the-Loop : Review AI-generated emails before sending. Why Turn cold outreach into a $5k/mo service by selling ‚Äúdone-for-you‚Äù lead gen. Replace expensive tools like Apollo ($99/mo) or HubSpot ($800/mo) with a free automated system. Actual client result : Explanation: I‚Äôve posted the full breakdown of n8n workflow here üöÄ Automate 500+ Personalized Emails DAILY with AI (Full Lead Gen Tutorial: n8n) - YouTube https://preview.redd.it/l88tezgt3fre1.png?width=2088&format=png&auto=webp&s=ea067b685f6d31e1efbb1d2e9f06fe635202c884 # EDIT: To people saying cold email does not work, this is dead etc in the comments. Results after 24 hours of setting this up for a client. Of course, what happens after that depends on your offer and if it is valuable. https://preview.redd.it/knsyfo5rkvre1.jpg?width=1290&format=pjpg&auto=webp&s=d5a3fe65af46bcc260672d58a2d11bb9b0b1c48a And many more, in this example the leads generated are using a template in instantly and not personalisation, but the same concept."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI that's smarter than most real estate agents. Here's the n8n blueprint.",
          "link": "https://old.reddit.com/r/n8n/comments/1kml8st/i_made_a_google_maps_scraper_designed/",
          "author": "conor_is_my_name",
          "time": "2025-05-14T17:33:42+00:00",
          "body_text": "Hey everyone! Today I am sharing my custom built google maps scraper.  It's extremely fast compared to most other maps scraping services and produces more reliable results as well. I've spent thousands of dollars over the years on scraping using APIFY, phantom buster, and other services.  They were ok but I also got many formatting issues which required significant data cleanup. Finally went ahead and just coded my own.  Here's the link to the GitHub repo, just give me a star: https://github.com/conor-is-my-name/google-maps-scraper It includes example json for n8n workflows to get started in the n8n nodes folder.  Also included the Postgres code you need to get basic tables up and running in your database. These scrapers are designed to be used in conjunction with my n8n build linked below.  They will work with any n8n install, but you will need to update the IP address rather than just using the container name like in the example. https://github.com/conor-is-my-name/n8n-autoscaling If using the 2 together, make sure that you set up the external docker network as described in the instructions.  Doing so makes it much easier to get the networking working. Why use this scraper? Best in class speed and reliability You can scale up with multiple containers on multiple computers/servers, just change the IP. A word of warning: Google will rate limit you if you just blast this a million times.  Slow and steady wins the race.  I'd recommend starting at no more than 1 per minute per IP address.  There are 1440 minutes in a day x 100 results per search = 144,000 results per day. https://preview.redd.it/ozwqpz31is0f1.png?width=644&format=png&auto=webp&s=a0b472a70594a4b3fbd72ffc7e429e7ae7acbb11 Example Search: Query = Hotels in 98392 (you can put anything here) language = en limit results = 1 (any number) headless = true [\n  {\n    \"name\": \"Comfort Inn On The Bay\",\n    \"place_id\": \"0x549037bf4a7fd889:0x7091242f04ffff4f\",\n    \"coordinates\": {\n      \"latitude\": 47.543005199999996,\n      \"longitude\": -122.6300069\n    },\n    \"address\": \"1121 Bay St, Port Orchard, WA 98366\",\n    \"rating\": 4,\n    \"reviews_count\": 735,\n    \"categories\": [\n      \"Hotel\"\n    ],\n    \"website\": \"https://www.choicehotels.com/washington/port-orchard/comfort-inn-hotels/wa167\",\n    \"phone\": \"3603294051\",\n    \"link\": \"https://www.google.com/maps/place/Comfort+Inn+On+The+Bay/data=!4m10!3m9!1s0x549037bf4a7fd889:0x7091242f04ffff4f!5m2!4m1!1i2!8m2!3d47.5430052!4d-122.6300069!16s%2Fg%2F1tfz9wzs!19sChIJidh_Sr83kFQRT___BC8kkXA?authuser=0&hl=en&rclk=1\"\n  },"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1iu2x5q/jobs_in_your_inbox_100_free_job_matching/",
          "author": "LilFingaz",
          "time": "2025-02-20T16:44:57+00:00",
          "body_text": "Scrape RSS feeds of your favorite job sites >> extract relevant data >> filter by date & job titles >> update database (Sheets) >> Send raw data to \"Job Matching Agent\" >> AI sets match percentage and advice for all filtered jobs >> updates database (Sheets) >> sends email with top 3 matches How cool is this set-up? Ideally, job platforms charge would charge a fee... if its free, it's limited. Attached images: Workflow Sample email Sheets Data Quick Video (for the skeptics) Workflow Email Update Sheets (Database) Quick video (for the skeptics) Update: You can download the JSON from here ."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Stealing someone else's project and claiming it as your own? That‚Äôs the new low for karma farmers",
          "link": "https://old.reddit.com/r/n8n/comments/1m4suad/i_feel_like_i_have_all_the_keys_to_succeed_except/",
          "author": "SOP-Ia",
          "time": "2025-07-20T16:14:09+00:00",
          "body_text": "Yo guys, I‚Äôm building a business around process optimization for agencies, focused on:\n‚û°Ô∏è Creating SOPs,\n‚û°Ô∏è Automations with tools like Zapier, Make, N8N, Airtable, etc.,\n‚û°Ô∏è Structuring workflows to pull founders out of operational chaos. I mainly target small agencies (marketing, sales, SMMA, creative‚Ä¶) that are stuck in the day-to-day and want to scale without burning out. Over the past 2 months, I‚Äôve been:\n    ‚Ä¢ Building an offer,\n    ‚Ä¢ Deep diving into the tools,\n    ‚Ä¢ Creating content on Instagram,\n    ‚Ä¢ Doing a few free audits‚Ä¶ And still: 0 clients. Nothing. Nada. ‚∏ª üòì Where I‚Äôm at right now Right now I‚Äôm in this weird space where:\n    ‚Ä¢ I know I‚Äôm bringing real value (or at least I used to be sure),\n    ‚Ä¢ But I‚Äôm doubting my offer,\n    ‚Ä¢ I know I can transform an agency,\n    ‚Ä¢ But no one‚Äôs paying me to do it. I feel like I‚Äôm 1% away from unlocking something, but I just can‚Äôt seem to find that 1%. No clear sales process.\nI‚Äôm doing outreach, but with no real method.\nPosting content, but no traction. I‚Äôve got the vision: a scalable, productized business that truly empowers agencies,\nbut I haven‚Äôt found the right messaging, the right angle, or the right sales mechanism yet. ‚∏ª üîÅ My question For those who‚Äôve been through this (or are in it now):\nHow did you pivot or unlock your first bit of client traction?\nWas it the offer? The channel? The pricing? The positioning? I‚Äôm open to all feedback.\nEven the hard stuff.\nI just want to move forward."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built TikTok brainrot generator, includes automatic AI script generation",
          "link": "https://old.reddit.com/r/n8n/comments/1lbwq0f/i_built_tiktok_brainrot_generator_includes/",
          "author": "SignatureSharp3215",
          "time": "2025-06-15T09:58:06+00:00",
          "body_text": "I've written a script to generate education brainrot videos. You will write a question, and then a dialogue between two people is generated, to educate and challenge the topic around the question. Example output video below: https://reddit.com/link/1lbwq0f/video/wggylxnad27f1/player I got the workflow from X user /paoloanzn, but the script was full of hard-coded decisions, and some poor decisions in my opinion. So I enhanced it and switched to using ElevenLabs. The workflow can be found at Github | TeemuSo | n8n-brainrot-generator . Steps to use workflow Connect your Google Drive Add Anthropic API key Authenticate ElevenLabs, replace voiceId in ElevenLabs API calls Add Json2Video API key Add two images to /assets folder in Google Drive, they will be alternating Crop background videos to /background-clips folder Update 'Create Render Object' script Update the Anthropic system prompt to generate the type of script you want Run workflow Write your question to the chat. I hate reels, but I love this type of dialogue as an educational methodology."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1ktvso1/holy_shit_wtf_is_going_on_with_ai_agents_insane/",
          "author": "_artemisdigital",
          "time": "2025-05-23T21:59:10+00:00",
          "body_text": "Kek I'm almost questioning this whole automation thing when it includes AI (not talking about regular automations. I spent the past 3 months learning relentlessly so many things to catch up, as a non-tech person. Github, Docker, Vibe coding (lol), Web crawling, Nextjs, Gen AI, Python scripts, RAGs, and of course, the most important n8n. But I never took time to make proper testing on n8n until now. Today, I actually spent some time testing a VERY simple workflow for a \"groceries list AI assistant\". I tried mini o4 / Gemini 2.5 Flash / 2.5 Pro / Gpt 4.1 / and Claude 3.7. I tried LLM temperature from 0.2 to 0.9 I have a 4 x 7 Google sheet. Very small. See picture attached. My test was simply asking a bunch of questions, either to get info, or perform simple actions, and check the % of success vs failure. Absolutely ALL MODELS except Claude 3.7 miserably failed . I'm shocked. (I am not shilling Claude btw, in fact I've always preferred Gemini 2.5 on a daily basis). The level of unreliability for a simple, minuscule groceries list is just slapping me in the face. How do you sell complex automation involving IA to customers, whereas even gigantic models like Gemini 2.5 Pro can't tell you how many fruits you have left in your groceries list ? lmao wtf seriously. One time out of 3-4, it will make shit up instead of systematically look up the sheet, or add duplicates despite the system prompt. The questions to which I expected all LLMs to pass very easily. How naive: - how much milk left ?\n- do we have fruits left ?\n- what about cutleries ?\n- add 4 cakes at 41 bucks\n- add 4 knives at 5 bucks\n- what kind of meat do we have left ? My System (OLD) Prompt (read the edit at the bottom before answering): STRICTLY FOLLOW THESE RULES AT ALL COSTS.\n\nYou help me to do data entry for my groceries list or get information about it.\n- You are very concise and to the point\n- NEVER make shit up. \n- ALWAYS check the Google sheet before answering.\n- DO NOT HALLUCINATE\n- NEVER add a new row / a new item to the list, before checking if the item doesn't already exist.\n- Use your common sense. E.g. \"Apple\" and \"Apples\" is the same thing. Use that principle of common sense all the time.\n- Adding a new row is only in the case the item truly doesn't exist yet.\n- if the item I mention exists, check the list and update accordingly if I say so.\n- item duplicates are not allowed.\n\nSTRICTLY FOLLOW THESE RULES AT ALL COSTS. Note: the questions are INTENTIONALLY brief and not descriptive because I expect the LLM to understand such trivial requests. So now I'm wondering how could these agents be good for ANYTHING professional, if they can't handle such a trivial task ? Am I doing it completely wrong ? https://preview.redd.it/agoid06f1m2f1.png?width=2482&format=png&auto=webp&s=56fc2c1c1ca634d88ecdb9d79ca3ce97ca1835de ------- EDIT 2: I updated the prompt again. It keeps screwing it up royally... There is just no way of stopping that hallucination. ONLY Claude does the work perfectly for all questions. The first request usually works with other LLMs. The second and third start failing as the LLM refuses to use the \"read\" Tool and makes shit up instead. It will just decide to completely ignore your prompt lol. New system Prompt (Column names have been changed accordingly - screenshot is old): **IMPORTANT: Always check Google Sheets before responding**\n\nYou are a Grocery List Manager for Google Sheets with columns: ItemName, Quantity, UnitPrice.\n\n**GOOGLE SHEETS INTEGRATION:**\nWhen performing actions, use these exact parameter names in your tool calls:\n- `item_name` - for the item name (matches ItemName column)\n- `quantity` - for the quantity value (matches Quantity column)  \n- `unit_price` - for the unit price (matches UnitPrice column)\n\nThe system uses \"Append or Update Row\" operation with \"ItemName\" as the matching column. This means:\n- If item_name exists: updates that row's quantity and unit_price\n- If item_name doesn't exist: creates new row with all three values\n\n**WORKFLOW:**\n1. ANALYZE user intent (add, update, remove, query)\n2. SEARCH sheet using fuzzy matching and category matching\n3. EXECUTE action or ASK for clarification when ambiguous\n4. RESPOND with specific quantities and details\n\n**MATCHING RULES:**\n- Handle singular/plural: knife=knives, apple=apples\n- Match categories: fruits=apple/banana, vegetables=carrot/tomato\n- Use semantic proximity for categorization\n- Check exact name AND category before creating new items\n\n**AMBIGUITY HANDLING:**\nWhen adding items that exist, ASK: \"You have [X] [item]. Should I: (1) Add [Y] more (total=[X+Y]), (2) Replace with [Y], or (3) Create separate entry?\"\n\n**RESPONSE FORMAT:**\n- Queries: \"Yes/No, [item]: [quantity], [item]: [quantity]\"\n- Actions: \"[Item] now has [quantity] at [unit price] each\"\n- Clarifications: \"Current: [item] has [quantity]. Should I: [options]?\"\n\n**CONSTRAINTS:**\n- Use exact parameter names: item_name, quantity, unit_price\n- Include exact quantities in all responses\n- Never give vague responses\n- Match items intelligently before creating new entries"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Holy Shit WTF is going on with AI agents' insane unreliability??",
          "link": "https://old.reddit.com/r/n8n/comments/1lpgp5h/this_is_what_actual_agentic_workflow_looks_like/",
          "author": "Conscious_Tension811",
          "time": "2025-07-01T23:23:54+00:00",
          "body_text": "1x http tool 2x subflow 1x code node 2x postgres tool not a gigantic linear workflow, agent knows what to do based on input, updates into db, fetchs knowledge, calls API/third-party web services."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lw5c1s/i_built_a_twitter_time_machine_and_now_im/",
          "author": "Puzzleheaded-Box4526",
          "time": "2025-07-10T06:06:02+00:00",
          "body_text": "Okay so I was supposed to be working but instead made this ridiculous thing that lets me see what went viral on Twitter/X on ANY day. Like, literally ANY day. And now I can't stop looking up random stuff üòÖ What this dumb thing does: - You pick ANY date - Type ANY topic - It finds all viral tweets from that day - Sorts them by views (because drama) - Dumps everything in a spreadsheet The rabbit holes are ENDLESS. Like, did you know there's viral content about literally everything? I found heated debates about: - The correct way to fold socks (2 days ago) - People arguing about the best way to organize their desk (this got WAY too intense) How I made it: - n8n for the workflow (because copy-paste is my coding style) - Twitter/X API (my accidental treasure) - Google Sheets (it ain't fancy but it works) Is it productive? Absolutely not. Am I going to keep using it? You bet! Current uses: - Finding out what I missed while touching grass - Tracking random food wars through time - Discovering why things went viral last week - Professional procrastination research Drop a random topic + date in the comments and I'll tell you what drama you missed!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "this is what actual agentic workflow looks like",
          "link": "https://old.reddit.com/r/n8n/comments/1lnwxc6/the_ultimate_n8n_cheat_sheet_20_essential_nodes/",
          "author": "HabibNayeem",
          "time": "2025-06-30T02:59:44+00:00",
          "body_text": "What is a Node in n8n? In n8n, a node is a fundamental building block that represents a single action or operation within a workflow. Think of nodes as individual components that perform specific tasks, such as: Making an API call to retrieve data Sending an email or notification Processing or transforming data Making decisions based on conditions Connecting to databases or external services Each node has: Inputs: Data flowing into the node from previous steps Outputs: Processed data that flows to the next node Configuration: Settings that define how the node operates Connections: Links that connect nodes together to create a workflow Nodes are connected together in a visual flow to create automated workflows. Data flows from one node to another, with each node performing its specific function and passing the results to the next node in the sequence. 1. HTTP Request The HTTP Request node allows you to make API calls to any web service. It supports GET, POST, PUT, DELETE, and other HTTP methods. This is essential for integrating with external services, fetching data from APIs, or sending data to webhooks. You can configure headers, authentication, and request bodies to interact with virtually any REST API. 2. Webhook The Webhook node creates an endpoint that can receive HTTP requests, making it perfect for triggering workflows from external applications. It's commonly used for receiving notifications from services like GitHub, Slack, or payment processors. The node can handle different HTTP methods and extract data from incoming requests to use in your workflow. 3. Gmail The Gmail node provides comprehensive email automation capabilities. You can send emails, read messages, search through your inbox, download attachments, and manage labels. It's particularly useful for automating email responses, processing incoming emails, or creating email-based workflows for customer support or notifications. 4. Google Sheets This node connects to Google Sheets for reading, writing, and manipulating spreadsheet data. You can append rows, update cells, create new sheets, or retrieve data for processing. It's excellent for data logging, report generation, inventory management, or creating simple databases that non-technical users can access. 5. Slack The Slack node enables comprehensive Slack integration, allowing you to send messages, create channels, upload files, and manage users. It's perfect for team notifications, automated alerts, status updates, or creating chatbots. You can also listen for specific events or messages to trigger automated responses. 6. Code (JavaScript) The Code node lets you write custom JavaScript to process data, perform complex calculations, or implement custom logic that isn't available in other nodes. It's invaluable for data transformation, custom validation, mathematical operations, or integrating with JavaScript libraries for specialized functionality. 7. Split In Batches This node processes large datasets by breaking them into smaller, manageable chunks. It's essential when working with APIs that have rate limits or when processing thousands of records. The node ensures your workflow doesn't overwhelm external services and helps maintain performance when dealing with bulk operations. 8. Set The Set node allows you to create, modify, or remove data properties in your workflow. You can add new fields, transform existing data, set default values, or restructure data formats. It's fundamental for data preparation, cleaning, and ensuring data consistency across different parts of your workflow. 9. If The If node provides conditional logic to create different execution paths based on specific criteria. You can compare values, check for existence of data, or evaluate complex conditions. This node is crucial for building intelligent workflows that respond differently based on the data they process. 10. MySQL The MySQL node connects to MySQL databases for reading, writing, and executing custom SQL queries. It's essential for applications that need to interact with relational databases, whether for data storage, retrieval, or complex data operations. You can perform inserts, updates, deletes, and complex joins. 11. Wait The Wait node introduces delays or pauses in your workflow execution. You can wait for a specific time, until a certain date, or for a webhook to be called. This is useful for implementing timeouts, scheduling delayed actions, or creating workflows that need to pause for external processes to complete. 12. Merge The Merge node combines data from multiple sources or workflow branches. You can merge data by appending, combining based on keys, or creating unions of datasets. It's essential for workflows that collect data from multiple sources and need to consolidate it for further processing or output. 13. Function Similar to the Code node but specifically designed for data transformation using JavaScript functions. It's optimized for processing workflow data and provides easier access to n8n's data structure. Use it for complex data manipulations, custom formatting, or implementing business logic that requires programming. 14. Google Drive The Google Drive node manages files and folders in Google Drive. You can upload files, download documents, create folders, share files, or search for content. It's perfect for document management workflows, backup automation, or integrating file operations into larger business processes. 15. Telegram The Telegram node sends and receives messages through Telegram's API. You can send text messages, photos, documents, or create interactive bot conversations. It's excellent for notifications, alerts, customer support bots, or creating automated communication channels with users or team members. 16. Schedule Trigger This trigger node starts workflows at specified times or intervals. You can schedule workflows to run daily, weekly, monthly, or at custom intervals using cron expressions. It's essential for automated reports, data synchronization, maintenance tasks, or any recurring automation needs. 17. Item Lists The Item Lists node manipulates arrays and lists within your workflow data. You can split, aggregate, sort, filter, or perform other list operations. It's crucial when working with multiple records, processing collections of data, or when you need to restructure data arrays for further processing. 18. Switch The Switch node routes workflow execution to different paths based on data values or conditions. Unlike the If node which creates binary paths, Switch can create multiple execution routes. It's perfect for workflows that need to handle different types of data or route processing based on categories or classifications. 19. Loop Over Items This node executes a set of operations for each item in a dataset individually. It's essential when you need to process items one by one, especially when dealing with APIs that don't support batch operations or when each item requires individual handling with different logic. 20. Airtable The Airtable node connects to Airtable bases for database operations. You can create, read, update, and delete records in Airtable tables. It's excellent for managing structured data with a user-friendly interface, creating CRM systems, project management workflows, or maintaining databases that need to be accessible to non-technical team members. What's your go-to n8n node? Drop a comment below and share which node you use most in your workflows! New to n8n? Start with the HTTP Request and Webhook nodes - they're the gateway to endless automation possibilities. Pro tip: Bookmark this list and refer back to it when planning your next workflow. Each of these nodes can save you hours of manual work when used correctly. üëÜ Upvote if this helped you! Let's help more people discover the power of n8n automation."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1loanw9/looking_for_review/",
          "author": "Abhi-Age-2050",
          "time": "2025-06-30T15:35:33+00:00",
          "body_text": "I have built a workflow that automates posting on LinkedIn. In a custom way requested by the user! I need some suggestions This is a custom workflow where, User on providing link work close scrapes the relevant content from the static website.\nReturns the website content for content generation using custom prompts provided to me by the client. In the End the Reviewer reviews the content and posts it on LinkedIn after approval. Approval involves for the media being posted and the content once both are done it post's on LinkedIn.‚úÖ How can I optimize workflow?\nHow can I optomize token usage?\nAny other opinions on what more could be done?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a Twitter Time Machine and now I'm addicted to finding random viral drama from any day üò≠",
          "link": "https://old.reddit.com/r/n8n/comments/1lvvk9f/my_first_automation_ever_i_built_a_full_timesheet/",
          "author": "Substantial-Wallaby6",
          "time": "2025-07-09T21:55:36+00:00",
          "body_text": "Hey everyone! I just finished building my very first automation , and I wanted to share it with you. I made it with N8N, completely for free, just to discover what it could do ‚Äî and wow, I learned so much!üôå üß† The Contex t In my company, every month, consultants send their timesheets to a shared email address like¬†[ timesheets@company.com ](mailto: timesheets@company.com ). The process to handle them was manual and repetitive: download the file, rename it properly, upload to Dropbox, update a Google Sheet‚Ä¶ So I thought: let‚Äôs automate the whole thing from scratch. ‚öôÔ∏è The Flow (in n8n) Here‚Äôs how it works step-by-step: Trigger: Gmail (New Message with Label): The flow starts when a new email is received with a specific Gmail label (timesheets). Fetch Email Details: I extract the subject, attachments, and other metadata from the message. Check Attachment Count: If the email doesn‚Äôt have exactly one attachment , I send a Slack alert. Some consultants send both timesheet and invoice together, but that‚Äôs not allowed. Extract the Target Month: From the subject (which can be in EN, FR, or NL), I extract the month of the timesheet (e.g. 2025-07). Check Google Sheet: I look into the Google Sheet tab that matches the target month and try to find the consultant using their full name. Merge Info: I merge the consultant info with the month to make sure I have the right line and sheet. Generate Clean Filename: I rename the file in the format: Timesheet - 2025-07 - Firstname Lastname.pdf. Convert Excel to PDF (if needed): If the attachment is an Excel file, I send it to CloudConvert to get a clean PDF version. Upload to Dropbox: Finally, the PDF gets uploaded to the correct /timesheet/YYYY-MM/ folder in Dropbox ‚Äî all done automatically. üîÅ Extra: Error Handlin g The flow also includes a small error-handling system. If: No attachment is found in the email, or More than one attachment is detected (we only accept one timesheet per email) üëâ then two things happen automatically: The consultant receives an error email explaining the issue and asking them to resend their timesheet correctly. A Slack notification is sent to the fleet manager responsible for timesheets, via a bot in a dedicated channel called #timesheet-bug. Additionally, if there is any conversion error with CloudConvert (e.g. when converting an Excel file to PDF), the same Slack alert mechanism is triggered ‚Äî so the fleet manager stays informed and can act if needed. üß™ Resul t This whole process used to take several minutes per email. Now, it runs silently in the background, and we only get alerts if something goes wrong. I‚Äôm really proud, especially since it‚Äôs my first automation ever! What do you think ? Happy to hear your thoughts on this and answer any questions."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l0nj7m/i_built_a_workflow_that_generates_longform_blog/",
          "author": "LargePay1357",
          "time": "2025-06-01T12:16:04+00:00",
          "body_text": "First, an AI agent goes back and forth with you to finalize the title and headings. Then, it calls the next workflow to write the blog post. Code for both workflows: https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json Video tutorial: https://www.youtube.com/watch?v=6AgfcKUMkVY"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "The Ultimate n8n Cheat Sheet: 20 Essential Nodes Every Automation Enthusiast Should Know üî•",
          "link": "https://old.reddit.com/r/n8n/comments/1kbb1br/i_built_an_ai_marketing_team_that_actually_works/",
          "author": "Smart-Echo6402",
          "time": "2025-04-30T07:55:31+00:00",
          "body_text": "I just built something that completely blew my mind and had to share it with you all. I've been messing around with AI tools for a while now, but this is the first time I've put something together that feels like magic.\nThe Problem I Was Trying to Solve\nI was getting crushed trying to keep up with all my marketing content. Between writing blog posts, creating graphics, editing videos, and posting on LinkedIn, I was spending like 20+ hours a week just on content creation. And I'm not even a full-time marketer - this was just a side project!\n¬†What I Built Instead\nI created this AI agent that I can just message on Telegram, and it handles everything. It's like having a whole marketing team in your pocket. Here's what it can do:\n- Create images (like really good ones)\n- Edit those images if I want changes\n- Write blog posts with actual research\n- Generate LinkedIn posts that don't sound like a robot wrote them\n- Create short videos with sound effects\n- Keep track of everything it creates\nLet Me Show You (with Real Examples)\nI recorded myself using it live, and here's what happened:\n1. For images:¬†I just typed \"Create an image for a flyer for cat food with a flash sale\", and about a minute later, boom - I had a professional-looking flyer with a cat eating food and sale messaging. Then I asked it to \"make that more realistic\", and it edited the image to look like a real photo.\n2. For blog content:¬†I asked to \"Create a blog post about the effect of sleep on productivity\", and I wrote this whole article with stats and references, plus it made a matching graphic. The post was good, not just AI garbage.\n3. For videos:¬†This is where it gets crazy. I asked to \"Create a video of a beaver building a house\", and it made this 20-second video showing the beaver gathering materials and building its home, with sound effects and everything. It's not Pixar quality, but it's pretty damn impressive for something I created with a text message.\n¬†How It Works\nI'm not a developer, so I kept this as simple as possible. The system uses:\n- Openai's image stuff for creating/editing images\n- Runway for the video generation\n- 11 Labs for sound effects\n- Creatmate for putting the videos together\n- Google Drive and Sheets to keep track of everything\nThe Cost (Because I Know You're Wondering)\nHere's what it costs me to run this:\n- Openai image stuff: About 20 cents per image/edit\n- Runway videos: Around a buck per video (it makes four 5-second clips)\n- 11 Labs sound effects: $5/month starter plan\n- Creatmate: I'm on the free trial with 50 credits (good for about 16 videos)\n- N8N (the automation platform): About $27/month\nSo for less than $50/month, I've got what feels like a full marketing team. That's less than I was paying for just one subscription to a video editing tool.\nHow to Build Your Own (If You're Interested)\nI've put all the resources together in my community for free. You'll need:\n1. Seven workflow templates (the main agent + six tools)\n2. A Google Sheets template for keeping track of everything\n3. A Creatmate template for the videos\n4. Various API keys (Openai, Runway, etc.)\nWhy This Matters\nLook, I'm not saying this replaces human creativity. But it sure as hell replaces a lot of the grunt work. Instead of:\n- Spending hours writing blog posts\n- Fighting with Canva to make graphics\n- Learning video editing software\n- Managing a dozen different subscriptions\nI can just message my AI agent and get everything done while I focus on the actual strategy and ideas.\nWant to Try It?\nI've put together a guide with all the templates and setup instructions. If you're interested in building your own AI marketing team, check out the resources in my video\nWhat would you want your AI marketing team to create first? I'm curious what you all would use this for!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1js4npo/which_ai_tool_works_best_for_writing_n8n_nodes/",
          "author": "radiogen",
          "time": "2025-04-05T14:27:39+00:00",
          "body_text": "I‚Äôm diving deeper into n8n (self-hosted setup) and want to streamline my workflow creation process using AI tools. I‚Äôve been experimenting with a few options like ChatGPT, Claude, DeepSeek, and even local setups like Ollama, but I‚Äôm curious‚Äîwhat do you all actually use and recommend for writing n8n nodes (like custom JS code) and designing the overall logic of workflows?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking for Review!",
          "link": "https://old.reddit.com/r/n8n/comments/1lineqi/how_you_can_setup_and_use_n8n_as_your_backend_for/",
          "author": "dudeson55",
          "time": "2025-06-23T17:52:36+00:00",
          "body_text": "I wanted to put together a quick guide and walk through on how you can use n8n to be the backend that powers your mobile apps / web apps / internal tools. I‚Äôve been using Lovable a lot lately and thought this would be the perfect opportunity to put together this tutorial and showcase this setup working end to end. The Goal - Clone the main app functionality Cal AI I thought a fun challenge for this would be cloning the core feature of the Cal AI mobile app which is an AI calorie tracker that let‚Äôs you snap a picture of your meal and get a breakdown of all nutritional info in the meal. I suspected this all could be done with a well written prompt + an API call into Open AI‚Äôs vision API (and it turns out I was right). 1. Setting up a basic API call between lovable and n8n Before building the whole frontend, the first thing I wanted to do was make sure I could get data flowing back and forth between a lovable app and a n8n workflow. So instead of building the full app UI in lovable, I made a very simple lovable project with 3 main components: Text input that accepts a webhook url (which will be our n8n API endpoint) File uploader that let‚Äôs me upload an image file for our meal we want scanned Submit button to make the HTTP request to n8n When I click the button, I want to see the request actually work from lovable ‚Üí n8n and then view the response data that actually comes back (just like a real API call). Here‚Äôs the prompt I used: jsx\nPlease build me a simple web app that contains three components. Number one, a text input that allows me to enter a URL. Number two, a file upload component that lets me upload an image of a meal. And number three, a button that will submit an HTTP request to the URL that was provided in the text input from before. Once that response is received from the HTTP request, I want you to print out JSON of the full details of the successful response. If there's any validation errors or any errors that come up during this process, please display that in an info box above. Here‚Äôs the lovable project if you would like to see the prompts / fork for your own testing: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 2. Setting up the n8n workflow for our backend Next up we need to setup the n8n workflow that will be our ‚Äúbackend‚Äù for the app. This step is actually pretty simple to get n8n working as your backend, all you need is the following: A Webhook Trigger on your workflow Some sort of data processing in the middle (like loading results from your database or making an LLM-chain call into an LLM like GPT) A Respond To Webhook node at the very end of the workflow to return the data that was processed On your initial Webhook Trigger it is very important that you change the Respond option set to Using ‚ÄòRespond To Webhook‚Äô Node. If you don‚Äôt have this option set, the webhook is going to return data immediately instead of waiting for any of your custom logic to process such as loading data from your database or calling into a LLM with a prompt. In the middle processing nodes, I ended up using Open AI‚Äôs vision API to upload the meal image that will be passed in through the API call from lovable and ran a prompt over it to extract the nutritional information from the image itself. Once that prompt finished running, I used another LLM-chain call with an extraction prompt to get the final analysis results into a structured JSON object that will be used for the final result. I found that using the Auto-fixing output parser helped a lot here to make this process more reliable and avoided errors during my testing. Meal image analysis prompt: ```jsx\n<identity>\nYou are a world-class AI Nutrition Analyst.\n</identity> <mission>\nYour mission is to perform a detailed nutritional analysis of a meal from a single image. You will identify the food, estimate portion sizes, calculate nutritional values, and provide a holistic health assessment.\n</mission> Analysis Protocol\n1. Identify: Scrutinize the image to identify the meal and all its distinct components. Use visual cues and any visible text or branding for accurate identification.\n2. Estimate: For each component, estimate the portion size in grams or standard units (e.g., 1 cup, 1 filet). This is critical for accuracy.\n3. Calculate: Based on the identification and portion estimates, calculate the total nutritional information for the entire meal.\n4. Assess & Justify: Evaluate the meal's overall healthiness and your confidence in the analysis. Justify your assessments based on the provided rubrics. Output Instructions\nYour final output MUST be a single, valid JSON object and nothing else. Do not include json markers or any text before or after the object. Error Handling\nIf the image does not contain food or is too ambiguous to analyze, return a JSON object where confidenceScore is 0.0 , mealName is \"Unidentifiable\", and all other numeric fields are 0 . OUTPUT_SCHEMA json\n{\n  \"mealName\": \"string\",\n  \"calories\": \"integer\",\n  \"protein\": \"integer\",\n  \"carbs\": \"integer\",\n  \"fat\": \"integer\",\n  \"fiber\": \"integer\",\n  \"sugar\": \"integer\",\n  \"sodium\": \"integer\",\n  \"confidenceScore\": \"float\",\n  \"healthScore\": \"integer\",\n  \"rationale\": \"string\"\n} Field Definitions\n*   ** mealName : A concise name for the meal (e.g., \"Chicken Caesar Salad\", \"Starbucks Grande Latte with Whole Milk\"). If multiple items of food are present in the image, include that in the name like \"2 Big Macs\".\n*   ** calories : Total estimated kilocalories.\n*   ** protein : Total estimated grams of protein.\n*   ** carbs : Total estimated grams of carbohydrates.\n*   ** fat : Total estimated grams of fat.\n*   ** fiber : Total estimated grams of fiber.\n*   ** sugar : Total estimated grams of sugar (a subset of carbohydrates).\n*   ** sodium : Total estimated milligrams (mg) of sodium.\n*   ** confidenceScore : A float from 0.0 to 1.0 indicating your certainty. Base this on:\n    *   Image clarity and quality.\n    *   How easily the food and its components are identified.\n    *   Ambiguity in portion size or hidden ingredients (e.g., sauces, oils).\n*   ** healthScore : An integer from 0 (extremely unhealthy) to 10 (highly nutritious and balanced). Base this on a holistic view of:\n    *   Level of processing (whole foods vs. ultra-processed).\n    *   Macronutrient balance.\n    *   Sugar and sodium content.\n    *   Estimated micronutrient density.\n*   ** rationale **: A brief (1-2 sentence) explanation justifying the healthScore and confidenceScore . State key assumptions made (e.g., \"Assumed dressing was a standard caesar\" or \"Portion size for rice was difficult to estimate\").\n``` On the final Respond To Webhook node it is also important to node that this is the spot where we will be cleaning up the final data setting the response Body for the HTTP request / API call. For my use-case where we are wanting to send back nutritional info for the provided image, I ended up formatting my response as JSON to look like this: jsx\n{\n  \"mealName\": \"Grilled Salmon with Roasted Potatoes and Kale Salad\",\n  \"calories\": 550,\n  \"protein\": 38,\n  \"carbs\": 32,\n  \"fat\": 30,\n  \"fiber\": 7,\n  \"sugar\": 4,\n  \"sodium\": 520,\n  \"confidenceScore\": 0.9,\n  \"healthScore\": 4\n} 3. Building the final lovable UI and connecting it to n8n With the full n8n backend now in place, it is time to spin up a new Lovable project and build the full functionality we want and style it to look exactly how we would like. You should expect this to be a pretty iterative process. I was not able to get a fully working app in 1-shot and had to chat back and forth in lovable to get the functionality working as expected. Here‚Äôs some of the key points in the prompt / conversation that had a large impact on the final result: Initial create app prompt: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8pekjpfeyrs52bdf1m1dm7 Style app to more closely match Cal AI: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rbd2wfvkrxxy7pc022n0e Setting up iphone mockup container: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rs1b8e7btc03gak9q4rbc Wiring up the app to make an API call to our n8n webhook: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxajea31e2xvtwbr1kytdxbb Updating app functionality to use real API response data instead of mocked dummy data (important - you may have to do something similar): https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxapb65ree5a18q99fsvdege If I was doing this again from the start, I think it would actually be much easier to get the lovable functionality working with default styles to start with and then finish up development by styling everything you need to change at the very end. The more styles, animations, other visual elements that get added in the beginning, the more complex it is to change as you get deeper into prompting. Lovable project with all prompts used: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 4. Extending this for more complex cases + security considerations This example is a very simple case and is not a complete app by any means. If you were to extend this functionality, you would likely need to add in many more endpoints to take care of other app logic + features like saving your history of scanned meals, loading up your history of scanned meals, other analysis features that can surface trends. So this tutorial is really meant to show you a bit of what is possible between lovable + n8n. The other really important thing I need to mention here is the security aspect of a workflow like this. When following my instructions above, your webhook url will not be secure. This means that if your webhook url leaks, it is completely possible for someone to make API requests into your backend and eat up your entire quota for n8n executions and run up your Open AI bill. In order to get around this for a production use-case, you will need to implement some form of authentication to protect your webhook url from malicious actors. This can be something as simple as basic auth where web apps that consume your API need to have a username / password or you could build out a more advanced auth system to protect your endpoints. My main point here is, make sure you know what you are doing before you publically rollout a n8n workflow like this or else you could be hit with a nasty bill or users of your app could be accessing things they should not have access to. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json Lovable project links: Simple n8n API request: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 Full Cal AI cloned project: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=4c-kYOiksFg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jqb8yg/realistic_expectations_for_n8n_rag/",
          "author": "Accurate-Jump-9679",
          "time": "2025-04-03T06:27:21+00:00",
          "body_text": "I have been experimenting with different RAG workflows (basically copying online tutorials) and my hope is to ingest PDF documents (academic literature, white papers) and achieve a high level of accuracy on queried output. So far the output I've gotten is underwhelming, including occasional hallucinations, returned code and some nonsense responses like \"I hope this has answered your question\".  I don't know if it's because I've been using Ollama and open source models like mxbai-embed-large for embeddings and Mistral, Supabase. The same setup will return a decent response sometimes, but garbage at other times. What is the setup that will yield the best possible performance? Is it unrealistic to expect a very high level of performance as compared to some commercial RAG document platforms?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "My first automation ever ‚Äì I built a full timesheet processing flow in n8n (for my company, for free!)",
          "link": "https://old.reddit.com/r/n8n/comments/1l5n6ex/66_million_points_of_interest_this_ai_agent/",
          "author": "Fearless-Pineapple36",
          "time": "2025-06-07T15:15:48+00:00",
          "body_text": "2 Demos Showcasing An AI Agent That Queries 66 Million+ Places üó∫Ô∏è Atlas ‚Äì Map Research Agent Atlas is an intelligent map data agent that translates natural-language prompts into SQL queries using LLMs, runs them against AWS Athena, and stores the results in Google Sheets ‚Äî no manual querying or scraping required. With access to over 66 million schools, businesses, hospitals, religious organizations, landmarks, mountain peaks, and much more, you will be able to perform a number of analyses with ease. Whether it's for competitive analysis, outbound marketing, route optimization, and more. üö® You can find the full step-by-step tutorial here: Don't Use Google Maps API, Use This AI Agent Instead ‚öôÔ∏è How It Works Workflow Breakdown: **Start Workflow (n8n Trigger):** * Accepts a plain-language query like: *‚ÄúGet every Starbucks in downtown Columbus.‚Äù* **Generate Query (LLM node):** * Converts the request into an Athena-compatible SQL query * Targets a specific Overture Maps theme (e.g., places, addresses) **Run Athena Query:** * Executes the SQL against a defined database/table * Handles errors (e.g., syntax, empty results) **Store Results:** * Saves valid results to a new/existing Google Sheet * Uses the `spreadsheetUrl` returned by: ```n8n {{ $('Research Agent Subworkflow').item.json.data.spreadsheetUrl }} ``` **Error Handling:** * If query fails or returns no results: * Passes the prior query + failure state into the next `Generate Query` run * Refines prompt based on schema + expected structure * Retries automatically üì¶ Project Setup Prerequisites * Docker * n8n (via Docker Compose) * AWS Athena credentials * Google Sheets API credentials * Optional: OpenAI API key or other LLM provider Install Instructions Clone this repo Run n8n locally: ```bash docker compose up ``` Access the UI at `http://localhost:5678/` Import the included JSON workflow (`Map Research Agent.json`) Connect: * Athena via AWS Node * Google Sheets via OAuth * LLM Provider of your choice (OpenAI, local model, etc.) üí° Example Prompts * ‚ÄúGet every McDonald's in Ohio‚Äù * ‚ÄúGet dentists within 5 miles of 123 Main St‚Äù * ‚ÄúGet the number of golf courses in California‚Äù"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1jwlw5j/mvp_generator/",
          "author": "Feisty-Economist6113",
          "time": "2025-04-11T09:39:16+00:00",
          "body_text": "Hi guys, I wanted to share a project I recently built that‚Äôs made my workflow a lot smoother. I build web apps pretty often, and I kept running into the same issue: every time I started a new project, I had to manually write out the same documents ‚Äî MVP descriptions, functional specs, technical design, business model, etc. It was repetitive and time-consuming. So I decided to automate it. I built a system that takes a simple idea and automatically generates all the core documentation you‚Äôd need to kick off a project. It currently creates: MVP summary Functional specifications Technical design Prototype description Business model Lean Startup validation plan It uses LLMs (OpenAI, Groq, DeepSeek) to generate the content and saves everything into a new Google Drive folder. This kind of documentation is super useful ‚Äî not just for clarity when building, but also when you‚Äôre applying for funding or sponsorship. It‚Äôs also a strong signal to potential clients that you‚Äôre organized and serious about your project. Right now, I just type in an idea, and the system handles the rest. It‚Äôs been a big boost to my efficiency, and I‚Äôm curious if others would find it useful too. Happy to hear any feedback or ideas for improvements."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a workflow that generates long-form blog posts with internal and external links",
          "link": "https://old.reddit.com/r/n8n/comments/1lk6ae6/exploring_a_potential_business_partnership/",
          "author": "MyAISalesProfile",
          "time": "2025-06-25T13:53:29+00:00",
          "body_text": "Hey everyone, I wanted to throw an idea out there and see if anyone in the n8n community might be seriously interested in exploring a business partnership, or at least having a conversation around one. What I‚Äôm proposing: A potential partnership/business venture built around recruiting, HR support, or lead generation services‚Äîleveraging automation, n8n, and AI (OpenAI/GPT) to deliver scalable, modern solutions for everyday businesses or internal talent teams. I‚Äôm a U.S.-based recruiter and business developer with 15+ years of experience across the agency, in-house/talent acquisition, and consulting sides of hiring. I‚Äôve worked with a variety of industries including healthcare, tech, legal, finance, and general SMBs. Along the way, I‚Äôve: Built efficient recruiting systems as part of a capstone in a Project Management certification program. Sold and closed business for recruiting/search contracts through cold outreach, email campaigns, and consultative sales calls. Owned a SaaS white-label smart phone app development company and sold services to small businesses in industries ranging from landscaping, real estate, legal and financial services. Created and refined successful email sequences that get responses whether cold outreach, nurture, or follow-up. Comfortable with Zoom/phone/in-person sales calls, fact-finding, and building client relationships from scratch. Currently teaching myself n8n automation and currently use OpenAI/ChatGPT in my recruiting workflows. Why I‚Äôm Posting Here I believe there‚Äôs a huge opportunity to combine: My background in recruiting, lead generation, and sales With someone who‚Äôs strong on technical automation, n8n workflows, and AI integrations We could create something that offers automated recruiting support, lead generation services, or even sales prospecting as a service for clients. (Bonus if you‚Äôve got a vision or technical edge to contribute‚ÄîI‚Äôm flexible on the actual service angle if we‚Äôre aligned.) I have an active LLC and business bank account, and I could easily spin up a DBA or new brand to get things going. Who I‚Äôd Love to Hear From: Someone strong in n8n (or no-code automation in general) who‚Äôs interested in teaming up. Anyone who has experience building or scaling lead gen, recruiting automation, or outbound tools. Or even just someone who sees the potential in this space and wants to brainstorm something together. Feel free to comment or DM me if any of this sounds interesting. Happy to share more details or ideas! I‚Äôve been on reddit for a bit but wanted to create a username for AI sales specific stuff. I‚Äôm happy to provide more details. Thanks for reading! Jared"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1kxk03i/generative_ai_made_easy/",
          "author": "Gvara",
          "time": "2025-05-28T15:27:30+00:00",
          "body_text": "Hi everyone, I want to share with you an update to my series \"Social Media Content Automation\", a very beginner friendly series, explaining step by step the process, all using selfhosted, opensource solutions. I published 3 videos on this series so far:\n  1 - Introduction to Generative AI\n  2 - Selfhosting n8n (with free custom domain, and ssl certs)\n  3 - Run LLMs locally, integrate them with n8n, and chain multiple agents to create Stories for the Videos. This is the link to the YouTube Playlist: Youtube/HomeStack What to expect nex on this series:\n  - Local Image Generation, using multiple options, and models (with n8n)\n  - local music generation\n  - local speach generation and transcription\n  - local video generation\n  - Compiling and publishing the videos to YouTube, Instagram, and Facebook I am also sharing the workflow in the below repo, currently covering Story Generation, and will update it as we make progress through the series (free, no paywall). GvaraX/HomeStack"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI Marketing Team That Actually Works - Here's What It Can Do",
          "link": "https://old.reddit.com/r/n8n/comments/1lrifp9/i_built_a_free_ai_email_assistant_that/",
          "author": "Puzzleheaded-Box4526",
          "time": "2025-07-04T13:13:12+00:00",
          "body_text": "Hey fellow automation enthusiasts! üëã I just built something that's been a game-changer for my email management, and I'm super excited to share it with you all! Using AI, I created an automated email system that: - ‚ú® Reads and categorizes your emails automatically - ü§ñ Sends customized responses based on Gmail labels - üîÑ Runs every minute, 24/7 - üí∞ Costs absolutely nothing to run! The Problem We All Face: We're drowning in emails, right? Managing different types of inquiries, sending appropriate responses, and keeping up with the inbox 24/7 is exhausting. I was spending hours each week just sorting and responding to repetitive emails. The Solution I Built: I created a completely free workflow that: Automatically reads your unread emails Uses AI to understand and categorize them with Gmail labels Sends customized responses based on those labels Runs continuously without any manual intervention The Best Part? - Zero coding required - Works while you sleep - Completely customizable responses - Handles unlimited emails - Did I mention it's FREE? üòâ Here's What Makes This Different: - Only processes unread messages (no spam worries!) - Smart enough to use default handling for uncategorized emails - Customizable responses for each label type - Set-and-forget system that runs every minute Want to See It in Action? I've created a detailed YouTube tutorial showing exactly how to set this up. Ready to Get Started? Watch the tutorial Join our Naas community to download the complete N8N workflow JSON for free. Set up your labels and customize your responses Watch your email management become automated! The Impact: - Hours saved every week - Professional responses 24/7 - Never miss an important email - Complete control over automated responses I'm super excited to share this with the community and can't wait to see how you customize it for your needs! What kind of emails would you want to automate first? Questions? I'm here to help!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1kfnydj/using_n8n_mcp_and_claude_desktop_to_automate/",
          "author": "miltonthecat",
          "time": "2025-05-05T21:46:34+00:00",
          "body_text": "I posted this over in r/ClaudeAI and thought it might also be appreciated here, especially since this sub is trying to improve its real content / snake oil ratio. I also think that there's untapped potential in this sub and the n8n community in general to focus on personal \"knowledge worker\" automation, as opposed to things like lead generation that have been beaten to death (valuable as they are). The further I progress in my career, the less time I have to spend on common managerial tasks, such as prepping for 1 on 1s, prepping for sprint retrospectives, managing my task lists, setting up meetings, and so on. These tasks are still important, but when I do them poorly due to lack of time, I do a major disservice to the people who depend on me. So I thought to myself, if an AI agent had access to my data, how many of these tasks could I fully or partially automate? I'll never escape full accountability for this work, but maybe AI can help me do it much faster, and better, too. For the past several weeks, I've been building an MCP server and few workflows in n8n designed to help me tackle this goal. The ROI has been immediate. My MCP server connects to my work's Google Drive, Slack, Outlook, and To Do environments. It also has access to institutional APIs that let me do things like search our HR system for person information. Finally, it has access to a couple of standard tools, such as web searching, web scraping, text to speech generation, Twilio voice, and a calculator. Here's a screenshot of my MCP server workflow. Using Claude Sonnet 3.7 and these tools I can easily do things like: \"Find 5 times next week that Jane Doe and I can meet, then send her an HTML email with those times so she can pick the best one.\" \"Read the article at $url and Slack me a text to speech summary.\" \"Call $coworker and let him know I'm running 10 minutes late to our meeting.\" \"Check my inbox and add any suggested tasks that don't already exist to my 'Suggested by Claude' task list.\" That's the easy personal assistant stuff. What else can it do? Using Claude Projects, an AI can also use these tools to reason its way toward a more complex goal, such as preparing me for 1 on 1s with my staff. Here's what a prompt for that might look like: Your job is to help me prepare for 1 on 1s with my direct reports. To do this, you'll review the Slack, Zendesk, Google Doc, and email queries contained in your config file. Before calling any tools, inform the user of the part of the team member config you plan to reference and the specific tools you plan to call. Access the tools in order with these instructions. Your config contains a default queries section. You must run all queries contained within it. Google Docs: For the 1 on 1 doc, reference the date of each meeting. Do not discuss content older than 1 month. Focus on content the staff member has prepared or items that are clearly outstanding that require some action. If a user has additional Google Docs in their config, search for them and read their contents. Slack: You need to look up the Slack username using the staff member's email address. Once you have that, run your default Slack queries and any user_slack_queries for the staff member. After receiving all of your Slack data, convert ts (timestamps) to datetimes using REPL. Zendesk: Your config file contains instructions on the specific ticket searches I'd like you to perform. Establish patterns and surface actionable intelligence if you find it. Outlook: Using the email section of your config file, find all messages sent to and received from the staff member in the last 2 weeks. Also find Zoom AI meeting summaries that reference the employee by name. At the end of this process, I want you to synthesize your findings into an artifact with actionable insights in a 1 on 1 prep document. Identify themes in your findings. Also give me a table with a statistical breakdown of activity in Slack, Zendesk, and Azure DevOps. Finally, please suggest some specific talking points that align with my 1 on 1 structure: Employee topics for this week My topics for this week Feedback for employee The result is a 1 on 1 prep document that I can use to have a far more informed conversation with my staff member than I ever could have cobbled together, no matter how much time I had to do it. It isn't a replacement for the human element in these conversations. But I've used this workflow dozens of times by now and the quality of my coaching and of these conversations has gone up dramatically. I have a similar set of Claude Project instructions for sprint retrospectives, just targeting a different set of Slack channels, Zendesk tickets, task boards, and Google Documents. It works just as well. I just used it today as a matter of fact. It's cutting meeting time by 50% while ensuring that the team gets the same or greater value from our time together. I really think this combination of n8n, MCP, and Claude Desktop is something special. Best of all, it's easily replicated by anyone who can stand up n8n and knows how to register an OAuth2 application in tools like Slack, Entra, or Google. Or can get someone to do that for them. More examples including setup instructions, as well as an example workflow for a daily priorities phone briefing, in my comment on the Claude subreddit."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Which AI tool works best for writing n8n nodes and designing workflows? Yours experiences?",
          "link": "https://old.reddit.com/r/n8n/comments/1j0rt5i/can_sonnet_37_build_an_n8n_workflow/",
          "author": "gatsbtc1",
          "time": "2025-03-01T05:01:33+00:00",
          "body_text": "UPDATE: I downloaded the JSON files that Claude created and uploaded them into n8n. Here's what it made. https://imgur.com/a/AEeXueP I want to reiterate that I've never used n8n before, this is all foreign to me so for everyone saying just test it, that's not something I currently know how to do and why I'm looking for people with more experience to give feedback. I just signed up for an n8n account so I could load this workflow in and see what it looks like. I'll start learning and seeing if I can get this to work over the next few weeks, but any help of feedback on this workflow is greatly appreciated. If it's all one big hallucination I'd appreciate if someone could let me know before I go crazy trying to make something work that isn't built properly. --__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__--__-- Hiya! I have a big project at work that has important info spread out over a lot of docs, at least 10 hours of meeting transcripts, hundreds of emails, and a few other piecemeal docs and literature. It's an overwhelming and disorganized amount of text to be manually trying to look through for answers so I'm determined to build a workflow where I can store all of this information in one place and be able to chat with agent to answer questions about the docs quickly. This kind of workflow seems fairly basic, but I have no experience in automation. I've never touched n8n before and the only coding experience I have is building silly apps with ChatGPT doing the heavy lifting. I asked Sonnet 3.7 to write me a step by step process to build this in n8n thinking it could guide me through this and this is what it spit out. For the experts in this group, would you mind letting me know if this is a correct guide to building the workflow I want? Thank you kindly for any advice and input! Comprehensive Guide: Building a Document AI Assistant with n8n This guide will walk you through the complete process of creating a document-based AI assistant using n8n without any coding experience. You'll be able to ask questions about your work documents and receive accurate answers based on their content. Prerequisites An n8n account (n8n.cloud or self-hosted) Access to your document repositories (Google Drive, SharePoint, email, etc.) An API key for an AI service (OpenAI, Anthropic, etc.) A simple database (Postgres, MongoDB, or even a spreadsheet can work to start) Part 1: Setting Up n8n Installation and First Steps Sign up for n8n.cloud : Go to n8n.cloud and create an account Choose the plan that fits your needs (they offer a free trial) Create a new workspace Familiarize yourself with the interface : Nodes Panel : Left side - contains all available integrations Canvas : Center - where you build your workflow Node Editor : Right side - appears when you select a node Execution Panel : Bottom - shows results when testing Create your first workflow : Click \"Workflows\" in the left sidebar Click \"+ Create workflow\" Name it \"Document AI Assistant\" Part 2: Document Collection System Setting Up Document Sources Add a trigger node : Click the \"+\" button on the canvas Search for your preferred storage (example: Google Drive) Select \"Google Drive Trigger\" node Configure Google Drive integration : Click on the node to open settings Click \"Add Credential\" and follow OAuth steps For \"Trigger On\": Choose \"File Created/Updated\" For \"Folders\": Select your project folders For \"File Types\": Add your document types (pdf, docx, txt, etc.) Test the connection : Click \"Execute Workflow\" at the bottom You should see sample document data in the execution panel Add additional document sources (if needed): Repeat steps for other sources (Outlook, SharePoint, etc.) Connect them all to the next step Document Processing Add a Router node (if using multiple sources): This lets you process different document types uniquely Connect all source nodes to this router Process PDFs : Add a \"PDF Extract\" node Connect it to the router Configure to extract text and metadata Process Office documents : Add \"Microsoft Office\" node for Word/Excel/PowerPoint Configure to extract text content Process emails : Add \"Email Parser\" node Configure to extract body text and attachments Add a Merge node : This combines all document types back into a single stream Connect all document processor nodes here Part 3: Setting Up Document Processing for AI Chunking Documents Add a Function node :const maxChunkSize = 1000; // characters per chunk const overlap = 200; // overlap between chunks  // Get the document text const text = items[0].json.documentText;  // Create chunks let chunks = []; let position = 0;  while (position < text.length) {   const chunk = text.slice(     Math.max(0, position - (position > 0 ? overlap : 0)),     Math.min(text.length, position + maxChunkSize)   );      chunks.push({     text: chunk,     metadata: {       source: items[0].json.filename,       position: position,       chunk_id: `${items[0].json.filename}-${position}`     }   });      position += maxChunkSize - overlap; }  return chunks.map(chunk => ({json: chunk})); Name it \"Chunk Documents\" This divides large documents into manageable pieces In the \"Function\" field, use this template (n8n provides this): Test the chunking : Execute the workflow and check the output You should see your document divided into overlapping chunks Creating Embeddings Add OpenAI node (or other embedding service): Click \"+\" and search for \"OpenAI\" Select the node and configure it Add your API key credential Set \"Operation\" to \"Create Embedding\" Set \"Input\" to \"={{$json.text}}\" (this references chunk text) Set \"Model\" to \"text-embedding-ada-002\" (or your preferred model) Test the embedding : Execute the workflow to verify embeddings are generated You should see vector representations in the output Storing Documents and Embeddings Add a Database node : Options include PostgreSQL, MongoDB, or even Google Sheets to start For this example, we'll use \"PostgreSQL\" Configure the database node : Add your database credentials Set \"Operation\" to \"Insert\" Set \"Table\" to \"document_chunks\" Map the following fields: \"chunk_text\": \"={{$json.text}}\" \"embedding\": \"={{$json.embedding}}\" \"document_name\": \"={{$json.metadata.source}}\" \"chunk_id\": \"={{$json.metadata.chunk_id}}\" Create a table in your database :CREATE TABLE document_chunks (   id SERIAL PRIMARY KEY,   chunk_text TEXT,   embedding VECTOR(1536),  -- Adjust dimension per your embedding model   document_name TEXT,   chunk_id TEXT,   created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); If using PostgreSQL, you'll need this table: Note: You can use n8n's \"Execute Query\" operation to create this table Test the storage : Run the workflow and verify data is stored in your database Part 4: Building the Question-Answering System Creating the Question Input Create a new workflow named \"AI Answer\": This will be triggered when you ask a question Add a Webhook node : This creates an endpoint where you can send questions Configure it as \"POST\" request Save the webhook URL that's generated (you'll use this to ask questions) Test the webhook : Click \"Execute Workflow\" Send a test POST request with a question in the body Retrieving Relevant Documents Add a Function node to format your question: Name it \"Prepare Question\" Process the incoming question from the webhook Add an OpenAI node to create question embedding: Configure similarly to document embedding This converts your question to the same vector space Add a PostgreSQL node to query similar chunks:SELECT chunk_text, document_name,    1 - (embedding <=> '{{$json.embedding}}') as similarity FROM document_chunks ORDER BY similarity DESC LIMIT 5; Set \"Operation\" to \"Execute Query\" Use this query template (for vector similarity search): Test the retrieval : Execute with a sample question Verify that relevant document chunks are returned Generating the AI Response Add a Function node to prepare prompt:// Get question and retrieved chunks const question = items[0].json.question; const chunks = items[1].json.rows.map(row => row.chunk_text).join(\"\\n\\n\");  // Create the prompt const prompt = ` Answer the following question based ONLY on the information provided below:  INFORMATION: ${chunks}  QUESTION: ${question}  ANSWER:`;  return [{json: {prompt}}]; Name it \"Prepare Context\" Combine question with retrieved document chunks Add an OpenAI or Anthropic node for answer generation: Add the AI node of your choice Set \"Operation\" to \"Create Chat Completion\" (OpenAI) or equivalent Set \"Messages\" to include your prompt with context Configure model parameters (temperature, max tokens, etc.) Add a Set node to format the response: Prepare the final answer format Include sources from original documents Connect back to Webhook node : Configure response settings Set \"Response Body\" to \"={{$json.answer}}\" Test the entire workflow : Ask a test question through the webhook Verify you get a proper answer with context Part 5: Creating a User Interface Simple Options for Non-Technical Users Using Make.com or Zapier : Create a simple form that sends data to your n8n webhook These platforms have user-friendly form builders Using Airtable or Google Forms : Create a form for questions Use Airtable/Google Sheets automations to send to n8n Store answers in the same sheet Using Microsoft Power Apps (if in a Microsoft environment): Create a simple app with a question input Connect to your n8n webhook Display the returned answer Part 6: Enhancing Your System Adding Real-Time Document Processing Schedule periodic updates : Add a \"Schedule Trigger\" node to your document processing workflow Configure it to run daily or hourly This will process new documents automatically Add document filtering : Use \"Filter\" nodes to only process new or updated documents Track document versions to avoid duplicate processing Improving Answer Quality Add prompt engineering : Refine your prompts for better answers Include specific instructions for formatting or reasoning Implement feedback mechanism : Create a simple workflow for users to rate answers Use this to improve your system Part 7: Maintenance and Monitoring Workflow Monitoring Set up n8n monitoring : Enable execution history Set up notifications for workflow failures Create a dashboard : Track usage statistics Monitor document processing volume Regular Updates Document database maintenance : Periodically clean up outdated documents Update embeddings as models improve AI service optimization : Update to newer models as they become available Optimize prompt templates based on performance Troubleshooting Tips Check execution logs : If something fails, n8n provides detailed logs Test nodes individually : Execute one node at a time to isolate issues Join n8n community forums : Ask questions when stuck Start with smaller document sets : Build confidence before scaling"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How you can setup and use n8n as your backend for a Lovable.dev app (I cloned the mobile app Cal AI)",
          "link": "https://old.reddit.com/r/n8n/comments/1lvlb8n/i_built_a_content_repurposing_system_that_turns/",
          "author": "dudeson55",
          "time": "2025-07-09T15:12:29+00:00",
          "body_text": "I built a content repurposing system that I have been using for the past several weeks that my YouTube video as input ‚Üí scrapes the transcript ‚Üí repurposes it into a post that is optimized for engagement on the platform I am posting to (right now just Twitter and LinkedIn but can be extended to many more). My social accounts are still pretty young so I don‚Äôt have great before/after stats to share, but I‚Äôm confident that the output quality here is on-par with what other creators are making and going viral with. My goal with this is to share a basic setup that you can take an run with in your own business to be customized for your niche / industry and add additional target platforms that you want to repurpose to. You could even change the main input to a long form blog post as your starting point instead of a youtube video. Here's a full breakdown of the automation 1. Workflow Trigger / Input The workflow starts with a simple form trigger that accepts a YouTube video URL as input. This is specific to our business since we always start with creating YouTube content first and then repurpose it into other formats. Form trigger accepts YouTube video URL as required text input If your content workflow starts with blog posts or other formats, you'll need to modify this trigger accordingly The URL gets passed through to the scraping operation (If your company and or your client‚Äôs company starts with a blog post first, I‚Äôd suggested simply using a tool to scrape that web page to load of that text content) 2. Scrape YouTube Video This is where we extract the video metadata and full transcript using a YouTube Scraper on Apify. Starts by using the streamers/youtube-scraper actor from the apify store (Costs $5 per 1,000 videos you scrape) Makes an HTTP request to the /run-sync-get-dataset-items endpoint to start scraping / get results back I like using this endpoint when consuming apify actors as it returns data back in the same http request we make. No need to setup polling or extra n8n nodes to use The scraper extracts title, metadata, and most importantly the full transcript in SRT format (timestamps w/ the text that was said in the video) 3. Generate Twitter Post The Twitter repurposing path follows a structured approach using a few examples I want to replicate + a detailed prompt. Set Twitter Examples : Simple ‚ÄúSet Field‚Äù node where I curated and put in 8 high-performing tweet examples that define the style and structure I want to replicate Build Master Prompt : Another Set Field node where I build a prompt that will tell the LLM to: Analyze the source YouTube transcript material Study the Twitter examples for structure and tone Generate 3 unique viral tweet options based on the content LLM Chain Call : Pass the complete prompt to Claude Sonnet Format and Share : Clean up the output and share the best 3 tweet options to Slack for me to review ```jsx ROLE: You are a world-class social media copywriter and viral growth hacker. Your expertise is in the AI, automation, and no-code space on Twitter/X. You are a master at deconstructing viral content and applying its core principles to generate new, successful posts. OBJECTIVE: Your mission is to generate three distinct, high-potential viral tweets . This tweet will promote a specific n8n automation, with the ultimate goal of getting people to follow my profile, retweet the post, and comment a specific keyword to receive the n8n workflow template via DM. STEP 1: ANALYZE SOURCE MATERIAL First, meticulously analyze the provided YouTube video transcript below. Do not summarize it. Instead, your goal is to extract the following key elements:\n1. The Core Pain Point: What is the single most frustrating, time-consuming, or tedious manual task that this automation eliminates?\n2. The \"Magic\" Solution: What is the most impressive or \"wow\" moment of the automation? What does it enable the user to do that felt impossible or difficult before?\n3. The Quantifiable Outcome: Identify any specific metrics of success mentioned (e.g., \"saves 10 hours a week,\" \"processes 100 leads a day,\" \"automates 90% of the workflow\"). If none are mentioned, create a powerful and believable one. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES Next, study the structure, tone, and psychological hooks of the following successful tweets. These examples are your primary source for determining the structure of the tweets you will generate. <twitter_tweet_examples>\n{{ $('set_twitter_examples').item.json.twitter_examples }}\n</twitter_tweet_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE TWEETS Now you will generate the 3 unique, viral tweet options. Your primary task is to act as a structural analyst: analyze the provided examples, identify the most effective structures, and then apply those structures to the content from Step 1. Your process: 1. Identify Core Structures: Analyze the <twitter_tweet_examples> . Identify the different underlying formats. For instance, is there a \"Problem ‚Üí Solution\" structure? A \"Shocking Result ‚Üí How-to\" structure? A \"Controversial Statement ‚Üí Justification\" structure? Identify the 3 most distinct and powerful structures present.\n2. Map Content to Structures: For each of the 3 structures you identified, map the \"Pain Point,\" \"Magic Solution,\" and \"Outcome\" from Step 1 into that framework.\n3. Craft the Tweets: Generate one tweet for each of the 3 structures you've chosen. The structure of each tweet (the hook, the flow, the tone) should directly mirror the style of the example it is based on. Essential Components: While you choose the overall structure, ensure each tweet you craft contains these four key elements, integrated naturally within the chosen format:\n- A Powerful Hook: The opening line that grabs attention.\n- A Clear Value Proposition: The \"what's in it for me\" for the reader.\n- An Irresistible Offer: The free n8n workflow template.\n- A High-Engagement Call to Action (CTA): The final call to action must include elements the ask for a follow, a retweet, and a comment of the \"[KEYWORD]\". CONSTRAINTS: -   Vary light use of emojis to add personality and break up the text. Not all Tweets you write should have emojis.\n-   Keep the tone energetic, confident, and educational, mirroring the tone found in the examples.\n-   Ensure the chosen [KEYWORD] is simple, relevant, and in all caps. Now, generate the 3 distinct tweet options, clearly labeled as Tweet Option 1 , Tweet Option 2 , and Tweet Option 3 . For each option, briefly state which example structure you are applying. (e.g., \"Tweet Option 1: Applying the 'Problem ‚Üí Solution' structure from Example 2.\").\n``` 4. Generate LinkedIn Post The LinkedIn path follows a similar but platform-specific approach (better grammar and different call to action): Set LinkedIn Examples : Curated examples of high-performing LinkedIn posts with different formatting and professional tone Build LinkedIn-Specific Prompt : Modified prompt that positions the LLM as a \"B2B content strategist and LinkedIn growth expert\" rather than a viral Twitter copywriter Generate Multiple Options : Creates 3 different LinkedIn post variations optimized for professional engagement Review Process : Posts all options to Slack for me to review The key difference is tone and structure - LinkedIn posts are longer, more professional, minimize emoji usage, and focus on business value rather than viral hooks. It is important to know your audience here and have a deep understanding of the types of posts that will do well. ```jsx ROLE: You are a world-class B2B content strategist and LinkedIn growth expert. Your expertise lies in creating compelling professional content around AI, automation, and no-code solutions. You are a master of professional storytelling, turning technical case studies into insightful, engaging posts that drive meaningful connections and establish thought leadership. OBJECTIVE: Your mission is to generate three distinct, high-potential LinkedIn posts . Each post will promote a specific n8n automation, framing it as a professional case study. The ultimate goals are to:\n1.  Grow my LinkedIn professional network (followers).\n2.  Establish my profile as a go-to resource for AI and automation.\n3.  Drive awareness and interest in my YouTube channel.\n4.  Get users to comment for a lead magnet (the n8n workflow). STEP 1: ANALYZE SOURCE MATERIAL (THE BUSINESS CASE) First, meticulously analyze the provided YouTube video transcript. Do not summarize it. Instead, extract the following key business-oriented elements:\n1. The Business Pain Point: What common, frustrating, or inefficient business process does this automation solve? Frame it in terms of lost time, potential for human error, or missed opportunities.\n2. The Strategic Solution: How does the n8n automation provide a smart, strategic solution? What is the core \"insight\" or \"lever\" it uses to create value?\n3. The Quantifiable Business Impact: What is the measurable outcome? Frame it in business terms (e.g., \"reclaimed 10+ hours for strategic work,\" \"achieved 99% accuracy in data processing,\" \"reduced new client onboarding time by 50%\"). If not explicitly mentioned, create a powerful and believable metric. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES (LINKEDIN POSTS) Next, study the structure, tone, and especially the Call to Action (CTA) of the following successful LinkedIn posts. These examples are your primary source for determining the structure of the posts you will generate. Pay close attention to the length of the examples as they \"feel\" right in length. <linkedin_post_examples>\n{{ $('set_linked_in_examples').item.json.linked_in_examples }}\n</linkedin_post_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE POSTS Now you will generate 3 unique LinkedIn post options. Your primary task is to act as a content strategist: analyze the provided LinkedIn examples, identify the most effective post structures, and then apply those structures to the business case from Step 1. Your process: 1. Identify Core Structures: Analyze the <linkedin_post_examples> . Identify 3 distinct formats (e.g., \"Problem/Agitate/Solve,\" \"Personal Story ‚Üí Business Lesson,\" \"Contrarian Take ‚Üí Justification\").\n2. Map Content to Structures: For each structure, weave the \"Business Pain Point,\" \"Strategic Solution,\" and \"Business Impact\" into a compelling narrative.\n3. Craft the Posts: Generate one post for each chosen structure. The post should be highly readable, using short paragraphs and ample white space. Essential Components for each LinkedIn Post: - An Intriguing Hook: A first line that stops the scroll and speaks to a professional ambition or frustration.\n- A Relatable Story/Problem: Briefly set the scene using the \"Business Pain Point.\"\n- The Insightful Solution: Explain the \"Strategic Solution\" as the turning point.\n- A Dynamic, High-Engagement Call to Action (CTA): This is critical. Instead of a fixed format, you will craft the most effective CTA by analyzing the examples provided. Your CTA must accomplish two things:\n    1.  Clearly state how to get the free n8n workflow template by commenting with a specific [KEYWORD] .\n    2.  Naturally encourage following my profile and sharing the post. Draw inspiration for the wording and style directly from the successful CTAs in the examples. If it fits the narrative, you can subtly mention that more deep dives are on my YouTube. CONSTRAINTS: -   Use emojis sparingly and professionally (e.g., ‚úÖ, üí°, üöÄ) to enhance readability.\n-   The tone must be professional, insightful, and helpful.\n-   The [KEYWORD] should be a professional, single word in all caps (e.g., BLUEPRINT, WORKFLOW, SYSTEM). FINAL OUTPUT FORMAT: You MUST format your entire response as a single, valid JSON object. The root of the object should be a key named \"post_options\", which contains an array of three post objects. Adhere strictly to the following structure for each object:\n{\n  \"analysis\": \"<string: Explain which LinkedIn example structure was applied>\",\n  \"post_text\": \"<string: The full text of the LinkedIn post, with line breaks>\"\n}\nDo not include any text or explanations outside of the JSON object.\n``` 5. Final Output Review Both paths conclude by sharing the generated content to Slack channels for human review. This gives me 3 Twitter options and 3 LinkedIn options to choose from, each optimized for best engagement. All I have to do is copy and paste the one I like the most into my social media scheduling tool then I‚Äôm done. Extending the System The best part about this is it is very easy to extend this system for any type of repurposing you need to do. LinkedIn / Twitter is only the starting point, it can be taken much further. Instagram carousel posts - Take the transcript ‚Üí pull out a few quotes ‚Üí generate an image using either Canva an AI Image generator Newsletter sections - Take the transcript + video url ‚Üí build a prompt that will write a mini-promo section for your video to be included in your newsletter Blog post / tutorial post - Take the transcript ‚Üí write a prompt that will turn it into a text-based tutorial to be published on your blog. Each new path would follow the same pattern: curate platform-specific examples, build targeted prompts, and generate multiple options for review. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=u9gwOtjiYnI The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1kqyx63/n8n_workflow_generator_another_take_on_it/",
          "author": "CertainHorror7694",
          "time": "2025-05-20T07:25:43+00:00",
          "body_text": "Even though n8n is working on an internal tool for workflow generation from a prompt, I've build a generator, that for me is doing very well. - Based on 5000+ high quality templates and up-to-date documentation - Knows of all 400+ integrations - Full AI agent compatibility - Adds sticky notes with comments for the setup Saves me on average 87% of time when coming up with new flows. Give it a shot -> n8n-gen.com"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Realistic expectations for n8n RAG?",
          "link": "https://old.reddit.com/r/n8n/comments/1lq2ys4/built_a_tiny_n8n_agent_that_quietly_wins_back/",
          "author": "Charming_You_8285",
          "time": "2025-07-02T18:09:17+00:00",
          "body_text": "Out of desperation and frustration, I built an n8n agent to solve my problem... because 95% of the time, I had a bunch of users who signed up for my product but completely disappeared after a 15 days to 30 days. Basically, it does this: Runs daily on a schedule Checks Supabase for users whose last login was >30 days ago Cleans up duplicates Updates a Google Sheet with those users And finally sends them a personalized HTML email trying to bring them back Honestly, I didn‚Äôt expect much‚Ä¶ but it actually managed to get a few users to come back and use the product again. It‚Äôs super simple, but it feels kinda cool to have it running completely hands‚Äëfree. Note: For now it only supports Supabase, but the idea should work with any DB/API if you tweak the request node. After this I realise everything is trail & error in life. Atleast this worked after a lot of failures :)"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "66 Million Points of Interest + This AI Agent = ü§Ø",
          "link": "https://old.reddit.com/r/n8n/comments/1lxx6gp/my_2500_ai_agent_workflow_for_a_client/",
          "author": "Weird_Faithlessness1",
          "time": "2025-07-12T10:25:09+00:00",
          "body_text": "https://preview.redd.it/8or73y8f3fcf1.png?width=1660&format=png&auto=webp&s=063496b1e7181ab7d27c9e7b1f7b61e1dddc4d43 I am sharing a workflow I have done for a business consultant client, which is a Conversational AI Agent that intelligently conducts an interview with the user to create a pain point diagnosis. It asks questions to reveal insight into the pain points by analysing the user profile and subsequent answers. Once it has asked 5-10 questions, excluding the screening phase it feeds the Diagnosis Agent the conversation along with an airtable database of all their educational content to match the best recommendation. A chatbot interface is included with a paywall to get the content. The selling point is that you can automate services to low priority users with intelligent content recommendation, or use it as a demo consultation on a landing page to drive conversions. Discussion Point: The conversation is separated in phases with which I keep state of the conversation in redis in order to skip agents with if nodes, if the current session has already gone through the phase already according to the session state. How do you handle these problems? In my case I structured the workflow in the shape of a journey path with \"railway kind of levers\" as the if nodes that skip to the next agent if in the redis key the state is done. I feel though the workflow gets cluttered with redis states and if nodes. State Example: { screening_phase: done } Business Consultant Agent with Education Content Recommendation Phase 1 (Screening Interview Phase): - Collect users basic information to create a profile - Pass state to next agent with redis. Save to redis a hash key value pair: { team_size: role_title: industry: main_offer: } Phase 2 (Pain Point Discover Interview): - Ask questions based on profile to discover pain points. - Analyse subsequent answers to lead the conversation to pain point discover. - Ask 5-10 questions only. Phase 3 (Diagnosis Agent): - Get the conversation history, analyse it and provide a diagnosis that closely relates to a record in the database. - Provide sub-diagnosis and ask for confirmation for which pain point the user would like a video tutorial. - After confirmation respond with a link to the educational tutorial which link will be parsed and converted to a paywall button in the frontend interface code. I have just started to get into the habit of putting my work out there and trying new things, so if I missed anything out feel free to ask any questions and I would be happy to engage in any discussion."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lokcdl/fully_automated_api_documentation_scraper/",
          "author": "aiplusautomation",
          "time": "2025-06-30T21:49:52+00:00",
          "body_text": "Hiyo. First post here. Hope this is helpful... This is one of the most useful workflows I've built in n8n. I often rely on A.I. to help with the heavy lifting of development. That means I need to feed the LLM API reference documentation for context. LLMs are pretty smart, but unless they are using computer actions, they aren't smart enough to go to a URL and click through to more URLs, so you have to provide it with all API reference pages. To automate the process, I built this workflow. Here's how it works: Form input for the first page of the API reference (this triggers the workflow) New Google Doc is created. A couple of custom scripts are used in Puppeteer to -- take a screenshot AND unfurl nested text and scrape the text (with a bit of javascript formatting in between)...this uses the Puppeteer community node - https://www.npmjs.com/package/n8n-nodes-puppeteer Screenshot is uploaded to Gemini and the LLM is given the screenshot and the text as context. Gemini outputs the text of the documentation in markdown. The text is added to the Google Doc. The page's \"Next\" button is identified so that the process can loop through every page of the documentation. **Notes: This was designed with Fern documentation in mind...if the pages don't have a Next button then it probably won't work. But I'm confident the script can be adapted to fit whatever structure you want to scrape. This version also scrapes EVERY PAGE...including the deprecated stuff or the stuff you don't really need. So you'll probably need to prune it first. BUT, in the end you'll have API documentation in FULL in Markdown for LLM ingestion. [screenshot in first comment cuz...it's been so long I don't know how to add a screenshot to a post anymore apparently] Here's the workflow - {\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://generativelanguage.googleapis.com/upload/v1beta/files\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpQueryAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"X-Goog-Upload-Command\",\n              \"value\": \"start, upload, finalize\"\n            },\n            {\n              \"name\": \"X-Goog-Upload-Header-Content-Length\",\n              \"value\": \"=123\"\n            },\n            {\n              \"name\": \"X-Goog-Upload-Header-Content-Type\",\n              \"value\": \"=image/png\"\n            },\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"=image/png\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"binaryData\",\n        \"inputDataFieldName\": \"data\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        780,\n        -280\n      ],\n      \"id\": \"0361ea36-4e52-4bfa-9e78-20768e763588\",\n      \"name\": \"HTTP Request3\",\n      \"credentials\": {\n        \"httpQueryAuth\": {\n          \"id\": \"c0cNSRvwwkBXUfpc\",\n          \"name\": \"Gemini\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpQueryAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={\\n  \\\"contents\\\": [\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"parts\\\": [\\n        {\\n          \\\"fileData\\\": {\\n            \\\"fileUri\\\": \\\"{{ $json.file.uri }}\\\",\\n            \\\"mimeType\\\": \\\"{{ $json.file.mimeType }}\\\"\\n          }\\n        },\\n        {\\n          \\\"text\\\": \\\"Here is the text from an API document, along with a screenshot to illustrate its structure: title - {{ $('Code1').item.json.titleClean }} ### content - {{ $('Code1').item.json.contentEscaped }} ### Please convert this api documentation into Markdown for LLM ingestion. Keep all content intact as they need to be complete and full instruction.\\\"\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"generationConfig\\\": {\\n    \\\"temperature\\\": 0.2,\\n    \\\"topK\\\": 40,\\n    \\\"topP\\\": 0.9,\\n    \\\"maxOutputTokens\\\": 65536,\\n    \\\"thinking_config\\\": {\\n      \\\"thinking_budget\\\": 0\\n    }\\n  }\\n}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        960,\n        -280\n      ],\n      \"id\": \"f0f11f5a-5b18-413c-b609-bd30cdb2eb46\",\n      \"name\": \"HTTP Request4\",\n      \"credentials\": {\n        \"httpQueryAuth\": {\n          \"id\": \"c0cNSRvwwkBXUfpc\",\n          \"name\": \"Gemini\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.url }}\",\n        \"operation\": \"getScreenshot\",\n        \"fullPage\": true,\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-puppeteer.puppeteer\",\n      \"typeVersion\": 1,\n      \"position\": [\n        620,\n        -280\n      ],\n      \"id\": \"86e830c9-ff74-4736-add7-8df997975644\",\n      \"name\": \"Puppeteer1\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Code node to safely escape text for API calls\\n// Set to \\\"Run Once for Each Item\\\" mode\\n\\n// Get the data from Puppeteer node\\nconst puppeteerData = $('Puppeteer6').item.json;\\n\\n// Function to safely escape text for JSON\\nfunction escapeForJson(text) {\\n  if (!text) return '';\\n  \\n  return text\\n    .replace(/\\\\\\\\/g, '\\\\\\\\\\\\\\\\')   // Escape backslashes first\\n    .replace(/\\\"/g, '\\\\\\\\\\\"')     // Escape double quotes\\n    .replace(/\\\\n/g, '\\\\\\\\n')    // Escape newlines\\n    .replace(/\\\\r/g, '\\\\\\\\r')    // Escape carriage returns\\n    .replace(/\\\\t/g, '\\\\\\\\t')    // Escape tabs\\n    .replace(/\\\\f/g, '\\\\\\\\f')    // Escape form feeds\\n    .replace(/\\\\b/g, '\\\\\\\\b');   // Escape backspaces\\n}\\n\\n// Alternative: Remove problematic characters entirely\\nfunction cleanText(text) {\\n  if (!text) return '';\\n  \\n  return text\\n    .replace(/[\\\"']/g, '')     // Remove all quotes\\n    .replace(/\\\\s+/g, ' ')     // Normalize whitespace\\n    .trim();\\n}\\n\\n// Process title and content\\nconst titleEscaped = escapeForJson(puppeteerData.title || '');\\nconst contentEscaped = escapeForJson(puppeteerData.content || '');\\nconst titleClean = cleanText(puppeteerData.title || '');\\nconst contentClean = cleanText(puppeteerData.content || '');\\n\\n// Return the processed data\\nreturn [{\\n  json: {\\n    ...puppeteerData,\\n    titleEscaped: titleEscaped,\\n    contentEscaped: contentEscaped,\\n    titleClean: titleClean,\\n    contentClean: contentClean\\n  }\\n}];\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        420,\n        -280\n      ],\n      \"id\": \"96b16563-7e17-4d74-94ae-190daa2b1d31\",\n      \"name\": \"Code1\"\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"update\",\n        \"documentURL\": \"={{ $('Set Initial URL').item.json.google_doc_id }}\",\n        \"actionsUi\": {\n          \"actionFields\": [\n            {\n              \"action\": \"insert\",\n              \"text\": \"={{ $json.candidates[0].content.parts[0].text }}\"\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1160,\n        -280\n      ],\n      \"id\": \"e90768f2-e6aa-4b72-9bc5-b3329e5e31d7\",\n      \"name\": \"Google Docs\",\n      \"credentials\": {\n        \"googleDocsOAuth2Api\": {\n          \"id\": \"ch6o331MGzTxpfMS\",\n          \"name\": \"Google Docs account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"a50a4fd1-d813-4754-9aaf-edee6315b143\",\n              \"name\": \"url\",\n              \"value\": \"={{ $('On form submission').item.json.api_url }}\",\n              \"type\": \"string\"\n            },\n            {\n              \"id\": \"cebbed7e-0596-459d-af6a-cff17c0dd5c8\",\n              \"name\": \"google_doc_id\",\n              \"value\": \"={{ $json.id }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -40,\n        -280\n      ],\n      \"id\": \"64dfe918-f572-4c0c-8539-db9dac349e60\",\n      \"name\": \"Set Initial URL\"\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"runCustomScript\",\n        \"scriptCode\": \"// Merged Puppeteer Script: Scrapes content, expands collapsibles, and finds the next page URL.\\n// This script assumes it runs once per item, where each item contains a 'url' property.\\n\\nasync function processPageAndFindNext() {\\n  // Get the URL to process from the input item\\n  const currentUrl = $input.item.json.url;\\n\\n  if (!currentUrl) {\\n    console.error(\\\"‚ùå No URL provided in the input item.\\\");\\n    // Return an error item, also setting hasNextPage to false to stop the loop\\n    return [{ json: { error: \\\"No URL provided\\\", success: false, scrapedAt: new Date().toISOString(), hasNextPage: false } }];\\n  }\\n\\n  console.log(`üîç Starting to scrape and find next page for: ${currentUrl}`);\\n\\n  try {\\n    // Navigate to the page - networkidle2 should handle most loading\\n    // Set a reasonable timeout for page load\\n    await $page.goto(currentUrl, {\\n      waitUntil: 'networkidle2',\\n      timeout: 60000 // Increased timeout to 60 seconds for robustness\\n    });\\n\\n    // Wait a bit more for any dynamic content to load after navigation\\n    await new Promise(resolve => setTimeout(resolve, 3000)); // Increased wait time\\n\\n    // Unfurl all collapsible sections\\n    console.log(`üìÇ Expanding collapsible sections for ${currentUrl}`);\\n    const expandedCount = await expandCollapsibles($page);\\n    console.log(`‚úÖ Expanded ${expandedCount} collapsible sections`);\\n\\n    // Wait for any animations/content loading after expansion\\n    await new Promise(resolve => setTimeout(resolve, 1500)); // Increased wait time\\n\\n    // Extract all data (content and next page URL) in one evaluate call\\n    const data = await $page.evaluate(() => {\\n      // --- Content Scraping Logic (from your original Puppeteer script) ---\\n      const title = document.title;\\n\\n      let content = '';\\n      const contentSelectors = [\\n        'main', 'article', '.content', '.post-content', '.documentation-content',\\n        '.markdown-body', '.docs-content', '[role=\\\"main\\\"]'\\n      ];\\n      // Iterate through selectors to find the most appropriate content area\\n      for (const selector of contentSelectors) {\\n        const element = document.querySelector(selector);\\n        if (element && element.innerText.trim()) {\\n          content = element.innerText;\\n          break; // Found content, stop searching\\n        }\\n      }\\n      // Fallback to body text if no specific content area found\\n      if (!content) {\\n        content = document.body.innerText;\\n      }\\n\\n      // Extract headings\\n      const headings = Array.from(document.querySelectorAll('h1, h2, h3, h4, h5, h6'))\\n        .map(h => h.innerText.trim())\\n        .filter(h => h); // Filter out empty headings\\n\\n      // Extract code blocks (limiting to first 5, and minimum length)\\n      const codeBlocks = Array.from(document.querySelectorAll('pre code, .highlight code, code'))\\n        .map(code => code.innerText.trim())\\n        .filter(code => code && code.length > 20) // Only include non-empty, longer code blocks\\n        .slice(0, 5); // Limit to 5 code blocks\\n\\n      // Extract meta description\\n      const metaDescription = document.querySelector('meta[name=\\\"description\\\"]')?.getAttribute('content') || '';\\n\\n      // --- Next Page URL Extraction Logic (from your original Puppeteer2 script) ---\\n      let nextPageData = null; // Stores details of the found next page link\\n      const strategies = [\\n        // Strategy 1: Specific CSS selectors for \\\"Next\\\" buttons/links\\n        () => {\\n          const selectors = [\\n            'a:has(span:contains(\\\"Next\\\"))', // Link containing a span with \\\"Next\\\" text\\n            'a[href*=\\\"/sdk-reference/\\\"]:has(svg)', // Link with SDK reference in href and an SVG icon\\n            'a.bg-card-solid:has(span:contains(\\\"Next\\\"))', // Specific class with \\\"Next\\\" text\\n            'a:has(.lucide-chevron-right)', // Link with a specific icon class\\n            'a:has(svg path[d*=\\\"m9 18 6-6-6-6\\\"])' // Link with a specific SVG path (right arrow)\\n          ];\\n          for (const selector of selectors) {\\n            try {\\n              const element = document.querySelector(selector);\\n              if (element && element.href) {\\n                return {\\n                  url: element.href,\\n                  text: element.textContent?.trim() || '',\\n                  method: `CSS selector: ${selector}`\\n                };\\n              }\\n            } catch (e) {\\n              // Selector might not be supported or element not found, continue to next\\n            }\\n          }\\n          return null;\\n        },\\n        // Strategy 2: Links with \\\"Next\\\" text (case-insensitive, includes arrows)\\n        () => {\\n          const links = Array.from(document.querySelectorAll('a'));\\n          for (const link of links) {\\n            const text = link.textContent?.toLowerCase() || '';\\n            const hasNext = text.includes('next') || text.includes('‚Üí') || text.includes('‚ñ∂');\\n            if (hasNext && link.href) {\\n              return {\\n                url: link.href,\\n                text: link.textContent?.trim() || '',\\n                method: 'Text-based search for \\\"Next\\\"'\\n              };\\n            }\\n          }\\n          return null;\\n        },\\n        // Strategy 3: Navigation arrows (SVG, icon classes, chevrons)\\n        () => {\\n          const arrowElements = document.querySelectorAll('svg, .icon, [class*=\\\"chevron\\\"], [class*=\\\"arrow\\\"]');\\n          for (const arrow of arrowElements) {\\n            const link = arrow.closest('a'); // Find the closest parent <a> tag\\n            if (link && link.href) {\\n              const classes = arrow.className || '';\\n              const hasRightArrow = classes.includes('right') ||\\n                                    classes.includes('chevron-right') ||\\n                                    classes.includes('arrow-right') ||\\n                                    arrow.innerHTML?.includes('m9 18 6-6-6-6'); // SVG path for common right arrow\\n              if (hasRightArrow) {\\n                return {\\n                  url: link.href,\\n                  text: link.textContent?.trim() || '',\\n                  method: 'Arrow/chevron icon detection'\\n                };\\n              }\\n            }\\n          }\\n          return null;\\n        },\\n        // Strategy 4: Pagination or navigation containers (e.g., last link in a pagination group)\\n        () => {\\n          const navContainers = document.querySelectorAll('[class*=\\\"nav\\\"], [class*=\\\"pagination\\\"], [class*=\\\"next\\\"], .fern-background-image');\\n          for (const container of navContainers) {\\n            const links = container.querySelectorAll('a[href]');\\n            const lastLink = links[links.length - 1]; // Often the \\\"Next\\\" link is the last one\\n            if (lastLink && lastLink.href) {\\n                // Basic check to prevent infinite loop on \\\"current\\\" page link, if it's the last one\\n                if (lastLink.href !== window.location.href) {\\n                    return {\\n                        url: lastLink.href,\\n                        text: lastLink.textContent?.trim() || '',\\n                        method: 'Navigation container analysis'\\n                    };\\n                }\\n            }\\n          }\\n          return null;\\n        }\\n      ];\\n\\n      // Execute strategies in order until a next page link is found\\n      for (const strategy of strategies) {\\n        try {\\n          const result = strategy();\\n          if (result) {\\n            nextPageData = result;\\n            break; // Found a next page, no need to try further strategies\\n          }\\n        } catch (error) {\\n          // Log errors within strategies but don't stop the main evaluation\\n          console.log(`Next page detection strategy failed: ${error.message}`);\\n        }\\n      }\\n\\n      // Determine absolute URL and hasNextPage flag\\n      let nextPageUrlAbsolute = null;\\n      let hasNextPage = false;\\n      if (nextPageData && nextPageData.url) {\\n        hasNextPage = true;\\n        try {\\n          // Ensure the URL is absolute\\n          nextPageUrlAbsolute = new URL(nextPageData.url, window.location.href).href;\\n        } catch (e) {\\n          console.error(\\\"Error creating absolute URL:\\\", e);\\n          nextPageUrlAbsolute = nextPageData.url; // Fallback if URL is malformed\\n        }\\n        console.log(`‚úÖ Found next page URL: ${nextPageUrlAbsolute}`);\\n      } else {\\n        console.log(`‚ÑπÔ∏è No next page found for ${window.location.href}`);\\n      }\\n\\n      // Return all extracted data, including next page details\\n      return {\\n        url: window.location.href, // The URL of the page that was just scraped\\n        title: title,\\n        content: content?.substring(0, 8000) || '', // Limit content length if needed\\n        headings: headings.slice(0, 10), // Limit number of headings\\n        codeBlocks: codeBlocks,\\n        metaDescription: metaDescription,\\n        wordCount: content ? content.split(/\\\\s+/).length : 0,\\n\\n        // Data specifically for controlling the loop\\n        nextPageUrl: nextPageData?.url || null, // Original URL from the link (might be relative)\\n        nextPageText: nextPageData?.text || null,\\n        detectionMethod: nextPageData?.method || null,\\n        nextPageUrlAbsolute: nextPageUrlAbsolute, // Crucial: Absolute URL for next page\\n        hasNextPage: hasNextPage // Crucial: Boolean flag for loop condition\\n      };\\n    });\\n\\n    // Prepare the output for n8n\\n    return [{\\n      json: {\\n        ...data,\\n        scrapedAt: new Date().toISOString(), // Timestamp of scraping\\n        success: true,\\n        sourceUrl: currentUrl, // The URL that was initially provided to this node\\n        expandedSections: expandedCount // How many collapsibles were expanded\\n      }\\n    }];\\n\\n  } catch (error) {\\n    console.error(`‚ùå Fatal error scraping ${currentUrl}:`, error.message);\\n    // Return an error item, ensuring hasNextPage is false to stop the loop\\n    return [{\\n      json: {\\n        url: currentUrl,\\n        error: error.message,\\n        scrapedAt: new Date().toISOString(),\\n        success: false,\\n        hasNextPage: false // No next page if an error occurred during scraping\\n      }\\n    }];\\n  }\\n}\\n\\n// Helper function to expand all collapsible sections\\nasync function expandCollapsibles(page) {\\n  return await page.evaluate(async () => {\\n    let expandedCount = 0;\\n\\n    const strategies = [\\n      () => { // Fern UI specific collapsibles\\n        const fern = document.querySelectorAll('.fern-collapsible [data-state=\\\"closed\\\"]');\\n        fern.forEach(el => { if (el.click) { el.click(); expandedCount++; } });\\n      },\\n      () => { // Generic data-state=\\\"closed\\\" elements\\n        const collapsibles = document.querySelectorAll('[data-state=\\\"closed\\\"]');\\n        collapsibles.forEach(el => { if (el.click && (el.tagName === 'BUTTON' || el.role === 'button' || el.getAttribute('aria-expanded') === 'false')) { el.click(); expandedCount++; } });\\n      },\\n      () => { // Common expand/collapse button patterns\\n        const expandButtons = document.querySelectorAll([\\n          'button[aria-expanded=\\\"false\\\"]', '.expand-button', '.toggle-button',\\n          '.accordion-toggle', '.collapse-toggle', '[data-toggle=\\\"collapse\\\"]',\\n          '.dropdown-toggle'\\n        ].join(','));\\n        expandButtons.forEach(button => { if (button.click) { button.click(); expandedCount++; } });\\n      },\\n      () => { // <details> HTML element\\n        const details = document.querySelectorAll('details:not([open])');\\n        details.forEach(detail => { detail.open = true; expandedCount++; });\\n      },\\n      () => { // Text-based expand/show more buttons\\n        const expandTexts = ['expand', 'show more', 'view more', 'see more', 'more details', 'show all', 'expand all', '‚ñ∂', '‚ñº', '+'];\\n        const allClickables = document.querySelectorAll('button, [role=\\\"button\\\"], .clickable, [onclick]');\\n        allClickables.forEach(el => {\\n          const text = el.textContent?.toLowerCase() || '';\\n          const hasExpandText = expandTexts.some(expandText => text.includes(expandText));\\n          if (hasExpandText && el.click) { el.click(); expandedCount++; }\\n        });\\n      }\\n    ];\\n\\n    // Execute each strategy with a small delay\\n    for (const strategy of strategies) {\\n      try {\\n        strategy();\\n        await new Promise(resolve => setTimeout(resolve, 300)); // Small pause between strategies\\n      } catch (error) {\\n        // Log errors within strategies but don't stop the expansion process\\n        // console.log('Strategy failed in expandCollapsibles:', error.message);\\n      }\\n    }\\n    return expandedCount;\\n  });\\n}\\n\\n// Execute the main function to start the scraping process\\nreturn await processPageAndFindNext();\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-puppeteer.puppeteer\",\n      \"typeVersion\": 1,\n      \"position\": [\n        180,\n        -280\n      ],\n      \"id\": \"700ad23f-a1ab-4028-93df-4c6545eb697a\",\n      \"name\": \"Puppeteer6\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"2db5b7c3-dda3-465f-b26a-9f5a1d3b5590\",\n              \"leftValue\": \"={{ $('Code1').item.json.nextPageUrlAbsolute }}\",\n              \"rightValue\": \"\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"exists\",\n                \"singleValue\": true\n              }\n            }\n          ],\n          \"combinator\": \"and\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        1380,\n        -280\n      ],\n      \"id\": \"ccbde300-aa84-4e60-bf29-f90605502553\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"924271d1-3ed0-43fc-a1a9-c9537aed03bc\",\n              \"name\": \"url\",\n              \"value\": \"={{ $('Code1').item.json.nextPageUrlAbsolute }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        1600,\n        -380\n      ],\n      \"id\": \"faf82826-48bc-4223-95cc-63edb57a68a5\",\n      \"name\": \"Prepare Next Loop\"\n    },\n    {\n      \"parameters\": {\n        \"formTitle\": \"API Reference\",\n        \"formFields\": {\n          \"values\": [\n            {\n              \"fieldLabel\": \"api_url\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.formTrigger\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        -520,\n        -280\n      ],\n      \"id\": \"2bf8caf7-8163-4b44-a456-55a77b799f83\",\n      \"name\": \"On form submission\",\n      \"webhookId\": \"cf5e840c-6d47-4d42-915d-8fcc802ee479\"\n    },\n    {\n      \"parameters\": {\n        \"folderId\": \"1zgbIXwsmxS2sm0OaAtXD4-UVcnIXLCkb\",\n        \"title\": \"={{ $json.api_url }}\"\n      },\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -300,\n        -280\n      ],\n      \"id\": \"92fb2229-a2b4-4185-b4a0-63cc20a93afa\",\n      \"name\": \"Google Docs1\",\n      \"credentials\": {\n        \"googleDocsOAuth2Api\": {\n          \"id\": \"ch6o331MGzTxpfMS\",\n          \"name\": \"Google Docs account\"\n        }\n      }\n    }\n  ],\n  \"connections\": {\n    \"HTTP Request3\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request4\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTTP Request4\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Docs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Puppeteer1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request3\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Puppeteer1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Docs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set Initial URL\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Puppeteer6\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Puppeteer6\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Prepare Next Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Prepare Next Loop\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Puppeteer6\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"On form submission\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Docs1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Docs1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Initial URL\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"pinData\": {},\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"1dbf32ab27f7926a258ac270fe5e9e15871cfb01059a55b25aa401186050b9b5\"\n  }\n}"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "MVP Generator",
          "link": "https://old.reddit.com/r/n8n/comments/1ktun9q/you_guys_liked_my_custom_n8n_autoscaling_build/",
          "author": "conor_is_my_name",
          "time": "2025-05-23T21:08:42+00:00",
          "body_text": "I've shared a number of custom n8n builds and custom scrapers: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/crawlee-server What do you want next? Things I'm considering: a real enterprise grade chatbot using Chatwoot and AI agents Scraper deep dive using crawlee Workflow I used to update 50k product listings AI content creation for wordpress that doesn't seem like AI spam.  Actual scholarly articles. anything else? I'll probably do all 4 on the list at some point. Bonus points if you send me a real database of stuff to connect to, or a specific site you want data from.  I can demo a whole integration then."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Weekly Self Promotion Thread",
          "link": "https://old.reddit.com/r/n8n/comments/1lk9b7q/weekly_self_promotion_thread/",
          "author": "AutoModerator",
          "time": "2025-06-25T15:51:21+00:00",
          "body_text": "Weekly self-promotion thread to show off your workflows and offer services. Paid workflows are allowed only in this weekly thread. All workflows that are posted must include example output of the workflow. What does good self-promotion look like: More than just a screenshot: a detailed explanation shows that you know your stuff. Emoji's typically look unprofessional Excellent text formatting - if in doubt ask an AI to help - we don't consider that cheating Links to GitHub are strongly encouraged Not required but saying your real name, company name, and where you are based builds a lot of trust. You can make a new reddit account for free if you don't want to dox your main account."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Are we gonna talk about the 200+ step AI agent monstrosities clogging LinkedIn?",
          "link": "https://old.reddit.com/r/n8n/comments/1kqz6aq/are_we_gonna_talk_about_the_200_step_ai_agent/",
          "author": "Y0gl3ts",
          "time": "2025-05-20T07:44:02+00:00",
          "body_text": "Every other day I see one of these bros post a spaghetti diagram of an \"AI agent\" workflow with¬†200+ steps, all color-coded with a caption like: \"This replaces 3 full-time employees. Comment¬†AI agent¬†and connect with me to get the full setup.\" The amount of comments and people that want it is ridiculous, even tho it has: Dozens of unnecessary function nodes. Step after step of API calls that could be abstracted. Entire flows that fall apart the moment a response returns something unexpected. Part of the problem is you don't know what you don't know. The screenshot or video looks impressive and does most of the convincing. But these workflows aren‚Äôt scalable or maintainable and they‚Äôre definitely not \"agents.\" They‚Äôre¬†performances, automation as a LinkedIn engagement strategy. And worst of all it‚Äôs fueling this weird pipeline of people who say: \"I‚Äôve just started an AI automation agency‚Ä¶ but I can‚Äôt get clients. Can you tell me what automations you sell, to who, for how much?\" Like they want everything on a plate. I've seen many give up and pivot to YouTube content and courses. Nobody‚Äôs paying for these 200-step jokes that break the second someone forgets a semicolon. They want¬†results, not node-counts."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1kkhh1m/i_built_an_aipowered_automation_that_creates/",
          "author": "hashpanak",
          "time": "2025-05-12T02:12:37+00:00",
          "body_text": "I automated the creation of long-form, faceless YouTube videos using AI and n8n. This system churns out engaging videos and uploads them to Google Drive and YouTube, all in a no-code workflow. It‚Äôs perfect for anyone wanting to run a YouTube channel on autopilot without breaking the bank. Use It For: Creating faceless YouTube channels for motivational, educational, or storytelling content Generating video ideas and scripts for niches like history, self-improvement, or book summaries or any other use case. Automating daily, hourlyvideo uploads to grow your audience I explain the setup here -> https://youtu.be/8sIJdMwifSE I automated from ideation to publishing using n8n, OpenAI, and Flux AI (Fal.ai or GPT 4o image generation), with no third-party video assembly APIs - no JSON2VIDEO or CREATOMATE. Here‚Äôs the blueprint. How It Works This workflow turns a niche idea into a polished YouTube video in 5 stages: Video Ideation AI-Powered Ideas: Input a niche (e.g., ‚Äúmotivational‚Äù or ‚Äúhistory‚Äù) and topics (e.g., ‚Äúphilosophy‚Äù), and an AI agent (GPT-4.1 mini) generates JSON-formatted video ideas with category, story, duration, and tone. Google Sheets: Ideas are stored in Google Sheets, ready for manual tweaks or approval. Script and Audio Generation Story Agent: A second AI agent crafts a script based on the selected idea, tailored to the duration. Text-to-Speech: OpenAI‚Äôs TTS (or 11 Labs) converts the script into audio, with word-by-word transcription for precise timing. Scene Cutting and Image Generation Scene Editor: An AI agent splits the transcript into 2-5 second scenes, ensuring dynamic pacing. achieves this by transcript granularity set to words for whisper model and using an ai agent to cut at the end of a word Visuals via Flux  (Fal.ai): Another agent generates cinematic image prompts for each scene, rendered by Flux  Schnell model ($0.003/megapixel) - one of the cheapest. Video Assembly FFmpeg Magic: A custom 700-line code node uses FFmpeg to combine audio, images, and captions, cutting scenes to match spoken words with zoom-pan effects. No Third-Party APIs: Unlike JSON2Video or Creatomate, this runs locally or via a hosted API you control, or with a Code node. Publishing Google Drive Upload: The final video lands in a designated Drive folder. YouTube Push: The workflow creates a YouTube upload link and streams the video, setting title, description, and privacy status. Why This Works Save 10+ Hours/Week: Automates scripting, editing, and uploading - no manual work, except approvals. Grow your audience on autopilot Engaging Content: Dynamic scene cuts and AI-generated visuals keep viewers hooked. Budget-Friendly: NO CREATOMATE OR JSON2VIDEO - no manual editing I explain the setup here -> https://youtu.be/8sIJdMwifSE https://preview.redd.it/rja5cl4my80f1.png?width=2604&format=png&auto=webp&s=912478508f8f565686611262d7af5afc3a7f91ae https://preview.redd.it/wa7x7c7ny80f1.png?width=1236&format=png&auto=webp&s=63b19e6d209a23ebd8a5ec478992b2d4eeb29c54 Example: https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4/view?usp=drive_link"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Automation That Creates Long-Form YouTube Videos From Scratch ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1mk0e/i_built_an_ai_workflow_that_monitors_twitter_x/",
          "author": "dudeson55",
          "time": "2025-06-02T16:30:13+00:00",
          "body_text": "Now before I get started, I know this automation may be a bit controversial as there's a lot of spam already on Twitter, but I truly believe it is possible to build a Twitter / X reply bot that is useful to people if you get your messaging down and do a good job of filtering out irrelevant messages that don't make much sense to reply to. I currently run an AI Tools directory and we noticed that each day, there are a bunch of Tweets that get posted that ask for advice on choosing the best AI Tool for a specific task or job such as \"What is the best AI Tool for writing blog posts?\" or \"What is the best AI Tool for clipping short form videos?\" Tweets like this are perfect opportunity for us to jump in, and share a link to a category page or list of tools on our directory to help them find and explore exactly what they are looking for. The problem with this is it just would take forever to do this manually as I'd have to be in front of the screen all day watching Twitter instead of doing 'real work'. So, we decided to build an AI automation that completely automates this. At a high level, we use Mention.com to monitor and alert for AI Tool questions getting asked on twitter -> use a prompt to evaluate each of these tweets individually to see if it is a good and relevant question -> fetch a list of category pages from our own website -> write a helpful reply that mentions we have a page specifically for the type of tools they are looking for. Each reply we share here doesn't amount to a ton of impressions or traffic, but ultimately this is something we believe will compound over time as it lets us have this marketing motion turned on that wasn't feasible before. Here's a full breakdown of the automation 1. Trigger / Inputs The entry point into this whole automation starts with Mention.com , we setup a new keyword alert that monitors for phrases like \"Is there any AI Tool\" or \"How can I use AI to\", etc. This setup is really important as you need to filter out a bunch of the noise that doesn't make sense to reply to. It is also important that your alert that you have setup is going to be your target customer or persona you are trying to get in front of. After the alert is configured, we used the Mention.com <> Slack integration to post the feed of all alerts into a dedicated slack channel setup just for this. 2. Initial Filtering & Validation The next couple of nodes are responsible for further filtering out ineligible Tweets that we don't want to respond too. This includes checking if the Tweet from the alert is a Retweet or if the Tweet from the alert actually was from the account we want to with (avoid our own reply causing an infinite execution loop) 3. Evaluation Prompt + LLM Call The first LLM call we make here is a simple prompt that checks the text content of the Tweet from the alert and makes a decision if we want to proceed with creating a reply or if we should exit early out of the workflow. If you are taking this workflow and extending it for your own use-case, it will be important that you change this for your own goals. In this prompt, I found it most effective to include examples of Tweets that we did want to reply to and Tweets that we wanted to skip over. 4. Build Context for Tweet Reply This step is also going to be very specific to your own goals and how you want to modify this workflow. In our case, we are making an HTTP request to our own API in order to get back a JSON list of all category pages on our website. We then take that JSON and format it nicely into more LLM-friendly text We finally take that text and will include it in our next prompt to actually write the Tweet reply If you are going to use this workflow / automation, this step must be changed and customized for the kind of reply you are trying to create. If you are trying to share helpful resources with potential leads and customers, it would be a good idea to retrieve and build up that context at this step. 5. Write The Tweet Reply In this step we take all of the context created from before and use Claude to write a Tweet reply. For our reply, we like to keep it short + include a link to one of the category pages on the AI Tools website. Since our goal is to share these pages with people asking for AI Tool suggestions, we found it most effective to include Tweet input + good examples of a reply Tweet that we would personally write if we were doing this manually. 6. Posting The Reply + Notifying In Slack The final step here was actually using the X / Twiter node in n8n to post the reply to the original Tweet we got an alert for. All that is needed here is to pass in the initial Tweet Id we need to reply to and the output of our LLM call to claude which wrote the Tweet. After that, we have a couple of Slack nodes hooked up that leave a checkmark reaction and will share the Tweet output that claude decided to go with so we can easily monitor and make changes to the prompt if we found that the reply was not quite what we were looking for. Most of the work here comes from iterating on the prompt so its important to have a good feedback loop in place so you can see what is happening as the automation runs over more and more Tweets. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Q_b5uPndsLY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lplqzg/need_some_help_on_a_notion_ai_agent_job_listing/",
          "author": "enigmaCCN2023",
          "time": "2025-07-02T03:34:02+00:00",
          "body_text": "Seen so many advanced people here, impressed and in awe. I am newbie who‚Äôs on low code level ( I know basic python and worked as a pm for e-commerce and edtech for 3 + years ) I‚Äôm happy to pay as hourly consulting for now and working on similar workflows continuously in the future. I want to build a workflows to help manage job listing Automation via AI (Notion database trigger+ a few simple AI agent). The data is mostly text-based and relatively simple, but I‚Äôve hit a couple of roadblocks Here are the main issues I‚Äôm facing: 1.  After the AI extracting and summarizing the data, I need to classify and organize it properly before exporting it directly into Notion database page. However, the data classification step keeps failing, and I‚Äôm unsure which nodes or methods are best suited for this. I‚Äôve tried using the official ‚Äúedit field‚Äù node, but it doesn‚Äôt work as expected.\n\n2.  When the AI-translated result ( I wrote prompt so that the translation is output to a json file ) is placed into Notion, the formatting often gets messed up. For example, I want to modify the content blocks directly within the node since different types of content require different formatting, for instance, I want to turn the translated content directly into toggle down points, bullet points etc, I couldn‚Äôt get the input right from the translation somehow‚Ä¶ I‚Äôve already set up most of the basic nodes, but these two problems are blocking me from moving forward. For future projects, some background info: I‚Äôm running a small education consulting firm for helping students apply for all sorts of education programs ( summer schools, bachelor master PHD programs etc. ) want to build scrappers and workflows that can scrape and  updates these programs from original websites. If someone has experience with AI data classification in N8N or advanced Notion integration and is interested in working on some simple stuff like this job listing workflow first to see how it goes, please hit me up!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that monitors Twitter (X) for relevant keywords and posts a reply to promote my business (Mention.com + X API)",
          "link": "https://old.reddit.com/r/n8n/comments/1kc5kin/efficient_serp_analysis_export_results_to_google/",
          "author": "Marvomatic",
          "time": "2025-05-01T10:37:34+00:00",
          "body_text": "Hey everyone, I wanted to share something I‚Äôve been using in my own workflow that‚Äôs saved me a ton of time: a set of free n8n templates for automating SERP analysis. I built these mainly to speed up keyword research and competitor analysis for content creation, and thought they might be useful for others here too. What these workflows do: Basically, you enter a focus keyword and a target country, and the workflow fetches organic search results, related searches, and FAQs from Google (using either SerpAPI or Serper). It grabs the top results for both mobile and desktop, crawls the content of those pages (using either Crawl4AI or Firecrawl), and then runs some analysis on the content with an LLM (I‚Äôm using GPT-4o-mini, but you can swap in any LLM you prefer). How it works: You start by filling out a simple form in n8n with your keyword and country. The workflow pulls SERP data (organic results, related searches, FAQs) for both device types. It then crawls the top 3 results (you can adjust this) and analyzes the content by using an LLM. The analysis includes article summaries, potential focus keywords, long-tail keyword ideas, and even n-gram analysis if there‚Äôs enough content. All the data gets saved to Google Sheets, so you can easily review or use it for further research. What the output looks like: At the end, you get a Google Soreadsheet with: The top organic results (URLs, titles, snippets) Summaries of each top result Extracted FAQs and related searches Lists of suggested keywords and long-tail variations N-gram breakdowns for deeper content analysis Why Three Templates? I included three templates to give you flexibility based on your preferred tools, budget, and how quickly you want to get started. Each template uses a different combination of SERP data providers (SerpApi or Serper) and content crawlers (Crawl4AI or Firecrawl). This way, you can choose the setup that best fits your needs‚Äîwhether you want the most cost-effective option, the fastest setup, or a balance of both. Personally, I‚Äôm using the version with Serper and Crawl4AI, which is pretty cost-effective (though you do need to set up Crawl4AI). If you want to get started even faster, there‚Äôs also a version that uses Firecrawl instead. You can find the templates on my GitHub profile https://github.com/Marvomatic/n8n-templates . Each template has it's own set up instructions in a sticky node. If anyone‚Äôs interested, I‚Äôm happy to answer questions. Would love to hear any feedback or suggestions for improvement!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1k0m0nq/seo_marketing/",
          "author": "Downey07",
          "time": "2025-04-16T14:38:36+00:00",
          "body_text": "Hi everyone, I‚Äôve been exploring n8n for workflow automation and love its flexibility. But I‚Äôm curious: Is it possible to fully (or mostly) automate effective SEO and digital marketing using n8n? For example: - Automating keyword tracking + content optimization. - Scheduling/publishing posts with AI-driven SEO checks. - Auto-generating reports (GA4, Search Console, etc.). - Even link-building outreach or social media engagement. Has anyone built workflows like this? Do they actually compete with manual strategy, or just save time on repetitive tasks? Would love real-world examples or caveats!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need some help on a notion+ ai agent job listing workflow, future projects down the line too",
          "link": "https://old.reddit.com/r/n8n/comments/1knzwng/i_created_a_full_agent_service_scheduler_using/",
          "author": "raafaell",
          "time": "2025-05-16T12:43:55+00:00",
          "body_text": "Hey everyone! üëã I've been working with an n8n workflow to manage WhatsApp Business interactions for a landscaping company, and I wanted to share how it works for those interested. Overview This n8n workflow is designed to streamline communication via WhatsApp for a landscaping business called Verdalia. It automates message handling, reservation management, and customer service while maintaining a professional and friendly tone. Key Features Message Routing: Uses a Webhook to receive incoming WhatsApp messages. Messages are categorized as text, audio, or image using the Switch node. Message Processing: Text messages are processed directly. Audio messages are converted to text using OpenAI's transcription model . Image messages are analyzed using the GPT-4O-MINI model. Automated Response: Uses the OpenAI Chat Model to generate responses based on message content. Replies are sent back through the Evolution API to the WhatsApp contact. Reservation Management: Integrates with Google Calendar to create, update, and delete reservations. Uses Google Sheets to log reservations and confirmation status. Smart Handoff: If the customer requests human assistance, the system collects the best time for contact and informs that Rafael (the owner) will follow up. Confirmation and Follow-up: Sends confirmation messages via WhatsApp. Tracks the status of reservations and follows up when necessary. Why Use This Workflow? Efficiency: Automates routine tasks and reduces manual input. Accuracy: Uses AI to understand and respond accurately to customer messages. Customer Experience: Maintains a professional and responsive communication flow. Would love to hear your thoughts or any experiences you have with n8n workflows like this one! If you want to download this free workflow, it's available with an instructional youtube video here"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring",
          "link": "https://old.reddit.com/r/n8n/comments/1l569vz/so_far_this_is_what_i_have_learnt_in_last_6/",
          "author": "digit540",
          "time": "2025-06-06T23:09:06+00:00",
          "body_text": "I‚Äôve learnt so far: - Create simple and medium complex automation workflow using n8n. Learnt to use API to connect to various services - Filter, route, and transform data between steps using built-in functions and JavaScript, Merge, split, batch, or loop through data with nodes like IF, Switch, Set, Merge - Call OpenAI, Gemini endpoints for content generation, classification, or analysis. Handle inbound webhooks for real-time integrations (e.g., Twilio, ServiceNow). - hosting and scaling n8n on Google Cloud using Docker and Traefik. Containerized services using Docker Compose, including n8n, traefik. Will be doing next on Azure, AWS What Next: - Scaling n8n‚Äôs queue mode with Redis and multiple worker instances, but still unclear on how to go about- - How to share credentials across different environment with encryption - How to use role-based access, audit logs - backup/restore strategy for disaster recovery and migration. What else?  I am sure the list to do next is much longer than I have enlisted here. If anyone is running n8n at scale or in regulated environments, I‚Äôd love to hear what‚Äôs worked for you or you learnt the above on your own or working on real project."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Efficient SERP Analysis & Export Results to Google Sheets (SerpApi, Serper, Crawl4AI, Firecrawl)",
          "link": "https://old.reddit.com/r/n8n/comments/1jtaf3i/how_to_selfhost_n8n_on_railway_setup_community/",
          "author": "StudyMyPlays",
          "time": "2025-04-07T02:03:13+00:00",
          "body_text": "Master AI Automation: \nYour Guide to Self-Hosting n8n\nThis comprehensive guide provides everything you need to enhance your automation workflows. \nLearn how to:\n * Self-host n8n on Railway for free with step-by-step instructions.\n * Set up and manage community nodes.\n * Integrate Firecrawl as a Multi-Content Parser (MCP).\n * Build a custom AI agent. üëâ Read Full Guide On Medium Article"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lyd7uo/my_first_full_n8n_project_automated_ai_horror/",
          "author": "Ok_Friend_9829",
          "time": "2025-07-12T22:41:45+00:00",
          "body_text": "Hi, so i had been working on this project for about 3 weeks now since it's my first workflow (i was introduced to n8n and automation around a month ago soo i was testing and learning at the same time but i already got a pretty solid background in coding with js and know about apis and http request, crud ....), this project is split into 4 parts/workflows, the first retrieves stories from subreddits you specify, the second formats the stories and splits them into ready-to-use short story parts, the third one splits the script into scenes and each scene has its own image prompt that will work well with the speech, a chunk of the script and sound query which i will use later for the sound effect, then it uses kokoro tts to generate speech for the scene and using freepik api to generate pictures (they have flux and other models which have generous free tier), then i use nca-toolkit to use the zoom effect on the img and it turns it into a video, concat the video with the speech, add fade in and out, change its resolutions to 1080x1920, then add subtitles with word highlighting, after the loop on every scene is done then it concats all the scenes into one video, adds sound effect that we get using freesound api with the sound query we generated then it cleans up all the additional files we left in minio and it leaves only the video and a thumbnail to be used (this workflow takes about 40min - 1hr because i use these tools locally in docker, i have an i5 7gen and 8gb ram ddr4 which i restricted to be using only 6gb memory and 1gb memory swap, 3 cpus out of 4 to avoid the constant crashes), the fourth will get the story content, url, title, ... And then returns descriptions, titles, cta's, tags to use on different platforms. And these workflows are completely free and i will drop them as soon as i finish cleaning and fixing all the issues in it (i will add a fifth workflow which will work for marketing purposes whenever i upload a video then it makes posts to different platforms). Qst 01: what should i do next as a project to learn more about n8n and the whole automation process ? Qst 02: what are actually the profitable ways to earn money as a frontend developer and a n8n beginner ? Qst 03: this can be off the goal of this subreddit but i wanna know if this project can be profitable. because i have heard that fully ai generated videos aren't monitized and this content is not a profitable way soo please if not could you provide anything that i can do with this workflow ? By the way this is my YouTube channel if you want to check out the resulted videos : https://www.youtube.com/@TheHollowVoice-m7k Thank you so much and best of luck to you all"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "SEO & Marketing",
          "link": "https://old.reddit.com/r/n8n/comments/1ksrskx/stock_images_generation_for_adobe_stock_creatives/",
          "author": "VictorBwire",
          "time": "2025-05-22T14:14:51+00:00",
          "body_text": "Overview This n8n workflow system is composed of three integrated workflows that generate 1920 images in 24 hours Text Prompt Generator ‚Äì Generates high-quality, photorealistic prompts based on topics. Adobe Stock for Creatives ‚Äì Uses those prompts to create images, analyze metadata, and upload final assets to Google Drive and Sheets. Error Logger‚ÄîNotifies you via Telegram and logs any processing errors to a dedicated Google Sheet for monitoring and debugging. Combined, they provide a powerful automation pipeline for AI-driven stock content generation.Key Technologies Used n8n for workflow automation Google Sheets for prompt, metadata, and error tracking Google Drive for asset storage OpenAI (GPT-4o-mini) for prompt and metadata generation PIAPI for image generation Telegram for user notifications Workflow A: Text Prompt Generator. This is the initial workflow that runs daily at 4 AM to create fresh image prompts based on ideas 1. Trigger Schedule Trigger: Executes every day at 4 AM. Fetch Topic Google Sheets1: Retrieves the first topic marked as Created = NO from the \"Ideas\" sheet. Prepare Prompt Generation Set Topic: Passes the topic as a variable for prompt generation. Create Loop Indexes: Creates an array of 50 to simulate multiple batch jobs (used for merging with prompts). Generate Prompts Prompt Generator: Uses GPT-4o-mini with the instruction: Generate 20 unique, highly realistic, photorealistic image prompts based on the topic. Each prompt should describe a specific visual scene with concrete details like environment, lighting, perspective, colors, and objects. Return as a plain list. (Results per Run 1000 Prompts) Post-process Prompts Split Prompts: Breaks the response into individual prompts. Merge Batches: Merges the prompts with loop index items. Store Prompts Google Sheets2: Appends each prompt to the \"Generated Pmts\" sheet with Images created = NO. Workflow B: Adobe Stock for Creatives. This is the main execution workflow triggered every 3 minutes to process prompts and generate stock images 1. Trigger & Initialization Schedule Trigger: Runs every 3 minutes. Set Date Info: Converts to your timezone and creates date strings. Filter Data Date: Prepares formatted values for naming folders/sheets. Fetch Prompt Google Sheets: Gets one prompt where Images created = NO. Select Prompt: Extracts the prompt text and row number. File Infrastructure Check/Create Google Sheet: Verifies if the day's sheet exists; if not, duplicates a blueprint. Check/Create Drive Folder: Verifies/creates the folder to store generated images. Image Generation Edit Fields: Sets prompt and negative prompt text. Generate Image: Sends request to PIAPI to generate 4 images. Wait 20 Seconds: Delays to allow PIAPI to process. Get Images: Polls PIAPI for image URLs. Image Handling Check Response: If no images returned, loops back to wait. Split Out: Separates image URLs. Download Images: Downloads each image. Image Processing Comp Images: Shrinks images for metadata generation. Resize Image X2: Upscales for high-res upload. Metadata Generation Analyze Images: Sends image to GPT-4o-mini to generate: Split Out Data: Separates results per image. Parse OpenAI Response: Converts JSON to n8n-readable format. Format & Merge Numbering: Adds sequence to each image. Merge: Combines binary and metadata. Sanitize Filenames: Converts titles to clean, lowercase, underscore-based file names. Upload & Log Upload Images: Saves to Google Drive folder. Google Sheets3: Writes metadata to the new sheet. Google Sheets4: Marks original prompt as Images created = YES. Telegram: Sends message confirming upload. Workflow C: Error LoggerThis optional workflow is triggered when an error occurs in the image generation or metadata processing workflow.1. Trigger Can be connected to the Error Trigger node from any primary workflow. Capture Error Context Captures key error details: Log to Google Sheets Appends a new row to a dedicated \"Error Log\" sheet with the captured details. Telegram Notification Sends error alerts to Telegram. Highlights üîÅ Automated cycle: From topic ‚Üí prompts ‚Üí images ‚Üí metadata ‚Üí final assets üé® Detailed prompts: Ensures photorealism and creative diversity ü§ñ AI metadata: Optimized for Adobe Stock standards üìÅ Smart file handling: Unique folders and sheets per day üì¨ Real-time updates: Telegram notifications for visibility ‚ö†Ô∏è Robust error logging: Track failures with full context and notifies you to telegram Ideal Use Cases Stock photo creators Agencies generating niche content daily AI art businesses scaling uploads Print-on-demand sellers looking to automate content creation Final ThoughtsThis three-part n8n system turns daily ideas into publishable, metadata-rich images with full automation and error transparency. It‚Äôs modular, scalable, and ideal for creatives and content businesses looking to streamline their workflow. Ready to sell, deploy, or scale with confidence. Book Consultation https://calendly.com/victor_automations/2025"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Created a Full Agent Service Scheduler using Evolution API (WhatsApp)",
          "link": "https://old.reddit.com/r/n8n/comments/1jsk6b5/need_help_building_this_willing_to_pay/",
          "author": "EZ_PZ_LM_SQ_ZE",
          "time": "2025-04-06T02:39:10+00:00",
          "body_text": "Hey Reddit, I‚Äôm working on a project where I‚Äôm building a highly realistic Twitter bot network using n8n , Puppeteer , and GPT ‚Äî all running locally on a dedicated laptop. Speed doesn‚Äôt matter ‚Äî I‚Äôm prioritizing realism, variability, and stealth. Here‚Äôs what I‚Äôm aiming to build and what I could use help with. The Core Idea I want to control 50‚Äì100 Twitter accounts , each acting like a real person. Each bot will: Log in individually (no Twitter API ‚Äî browser-based only using Puppeteer) Scroll through their feed Like posts Occasionally retweet or follow accounts Post their own tweets randomly (including text + image tweets) Stay logged in for a random amount of time (between 1‚Äì10 minutes) Log out and rotate to the next account This bot loop runs from 6 a.m. to 1 a.m. every day , simulating sleep during off hours. Manual Tweet Trigger (Main Account) Whenever I manually tweet something from my main account , I‚Äôll trigger a webhook in n8n. That webhook will: Store the tweet text + link Initiate an engagement wave from a random group of bots that will: Mostly like the tweet Some retweet it A few reply or quote tweet it Replies will be generated contextually via GPT using the actual tweet content. Reply Workflow When replying to the main account: The bot reads the tweet text. GPT generates a reply based on the tweet and the bot‚Äôs personality . The bot posts the reply via Puppeteer (browser control). Image Tweeting System Some bots will also post their own original tweets with photos. The system will: Grab a random image from a local folder Use GPT to describe the image Re-prompt GPT to write a caption for the photo Tweet the image + caption Delete the image from the folder after use to prevent duplication Anti-Detection Measures Bots reuse saved login cookies to avoid repeated logins and security prompts Using stealth plugins and headless Chromium fingerprint spoofing (user-agent, timezone, screen size, etc.) Each bot uses a dedicated proxy/static IP to avoid shared-VPN detection Bots engage in non-main account activity as well (liking trending posts, following accounts, replying to niche tweets) Advanced Features I‚Äôm Implementing Here‚Äôs where the fun really starts ‚Äî I‚Äôm layering in complexity to make these bots more believable: 1. Bot Personalities Each bot has its own: Tone (playful, chill, sarcastic, poetic) Interests (dogs, coffee, tech, memes, food, etc.) Emoji usage level Reply style (casual, dramatic, minimalist) Fake timezone (used to limit ‚Äúawake‚Äù hours) Personalities will guide GPT responses and behavior patterns. 2. Bot-to-Bot Engagement Bots will: Follow each other Occasionally reply to or quote tweet each other Build micro ‚Äúcommunities‚Äù to simulate real friend groups or followers 3. Cooldowns and Suspicion Limits Each bot will: Track its last like / RT / reply time Follow cooldown logic so it doesn‚Äôt over-engage Get temporarily paused if flagged or if an error occurs (e.g., CAPTCHA) 4. Fake Timezone Schedules Each bot has its own fake timezone (e.g., GMT+2, GMT-5) It only runs sessions when it‚Äôs ‚Äúawake‚Äù in its timezone Adds realism and naturally staggers engagement waves 5. Post-Like Decay Logic Bots will: Prioritize liking tweets within the first 2 hours after they're posted Scale down likes/replies/RTs after 12‚Äì24h This makes main account tweet engagement look more organic 6. Internal Dashboard (n8n-powered) I‚Äôll be tracking: Which bots are active or on cooldown What tweets each bot engaged with Error logs (e.g., login failures, blocked actions) Manual override controls (to force engagement mode, pause accounts, etc.) 7. Event-Based Surges Bots will temporarily increase engagement based on: Keywords in tweets (e.g., ‚Äúgiveaway‚Äù, ‚Äúdrop‚Äù, ‚Äúlaunch‚Äù) Specific time triggers (e.g., Friday night, product drop window) What I Could Use Help With If you‚Äôve ever worked with Puppeteer, n8n, GPT, or any sort of stealth automation ‚Äî I‚Äôd love your input on: How to structure workflows cleanly (centralized or per-bot?) How to manage Puppeteer cookies per bot inside n8n Assigning unique proxies per bot in a scalable way Best practices for image deletion + storage Any known quirks with Puppeteer + Twitter that I should plan around Pls comment or DM."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l3i75y/do_ai_seo_agents_actually_work_looking_for_real/",
          "author": "maximemarsal",
          "time": "2025-06-04T21:51:05+00:00",
          "body_text": "I‚Äôve seen a surge of ‚ÄúAI SEO agents‚Äù whether it‚Äôs automated content creation, keyword clustering, internal linking, or even full-blown workflows. Some are open-source, others are expensive SaaS platforms promising insane traffic growth. But here‚Äôs the thing‚Ä¶ Do they really work in practice? I‚Äôm not talking about theoretical potential or marketing hype. I mean actual improvements in clicks, impressions, rankings, etc. If you‚Äôve implemented one, I‚Äôd love to hear: ‚Ä¢ Which tools or flows you used\n‚Ä¢ What kind of automation you set up\n‚Ä¢ If it‚Äôs open-source or paid\n‚Ä¢ Most importantly: what kind of results you got (any GSC screenshots appreciated üôè) Trying to figure out what‚Äôs worth investing time/money into and what‚Äôs just vaporware. All insights welcome even failures. Thanks so much in advance \n!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "So far this is what I have learnt in last 6 months in n8n, what else do I need to do next so as to be able to say... work for an enterprise client needing AI automation ...may be using n8n.",
          "link": "https://old.reddit.com/r/n8n/comments/1lfcnar/from_reddit_problem_to_business_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-19T14:46:50+00:00",
          "body_text": "The best way to make money is by finding and solving real-world problems that people are actively discussing. This guide provides a step-by-step framework for building a no-code AI agent that automatically finds these business problems on Reddit and generates potential money-making solutions, helping you uncover profitable niches you might have otherwise missed. This workflow automates the entire discovery process. Here are the actionable tips to build this idea factory yourself: Step 1: Find the Problems (Reddit Node) Start your n8n workflow with the \"Reddit\" node. Configure it to search specific subreddits, like r/smallbusiness , for posts containing keywords that indicate a need, such as \"looking for a solution\" or \"recommendation\". Step 2: Filter the Noise (IF Node) Add an \"IF\" node to filter the results for quality. You can set conditions to only process posts with real engagement, like having at least two upvotes, and ensure the idea is recent by only including posts from the last six months. Step 3: AI Problem Identification Connect an AI Agent to the filtered posts. Its only job is to read the post and determine if it describes a business-related problem, then output a simple \"yes\" or \"no\". Step 4: AI Solution Generation For every post that the AI identifies as a problem, send it to another AI tool, like the OpenAI node. Prompt this AI to suggest a business idea or service that could solve the problem described in the post. Step 5: Store Your Goldmine (Google Sheets) Finally, take all the organized data‚Äîthe original post, the problem summary, and the new business idea‚Äîand save it into a Google Sheet for easy review and analysis.\nIf you can do this, you will have an automated system that finds profitable niches and delivers validated business ideas straight to a spreadsheet. Made a full video to explain everything:- https://youtu.be/Aovh878cJc4 What subreddits would you monitor with a system like this? Let me know in the comments!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1ltnj89/help_needed_automate_daily_linkedin_posts_with/",
          "author": "AffectionateCap3371",
          "time": "2025-07-07T07:02:15+00:00",
          "body_text": "Hi everyone! I'm looking to build an automation in n8n that can post on LinkedIn every day at a specific time , and each post should include a newly generated image (based on predefined logic or dynamic content). Key features I need: Trigger: Once a day at a set time Generate an image dynamically (can use an external service or API) Post on my LinkedIn profile with text + generated image I've explored a few nodes and integrations, but I‚Äôm stuck on connecting all the pieces together in a reliable and clean workflow. If anyone has done something similar or can help guide me through the setup, I‚Äôd really appreciate it! üëâ I'm also happy to provide a testimonial or review in return if someone helps me get this working smoothly. Thanks in advance for your time and support!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How To SelfHost n8n On Railway, Setup Community Nodes, and Integrate FireCrawl MCP Server without touching a Single Line Of Code !",
          "link": "https://old.reddit.com/r/n8n/comments/1lyd7pe/four_types_of_ai_systems_that_actually_sell_and/",
          "author": "ChaseAI",
          "time": "2025-07-12T22:41:32+00:00",
          "body_text": "There was a great post on r/AI_Agents the other week that went through 6 months of Upwork data to get some insight on what sorts of AI systems companies are no shit paying money for. Believe it or not, it wasn't ASMR glass fruit cutting videos. Here is the quote: The Common Pain Points I Saw Repeated Over and Over: ‚ÄúI‚Äôm drowning in lead gen, I need this to run on autopilot‚Äù\n‚ÄúI get too many junk messages on WhatsApp / LinkedIn ‚Äî need something to filter and qualify leads‚Äù\n‚ÄúI have 10,000 rows of customer data and no time to sort through it manually‚Äù\n‚ÄúI want to turn YouTube videos into blog posts, tweets, summaries‚Ä¶ automatically‚Äù\n‚ÄúCan someone just connect GPT to my CRM and make it smart?‚Äù Exact Automations Clients Paid For: WhatsApp ‚Üí GPT lead qualification ‚Üí Google Sheets CRM\nAuto-reply bots for DMs that qualify and tag leads\nBrowser automations for LinkedIn scraping & DM follow-ups\nn8n flows that monitor RSS feeds and creates a custom news aggregator for finance companies For anyone whose been working with real clients in this space for any amount of time, this should come as no surprise. I think you can divide this up into roughly four categories: lead generation automations, lead qualification automations, content creation agents, and CRM integrated RAG agents. Those four alone cover probably 80% of the solutions I've sold since I started my agency so it makes sense to see that echoed in the data. I think people who want to get into the ai agency business get extremely overwhelmed with the breadth of \"viral\" n8n workflows shoved in their face on social media so I thought it would be helpful to not only call out the ones mentioned in this post but also run through an example for each. I'll link my YouTube video that goes over each of them in depth. The workflows for each are also freely available in the respective video description. ----- 1) Lead Generation https://www.youtube.com/watch?v=bFxWRkWAFzs User inputs the leads he wants, research is conducted on the sourced leads via Apollo + Tavily, research is used to create custom messaging, all info is then sent to Instantly.ai. I like this one since it all gets rolled into Instantly which handles all the cold email BS (email warm up, etc) and you won't have to bother with creating some fancy dashboard from scratch. 2) Lead Qualification https://www.youtube.com/watch?v=Vmgmva3dL44 This is a very simple foundation you could take a build upon to qualify leads. This uses gmail, but that could obviously be swapped for a true CRM input. Lead's info comes in, AI sorts / classifies / qualifies, and then you go down whatever path is appropriate from there. Really easy to customize and implement for a client. 3) Content Creation https://www.youtube.com/watch?v=mzxdtDCZbNY https://www.youtube.com/watch?v=Qp3h7WLYpH0 Obviously content comes in a billion different forms but it doesn't need to be complicated. Honestly I've found that clients are more pressed when it comes to just consistently posting the content across multiple platforms than they are about creating it but almost everyone wants some mega-simple LinkedIn post generator. 4) RAG https://www.youtube.com/watch?v=nwR5519zTC8 This form of RAG is as simple as it gets but I'm telling you most client's \"RAG Agent\" is a glorified FAQ chatbot that is able to locate and link internal documents to lazy employees. Understanding this very basic form of RAG will get you 90% of the way there. ----- The big takeaway is that the technical bar you must cross in order to make money selling AI solutions is not nearly as high as you think it is (the real hurdle is sourcing clients but that's an entirely different problem). The main issue is people get pulled into a million different directions trying to copy these over-engineered and flashy n8n workflows that are usually completely worthless instead of just mastering a handful of tried and true value generators. Hope this helps."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "My First Full n8n Project ‚Äì Automated AI Horror Video Generator (Feedback & Ideas Welcome)",
          "link": "https://old.reddit.com/r/n8n/comments/1m262vw/built_an_ig_carousel_generator_using_n8n_nodejs/",
          "author": "cyrusbuga",
          "time": "2025-07-17T12:21:27+00:00",
          "body_text": "Hey r/n8n fam,\nJust wrapped up a fun build using n8n + function nodes (Node.js) to automate an entire Instagram carousel generator. Thought I‚Äôd share it here! ‚öôÔ∏è What it does:\nTakes in a query like: \"prompts to 10x your productivity\"\nGenerates viral GPT-style prompts\nFormats them perfectly into an Instagram carousel structure\nSaves all assets to Google Drive (for sharing/posting)\nPosts go to my IG page üëâ @dope.gpt\nü§ñ Chat model setup:\nI originally tried using DeepSeek LLaMA R1, but ran into major headaches parsing its output (nested structures, inconsistent keys, etc.) in n8n.\nSo I switched to OpenRouter ‚Äî which gave more predictable and clean outputs for now. üôã‚Äç‚ôÇÔ∏è If anyone here has managed to handle DeepSeek LLaMA‚Äôs JSON formatting properly inside n8n, I‚Äôd love some help! Would still love to make that work. üß† Tech stack:\n100% built in n8n, no outside scripts\nHeavy use of function nodes (Node.js inside)\nGoogle Drive for storing assets\nFinal images posted manually to IG (might automate that next üëÄ)\nüí¨ Happy to drop screenshots of the workflow and the carousel posts (just ran a batch today). Open to feedback, improvement ideas, or sharing the setup if anyone‚Äôs interested. Let‚Äôs push n8n to the edge together üöÄ"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1lbnf0i/summoning_exnewbies/",
          "author": "Longjumping-Sky-8810",
          "time": "2025-06-15T00:31:22+00:00",
          "body_text": "Dear ex-newbies, My question is, when you guys started \n, which youtube tutorial or some other way helped you the best, so I dont have to watch all the tutorials or to avoid rage quitting. I recently started learning n8n with a non coding background, with the help of youtube tutorials. But majority of the beginners tutorials include some complex part, which the YouTuber gives you to copy paste, and it stresses me out, when I have to customise it, nothing works."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Stock images generation for Adobe stock creatives ‚Äì Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l461b9/i_trained_chatgpt_to_build_n8n_automations_for_my/",
          "author": "Organic-Injury4495",
          "time": "2025-06-05T18:02:23+00:00",
          "body_text": "This prompt is a thinking partner disguised as a tutorial. It doesn‚Äôt just teach you how to use n8n, it slows you down, helps you reflect, and guides you to build something with real leverage. It begins by asking for your business context, not to fill time, but to ensure every node you build actually matters. Then, it leads you through a calm, clear conversation, helping you spot where your time is bleeding and where automation could buy it back. Once you find the high-leverage process, it walks you through the build like a complete beginner, one node at a time, no assumptions, no skipped steps, asking for screenshots at milestones to confirm you‚Äôre on track. It‚Äôs not just a prompt to follow, it‚Äôs a prompt to think better, automate smarter, and build freedom into your workflow from the first click."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lt91sn/i_built_an_n8n_ai_agent_that_produces_educational/",
          "author": "Silent-Willow-7543",
          "time": "2025-07-06T18:59:32+00:00",
          "body_text": "In this step-by-step tutorial, I show you how to build a fully automated workflow using AI Agents and n8n to generate geography explainer videos on autopilot ‚Äî perfect for teachers, creators, or anyone looking to share knowledge at scale. What you‚Äôll learn:\n‚úÖ Generate video scripts with AI Agents\n‚úÖ Automatically source images, videos, and maps\n‚úÖ Assemble and edit videos hands-free\n‚úÖ Publish educational content consistently This workflow saves hours and helps you teach the world smarter without coding or manual editing. Tools used: LLM model (Gemini)\nGoogle APIs (Image generation)\nVideo generation: Runway ML\nVoice over (Elevenlabs)\nCaptions (Elevenlabs)\nFFmeg for combining the audio, video and captions into one MP4 file Sample video - Life in the Amazon: Uncontacted Tribes https://youtu.be/EZySnlX_yyQ"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need help building this. Willing to pay.",
          "link": "https://old.reddit.com/r/n8n/comments/1lu272i/convert_pdf_to_html_with_hn_structure/",
          "author": "EcceLez",
          "time": "2025-07-07T18:38:26+00:00",
          "body_text": "Hello everyone, I am looking to build a workflow to convert very long PDF documents into HTML with a perfect Hn structure. We're talking about files ranging from 180 to 350 pages. I would like some advice on how to proceed because I am encountering difficulties finalizing my workflow. My current process: A GDrive trigger retrieves the PDF and extracts the text The text is given to an LLM which analyzes and extracts the Hn structure in JSON A chunker splits the PDF text into blocks of 25,000 characters Then the outline + chunks are fed into another LLM node that loops, tasked with applying the HTML code with the Hn structure on all identified headings A code node then assembles the pieces According to my tests: You cannot give the text to convert directly into HTML to an LLM; no modern LLM can generate the output (even Gemini 2.5 Pro crashes) It is absolutely necessary to provide the outline with the chunks so the LLM can detect the hierarchical level of the outline parts it analyzes from the 2nd chunk onward Gemini 2.5 Flash generates errors in HTML creation about 10% of the time Gemini 2.5 Flash Lite does not work at all Gemini 2.5 Pro works well but is too expensive, I have over 1200 files to process All models crash about 50% of the time when looping over the chunks The Adobe API fails completely to extract the outline, results are catastrophic I also tried locally with Python and the LibreOffice engine but nothing works properly‚Ä¶ to the point I cannot do it without an LLM ‚Äì even though the LLM results are imperfect When I try to reduce the chunk size, I increase the token count because it increases how many times the outline is reanalyzed‚Ä¶ and above all it does not solve my main problem which is the LLM crashing when looping over chunks‚Ä¶ on the contrary, it quickly increases the number of chunks Any ideas to get out of this deadlock? Last point regarding the chunk-based process: the goal is to make my workflow scalable for PDFs of any size, and to control the size of the blocks analyzed by the LLM in order to increase the predictability of the results."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1ld61y9/i_built_a_nocode_n8n_gpt4_recipe_scraperturn_any/",
          "author": "automayweather",
          "time": "2025-06-16T21:59:29+00:00",
          "body_text": "I‚Äôve just shipped a plug-and-play n8n workflow that lets you: üó∫ Crawl any food blog (FireCrawl node maps every recipe URL) ü§ñ Extract Title | Ingredients | Steps with GPT-4 via LangChain üìä Auto-save to Google Sheets / Airtable / DB ‚Äîready for SEO, data analysis or your meal-planner app üîÅ Deduplicate & retry logic (never re-scrapes the same URL, survives 404s) ‚è∞ Manual trigger and cron schedule (default nightly at 02:05) Why it matters SEO squads: build a rich-snippet keyword database fast Founders: seed your recipe-app or chatbot with thousands of dishes Marketers: generate affiliate-ready cooking content at scale Data nerds: prototype food-analytics dashboards without Python or Selenium What‚Äôs inside the pack JSON export of the full workflow (import straight into n8n) Step-by-step setup guide (FireCrawl, OpenAI, Google auth) 3-minute Youtube walkthrough https://reddit.com/link/1ld61y9/video/hngq4kku2d7f1/player üí¨ Feedback / AMA Would you tweak or extend this for another niche? Need extra fields (calories, prep time)? Stuck on the API setup? Drop your questions below‚Äîhappy to help!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Do AI SEO Agents actually work? Looking for real results üôèüèª",
          "link": "https://old.reddit.com/r/n8n/comments/1l5tpyd/serious_question_why_every_n8n_agency_owner_with/",
          "author": "mazerfazersister",
          "time": "2025-06-07T19:58:58+00:00",
          "body_text": "i want to start content creation. and when i want to get inspiration i looked at most of the content creators who are telling \"i sell this automation to $5k\" or \"i automated this lead generation agent, comment for the workflow\" style contents, but these type of contents only attracts other n8n sellers not buyers. do you know any content creator or agency owner who creates for buyers not other agency owners?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1kx98ls/what_would_you_like_to_seeuse_in_an_n8n_workflow/",
          "author": "Tahycoon",
          "time": "2025-05-28T05:32:29+00:00",
          "body_text": "I‚Äôve been working on something the n8n community might find useful ‚Äî an AI-powered workflow generator. The goal is to save time on complex automation setups and let you export or tweak them inside tools like n8n, Make, etc. I‚Äôm about 70% done and have trained it on 4k+ templates, so far. Figured I‚Äôd ask now while I‚Äôm still building: ‚Äì What kind of automations would you use this for? ‚Äì Any features or ideas that would make it more useful? ‚Äì Any pain points when building workflows that AI could help with? Would really appreciate your input! Trying to make this genuinely helpful."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I need your help to write the description of my quote",
          "link": "https://old.reddit.com/r/n8n/comments/1lpxnzl/i_need_your_help_to_write_the_description_of_my/",
          "author": "growth-ai",
          "time": "2025-07-02T14:43:07+00:00",
          "body_text": "Hello üëã I'm launching an n8n agency in France. We've recently had our first prospect who wants to automate the processing of data from Google Form and then produce a .pdf summary. I'm in the process of drawing up the quote for the service and I'm wondering what to include in the description of each service. Here's what I've come up with: 1- analysis and design phase : - expression of requirements - specifications - design workflows - validation by the customer 2- setup of the n8n instance + data base : - n8n account setup (cloud/self hosted) - configuration of credentials for each tool - addition of API keys - creation of a storage database - creation of a RAG vector database 3- development phase : - development of workflows - creation of an AI cell - test run of automations - output analysis 4- deployment phase : - putting workflows into production - process security 5- maintenance : - functional maintenance of workflows - data hosting - invoicing for tokens used Am I missing something? I really need your help to know what to add/remove from my quotes. Thank you, love from France"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1l9y1iy/i_built_a_giant_red_wifienabled_button_to_trigger/",
          "author": "biozork",
          "time": "2025-06-12T21:10:41+00:00",
          "body_text": "Giant Red WiFI-enabled button to trigger my n8n workflows What is it ? Well - its a Big Red Mushroom shaped push button, with a raspberry pi zero 2 on the inside (it just exactly fits inside the chassis), with a usb cable powering the pi (in the video I use a powerbank.. but you can use whatever you want to usb-power source). I soldered the button switch to the GPIO of the pi, and added a tiny python script that calls a predifined webhook. Its wireless in that sense that it connects to a specified wifi (hotspot on my phone), and from there it can to the python call. But why? I really wanted something physical for triggering workflows doing live n8n demos. And.. it is really satisfying pressing that big red mushroom button. It also has quite an amazing sound (if you listen to the video, you can clearly hear the spring mechanism). In the video I have a n8n webhook trigger in testmode, and I added the test-url to the raspberry pi python script. When I enable the webhook listener in n8n, it listens for calls to the webhook and when I press the big red button, the raspberry pi calls the test-webhook url and triggers the flow. The demo flow, actually controls a philips hue lightbulp right above my desk (via the philips hue node), and I can toggle the lightbulp on and off with this flow. But really - it could be used for anything. Think BIG - maybe a \" launchbutton \" (maybe you played satisfactory?) that triggers a flow to commit to github, or starts deployment. Maybe it triggers a flow that pulls a random winner from a list and send them a happy congratulations mail, written by an AI agent. Whats the hardware ? Eaton FAK-R/KC11/I palm switch (red) Raspberry Pi Zero 2 W Power-bank (just one I had already) 16gb sd card Two jumper wires with pin header A micro-usb to usb-a cable (for powering the pi) There are cheaper buttons out there, but I wanted one that had a really nice feel and sturdy quality. Was it difficult to built Not really. I built it (including the soldering of the two wires) in perhabs an hour. But thats mostly thanks to ChatGPT, helping me get the right OS onto an ssd and made all the scripts for me. The pi is running \"headless\". As soon as it boots it connects to the WiFi I configured in the image placed on the sd card, and then I can connect to it via SSH and control the pi that way. Before yesterday I didnt know how to do that, but ChatGPT showed me how :) A few concerns while building I was very concerned about picking the correct pins for soldering, so I scoured the net to find \"gpio diagrams\" to confirm that I was actually soldering the right pins. I also did some coding errors, because I was not reading all the details that ChatGPT wrote, so I was getting concerned about the soldering again. But after re-reading the python script I found my error and it worked perfectly from then on. I was also concerned about wifi connectivity. The pi connects to 2.4ghz networks, and creating a hotspot on my phone and letting it connect to that works amazingly well. I can go 20-30 meters away and still see the button being connected to my hotspot. It works much better than I hoped for. And will work perfectly for live n8n demos. Future ideas. I would love to have a battery or a \"UPS\" hat for the pi, inside the button. Maybe with a small on/off-switch on one side, and a usb-c outlet for charging the battery. Maybe an external LED to show if its powered up, could be really nice (if I peek inside the hole where the usb cable comes out, I can barely see the light onboard pi right now). What do you think? Do you have any feedback for me ? Do you think it is a stupid idea (well, it kind of is.. but hey, its fun), or would you love to use it for your workflow. And if so, what would that workflow be that such a nice Big Red Button could complete ? I'm also considering writing up a tutorial, and posting it on Medium. Would you be interested in one"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built a Giant Red WiFi-enabled Button to trigger My Automations. What do you think ?",
          "link": "https://old.reddit.com/r/n8n/comments/1jxcoav/create_business_docs_and_finds_leads/",
          "author": "Feisty-Economist6113",
          "time": "2025-04-12T08:30:06+00:00",
          "body_text": "Hi guys, I wanted to share a project I built recently that's completely transformed my workflow. As a web developer, I kept hitting the same two roadblocks: spending hours creating business documentation for each project and struggling to find qualified leads who actually needed my services. Both tasks were manual, tedious, and eating up time I could spend coding. So I automated the whole thing - and the best part? I did it all in n8n without heavy coding. A while ago, I actually wanted to start a service called 'MyAIdea' to do this for others, but the coding was a bit too challenging. That's why I'm extra excited about what I managed to build with n8n's visual workflow builder. When I input a business niche through a simple chat interface, the system springs into action and generates a full suite of professional documents: Complete business plan SWOT analysis One-pager for quick pitches Pitch deck content Go-to-market strategy blueprint Everything gets organized into a clean folder structure in Google Drive, ready to go. The system then tackles the lead generation problem by: Pulling targeted LinkedIn leads based on my parameters Extracting key information about each prospect Crafting personalized outreach messages that speak to their specific needs Organizing everything in Airtable so I can track conversations The entire flow uses a combination of AI models (OpenAI Chat, DeepSeek) with structured output parsers to handle the different tasks. And except for the LLM costs, you don't have to pay for any APIs - n8n handles all the connections for free. I've attached a flowchart showing how all the components connect. This automation has seriously amplified my business development efforts. I can quickly demonstrate my value to potential clients with professional materials and connect with qualified leads in a systematic way - all without the tedious manual work. Now I just enter a business niche, and the system does the heavy lifting. My productivity has skyrocketed, and I'm wondering if other devs might find something similar useful? Would love to hear your thoughts or suggestions for making it even better!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1huce7n/teach_your_ai_to_use_n8n_code_node_js_expressions/",
          "author": "hurryup",
          "time": "2025-01-05T17:39:44+00:00",
          "body_text": "If you, like me, struggle with writing n8n code nodes or inline JavaScript expressions and don't know how to code, I've created a very detailed document for myself to use as a system prompt in AI because many LLMs don't understand n8n's structure.  You probably can't define this as a Chat Instruction in r/ChatGPT - but if you add it as a custom instruction in products like r/Thinkbuddy - you can use it in all LLMs.  It works great when you write prompts like \"The input data is this, the custom code does this and the output will be like this.\" Now, when you say \"write inline js,\" it will understand instantly because the context is in the system prompt. For easy copy-pasting, here's the pastebin link: https://pastebin.com/Y4hk2uZU For those who want to browse, the document is below: Comprehensive Guide to Inline JavaScript and the Code Node in n8n This comprehensive guide explores the utilization of inline JavaScript transformations and the Code node within n8n workflows. It is structured to provide a seamless understanding of both methods, their functionalities, built-in transformation functions, variables, and best practices. By the end of this guide, you'll be equipped to harness the full potential of JavaScript within your n8n automation workflows. 1. Introduction n8n is a robust, open-source workflow automation tool that enables users to connect various applications and services, facilitating automated tasks without the need for extensive coding. Among its versatile nodes, the Code node and inline JavaScript transformations empower users to implement custom logic and data manipulations, enhancing the flexibility and power of workflows. This guide is designed to provide a comprehensive understanding of these JavaScript integration methods, enabling you to harness their full potential within your n8n workflows. Whether you're performing simple data formatting or implementing complex business logic, mastering both inline expressions and the Code node will significantly enhance your automation capabilities. 2. Inline Expressions vs. Code Node Understanding the distinction between Inline Expressions and the Code Node is crucial for effectively leveraging JavaScript within n8n workflows. Each method serves different purposes and is suited to different types of tasks. 2.1 Key Differences Aspect Inline Expressions Code Node Where Directly in node parameters (toggle ‚ÄúFixed / Expression‚Äù) As a separate node in the workflow Syntax Single‚Äêline JavaScript wrapped in {{ }} Full multi‚Äêline JavaScript (or Python) Ideal Use Case Quick transformations (like .isEmail() , .sum() ) More complex logic (loops, conditionals, multiple variables) Access to n8n Helpers Some helpers are inline‚Äêonly ( $if() , $itemIndex ) Many helpers are available, but not all (e.g., $if() is not) Data Handling Must remain on a single line No line limits; can perform multi‚Äêstep code 2.2 Tournament Templating Engine Starting in n8n v1.9.0 , expressions use an enhanced engine nicknamed ‚Äú Tournament ,‚Äù which provides built‚Äêin data transformation functions out of the box. These functions allow you to perform operations like .isEmail() , .extractDomain() , .removeDuplicates() , and more directly on your data types (strings, arrays, numbers, etc.) within inline expressions. 3. Inline Expressions and Built-in Transformations Inline Expressions in n8n allow you to embed JavaScript directly within node parameters, enabling dynamic data manipulation without the need for separate nodes. This section delves into the basics of inline expressions, their syntax, and the powerful transformation functions available. 3.1 Inline JavaScript Basics Whenever you have a node parameter that can switch between Fixed and Expression modes, you can insert code in the form: {{ <your JavaScript here> }} Single-line : All logic must fit on one line. JavaScript Features : You can use standard JS string or array methods (e.g., .split() , .join() , .map() ) alongside n8n‚Äôs built-in ‚Äúdata transformation functions‚Äù if the data type matches (string, array, etc.). Chaining : You can chain multiple functions (both built-in and vanilla JS) on a single value. Example: {{ \"hello@example.com\".isEmail() }}\n// => true Here, .isEmail() is a built-in transformation function available for strings . 3.2 Data Transformation Functions n8n's Tournament Templating Engine introduces a suite of data transformation functions categorized by data type. These functions can be chained and combined with standard JavaScript methods to perform complex transformations within inline expressions. 3.2.1 String Transformations .isEmail() : Checks if the string is a valid email. .extractDomain() : Extracts the domain from a URL. .removeTags() : Removes HTML tags from text. .base64Encode() / .base64Decode() : Encodes or decodes the string in Base64. .toSnakeCase() / .toCamelCase() **: Converts the string to snake_case or camelCase. .extractUrlPath() : Extracts the path from a URL. Examples: {{ \"john.doe@example.com\".isEmail() }}\n// Returns: true\n\n{{ \"https://www.example.com/path\".extractDomain() }}\n// Returns: \"www.example.com\"\n\n{{ \"<p>Sample Text</p>\".removeTags() }}\n// Returns: \"Sample Text\"\n\n{{ \"Hello World!\".toSnakeCase() }}\n// Returns: \"hello_world!\" 3.2.2 Array Transformations .sum() : Sums all numerical elements in the array. .removeDuplicates() : Eliminates duplicate elements. .merge(array) : Merges another array into the current one. .isEmpty() : Checks if the array has no elements. .randomItem() : Retrieves a random element from the array. .first() / .last() : Retrieves the first or last element. Examples: {{ [1, 2, 2, 4].removeDuplicates() }}\n// Returns: [1, 2, 4]\n\n{{ [10, 20, 30].sum() }}\n// Returns: 60\n\n{{ [1, 2, 3].merge([4, 5]) }}\n// Returns: [1, 2, 3, 4, 5] 3.2.3 Number Transformations .round(decimalPlaces) : Rounds the number to the specified decimal places. .toBoolean() : Converts the number to a boolean ( 0 ‚Üí false , any other number ‚Üí true ). .format(locale) : Formats the number according to the specified locale. .isEven() / .isOdd() : Checks if the number is even or odd. Examples: {{ 123.456.round(2) }}\n// Returns: 123.46\n\n{{ 0.toBoolean() }}\n// Returns: false\n\n{{ 10.isEven() }}\n// Returns: true 3.2.4 Object Transformations .isEmpty() : Checks if the object has no keys. .removeField(key) : Removes a specified key from the object. .merge(object) : Merges another object into the current one. .toJsonString() : Converts the object to a JSON string. Examples: {{ { \"name\": \"Alice\", \"age\": 30 }.isEmpty() }}\n// Returns: false\n\n{{ { \"name\": \"Alice\" }.removeField(\"age\") }}\n// Returns: { \"name\": \"Alice\" }\n\n{{ { \"name\": \"Alice\" }.merge({ \"age\": 30, \"city\": \"New York\" }) }}\n// Returns: { \"name\": \"Alice\", \"age\": 30, \"city\": \"New York\" } 3.2.5 Boolean Transformations .toInt() : Converts true to 1 and false to 0 . Examples: {{ true.toInt() }}\n// Returns: 1\n\n{{ false.toInt() }}\n// Returns: 0 3.2.6 Date & Time Transformations (Luxon) n8n integrates the Luxon library for comprehensive date operations within inline expressions. .toDateTime() : Parses a string into a Luxon DateTime object. .plus(amount, unit) : Adds a specified amount of time. .minus(amount, unit) : Subtracts a specified amount of time. .format(formatString) : Formats the date according to the given pattern. .isWeekend() : Checks if the date falls on a weekend. Examples: {{ \"2025-01-03\".toDateTime().plus(3, \"days\").format(\"yyyy-MM-dd\") }}\n// Returns: \"2025-01-06\"\n\n{{ \"2025-01-03\".toDateTime().isWeekend() }}\n// Returns: false (assuming January 3, 2025, is a Friday)\n\n{{ \"2025-01-03\".toDateTime().minus(1, \"week\").format(\"MMMM dd, yyyy\") }}\n// Returns: \"December 27, 2024\" 3.3 Combining Inline JS Methods You can mix built-in transformations with standard JavaScript methods to perform more intricate data manipulations. Examples: {{ \"https://example.com/path\".extractDomain().toUpperCase() }}\n// Returns: \"EXAMPLE.COM\"\n\n{{ $(\"NodeName\").item.json.someArray\n   .removeDuplicates()\n   .join(\", \")\n   .toUpperCase() }}\n// Returns a string of unique array elements joined by commas and in uppercase This flexibility allows for powerful one-liner transformations that can handle complex data processing tasks directly within node parameters. 4. The Code Node When your workflow requires more elaborate logic‚Äîsuch as multiple lines of code, loops, conditionals, or the use of external libraries‚Äîthe Code node is the ideal choice. This section explores the functionalities and best practices associated with the Code node. 4.1 Multiple Languages (JavaScript & Python) The Code node supports writing scripts in JavaScript and optionally Python . While JavaScript is the primary language, Python support allows for leveraging Python's extensive libraries and functionalities within your workflows. JavaScript : Offers full multi-line scripting capabilities, access to n8n's built-in helpers, and the ability to write complex logic. Python : Provides an alternative for those more comfortable with Python, though it lacks direct access to n8n's JavaScript-specific transformations. Note : Switching to Python means you won't have access to JavaScript-specific built-in transformation functions like .isEmail() or .sum() . 4.2 Accessing the Current Node‚Äôs Input: $input Inside the Code node, $input is the primary way to access the data flowing into the node. Depending on the execution mode, you can retrieve all input items or process each item individually. Property / Method Purpose In Code Node? $input.item Current item (in ‚ÄúRun Once for Each Item‚Äù mode). ‚úì $input.all() Array of all input items. ‚úì $input.first() The first item. ‚úì $input.last() The last item. ‚úì $input.params Node configuration parameters from previous steps. ‚úì $json Shorthand for $input.item.json (if running once per item). ‚úì (context) $input.context.noItemsLeft Use in loops to detect if no more items exist. ‚úì Example (in a Code node running once per item): const current = $input.item; \nconst data = current.json;\ndata.newField = \"Hello!\";\nreturn [{ json: data }]; 4.3 Writing JavaScript in the Code Node Once the Code node is added and configured, you can begin writing your JavaScript code. This involves accessing input data, modifying it as needed, and returning the processed data. 4.3.1 Accessing Input Data Depending on the execution mode, you can access all input items or the current item being processed. Run Once for All Items : Explanation : Fetches all incoming items into an array called items . Each item typically contains a json property with the data. const items = $input.all(); Run Once for Each Item : Explanation : Retrieves the current item being processed. Useful when the node is set to execute once per item. const item = $input.item; Example : // Code Node (JavaScript)\n\n// For \"Run Once for All Items\" mode\nconst items = $input.all();\n\n// For \"Run Once for Each Item\" mode\nconst item = $input.item; 4.3.2 Modifying Data You can manipulate the data as needed, whether processing all items collectively or handling each item individually. Processing All Items Together :const processedItems = items.map(item => {   // Example: Add a new field   item.json.newField = 'New Value';   return item; }); Processing Each Item Individually :// Example: Modify a specific field item.json.existingField = item.json.existingField.toUpperCase(); return item; Example : // Code Node (JavaScript)\n\n// Run Once for All Items\nconst processedItems = items.map(item => {\n  // Add a timestamp to each item\n  item.json.processedAt = new Date().toISOString();\n  return item;\n});\n\nreturn processedItems;\n\n// Run Once for Each Item\nitem.json.processedAt = new Date().toISOString();\nreturn item; 4.3.3 Returning Output Data After processing, ensure your code returns the modified data in the expected format. For \"Run Once for All Items\" :return processedItems; For \"Run Once for Each Item\" :return item; Example : // Code Node (JavaScript)\n\n// Run Once for All Items\nreturn processedItems;\n\n// Run Once for Each Item\nreturn item; 4.4 Using External npm Modules Leveraging external npm modules can significantly enhance the capabilities of your Code node. However, this feature is only available if you are self-hosting n8n . Enabling External Modules Self-Hosted Requirement : Ensure you are running a self-hosted instance of n8n. Cloud-hosted versions might not support this feature due to security restrictions. Install Modules : Navigate to your n8n installation directory. Use npm or yarn to install the desired module. For example:npm install lodash Importing and Using Modules Once the module is installed, you can import and use it within the Code node. const _ = require('lodash');\n\nconst items = $input.all();\n\nconst processedItems = items.map(item => {\n  // Example: Use lodash to clone the item\n  const clonedItem = _.cloneDeep(item.json);\n  clonedItem.newField = 'Cloned Value';\n  return { json: clonedItem };\n});\n\nreturn processedItems; Example: Using Axios for HTTP Requests const axios = require('axios');\n\nconst items = $input.all();\n\nconst processedItems = await Promise.all(items.map(async item => {\n  const response = await axios.get(`https://api.example.com/data/${item.json.id}`);\n  item.json.externalData = response.data;\n  return item;\n}));\n\nreturn processedItems; This script fetches additional data from an external API for each item. Note : Always ensure that the modules you use are compatible with the Node.js version running n8n. 4.5 Debugging and Testing Your Code Ensuring your code works as intended is crucial. n8n provides tools to aid in debugging: Logging Use console.log() : Outputs messages and data to the n8n logs, which can be viewed in the workflow execution details. console.log('Input Items:', items); Example :// Code Node (JavaScript) console.log('Processing the following items:', items);  const processedItems = items.map(item => {   item.json.processed = true;   return item; });  return processedItems; Testing with Sample Data Execute Workflow Manually : Run the workflow with test data to observe how the Code node processes it. Check the output data to verify correctness. Handle Errors Gracefully : Implement error handling to catch and log issues without breaking the entire workflow. try {   // Your code logic } catch (error) {   console.error('An error occurred:', error);   // Optionally, you can throw the error to stop the workflow   throw error; } Example : // Code Node (JavaScript)\n\ntry {\n  const items = $input.all();\n  const processedItems = items.map(item => {\n    if (!item.json.email) {\n      throw new Error('Email is missing for item ID: ' + item.json.id);\n    }\n    item.json.email = item.json.email.toLowerCase();\n    return item;\n  });\n  return processedItems;\n} catch (error) {\n  console.error('Processing error:', error);\n  throw error; // This will stop the workflow and mark it as failed\n} 4.6 Best Practices Adhering to best practices ensures your Code node scripts are efficient, maintainable, and error-resistant. 4.6.1 Data Structure Awareness Understand Input Data : Familiarize yourself with the structure of incoming data ( item.json ) to manipulate it effectively. Consistent Output Structure : Ensure the output data maintains a consistent structure to prevent issues in downstream nodes. Example : // Ensure 'user' field exists before modifying\nif (item.json.user && typeof item.json.user === 'object') {\n  item.json.user.isActive = true;\n} else {\n  item.json.user = { isActive: true };\n}\nreturn item; 4.6.2 Error Handling Implement Try-Catch Blocks : Capture and handle potential errors to prevent workflow failures. try {   // Code that might throw an error } catch (error) {   console.error('Error processing item:', error);   // Decide whether to throw the error or handle it   throw error; } Validate Data : Check for the existence of necessary fields before processing to avoid undefined errors. if (!item.json.requiredField) {   throw new Error('requiredField is missing'); } Example : // Code Node (JavaScript)\ntry {\n  const items = $input.all();\n  const processedItems = items.map(item => {\n    if (!item.json.email) {\n      throw new Error('Email is missing for user: ' + item.json.name);\n    }\n    item.json.email = item.json.email.toLowerCase();\n    return item;\n  });\n  return processedItems;\n} catch (error) {\n  console.error('Processing error:', error);\n  throw error;\n} 4.6.3 Performance Considerations Optimize Loops : Use efficient looping methods like .map() or .forEach() instead of traditional for loops where possible. Limit External Calls : Minimize API calls or database queries within the Code node to enhance performance, especially in \"Run Once for All Items\" mode. Avoid Blocking Operations : Ensure that your code doesn't include long-running synchronous operations that can delay workflow execution. Example : // Code Node (JavaScript)\n\n// Efficiently map over items without unnecessary operations\nconst processedItems = items.map(item => {\n  item.json.processed = true;\n  return item;\n});\n\nreturn processedItems; 4.7 Limitations of the Code Node While the Code Node is a powerful tool, it's essential to be aware of its limitations to set appropriate expectations and design workflows accordingly. Execution Environment : The Code Node runs in a sandboxed environment with certain restrictions for security reasons. Some Node.js APIs or modules might not be available. Resource Constraints : Intensive computations or processing large datasets can impact performance. Always test and optimize your code for efficiency. External Modules (Self-Hosted Only) : The ability to use external npm modules is limited to self-hosted n8n instances. Cloud-hosted versions typically do not support this feature. Error Propagation : Unhandled errors within the Code Node can cause the entire workflow to fail. Implement robust error handling to mitigate this. State Persistence : The Code Node does not maintain state between executions. Each run is stateless unless you explicitly store and retrieve state using external storage solutions. Security Concerns : Executing custom code can introduce security vulnerabilities. Ensure that your code is secure, especially when handling sensitive data or integrating with external systems. Example of a Limitation : // Attempting to use a restricted Node.js module\nconst fs = require('fs'); // This will throw an error as 'fs' is not allowed This script will fail because the fs module is restricted in n8n's sandboxed environment. 5. Built-in Methods & Their Availability n8n provides a suite of built-in helper methods that simplify data manipulation within both inline expressions and the Code node. Understanding their availability and proper usage is key to effective workflow automation. 5.1 $evaluateExpression(expression: string, itemIndex?: number) Availability : Yes in Code node Purpose : Interprets a string as an inline n8n expression from within a Code node. Usage :// Evaluate with default item index = 0 const result = $evaluateExpression('{{$json.property}}');  // Evaluate with item index = 2 const result2 = $evaluateExpression('{{$json.property}}', 2); Example : // Code Node (JavaScript)\n\n// Evaluate an inline expression for the current item\nlet greeting = $evaluateExpression('{{ \"Hello, \" + $json.firstName + \"!\" }}', i); 5.2 $ifEmpty(value, defaultValue) Availability : Yes in Code node Purpose : Returns defaultValue if value is null , undefined , empty string, empty array, or empty object; otherwise returns value . Usage :const fallbackName = $ifEmpty($json.username, 'GuestUser'); Example : // Code Node (JavaScript)\nconst username = $ifEmpty(item.json.username, 'Guest');\nitem.json.displayName = username; 5.3 $if(condition, valueIfTrue, valueIfFalse) Availability : No in Code node Purpose : Inline‚Äêonly helper for quick ternary logic. Inline Expression Usage :{{ $if($json.age > 18, \"Adult\", \"Minor\") }} Code Node Alternative :const status = condition ? valueIfTrue : valueIfFalse; Example : // Code Node (JavaScript)\nconst status = (item.json.score >= 70) ? \"Pass\" : \"Fail\";\nitem.json.status = status; 5.4 $max and $min Availability : No in Code node Purpose : Inline‚Äêonly helpers to determine the maximum or minimum value among provided numbers. Inline Expression Usage :{{ $max(10, 20, 30) }} {{ $min(10, 20, 30) }} Code Node Alternative :const maxValue = Math.max(10, 20, 30); const minValue = Math.min(10, 20, 30); Example : // Code Node (JavaScript)\nconst highestScore = Math.max(item.json.score1, item.json.score2, item.json.score3);\nitem.json.highestScore = highestScore; 6. Built-in Variables & Execution Metadata n8n provides a range of global variables and execution metadata that can be accessed within both inline expressions and the Code node. These variables offer valuable context about the workflow's execution, environment, and state. 6.1 Workflow & Execution Details Variable Description In Code Node? $workflow.id Unique identifier of the workflow. ‚úì $workflow.name Name of the workflow. ‚úì $workflow.active Indicates if the workflow is active ( true ) or inactive ( false ). ‚úì $execution.id Unique ID of the current workflow run. ‚úì $execution.mode Execution mode: test or production . ‚úì $execution.resumeUrl URL to resume a workflow waiting at a Wait node. ‚úì $execution.customData Custom data you can store for this execution. ‚úì $env Accesses environment variables of the n8n instance. ‚úì $secrets Accesses your External secrets configuration. ‚úì $getWorkflowStaticData(type) Retrieves persisted data ( global or node ). ‚úì Inline-Expression-Only Variables : $itemIndex : Index of the current item in inline expressions. Not valid in the Code node. $version : Alias for node version in inline expressions. Not valid in the Code node. 6.2 HTTP Request Node‚ÄìSpecific Variables Only accessible inside the HTTP Request node‚Äôs inline expressions : Variable Description $pageCount Number of pages fetched so far (with built-in pagination). $request The outgoing request object (headers, method, etc.). $response The response object (body, headers, statusCode). Note : These variables do not exist in the Code node or other nodes. 6.3 Global Variables: $vars You can define global variables in the n8n Variables panel (left sidebar). Once created, they are accessible anywhere (including the Code node): const val = $vars.myVariable; \n// All $vars are read‚Äêonly strings Read-only at runtime. If you need to store data persistently, consider using $getWorkflowStaticData('global') or $getWorkflowStaticData('node') . Example : // Code Node (JavaScript)\nconst defaultShipping = $vars.defaultShipping || 'Standard';\nitem.json.shippingMethod = $ifEmpty(item.json.shippingMethod, defaultShipping); 7. Accessing Data from Other Nodes n8n expressions and Code nodes allow referencing data from other nodes using the $(\"<node-name>\") syntax. This capability is essential for creating dynamic and interconnected workflows. 7.1 $(\"NodeName\").all(), .first(), .last(), etc. These methods allow you to retrieve data from a specified node. .all(bIdx?, rIdx?) : Retrieves all items from the node‚Äôs output. .first(bIdx?, rIdx?) : Retrieves the first item from the node. .last(bIdx?, rIdx?) : Retrieves the last item from the node. Examples : Inline Expression :{{ $(\"HTTP Request1\").item.json.someField }} Code Node :const httpItems = $(\"HTTP Request1\").all(); // Array of items const firstItem = $(\"HTTP Request1\").first(); 7.2 $(\"NodeName\").item Availability : Inline expressions only . Purpose : Retrieves the item aligned with the current $itemIndex . Example : {{ $(\"UserFetcher\").item.json.email }} 7.3 $(\"NodeName\").itemMatching(index) Availability : Code node‚Äìonly . Purpose : Retrieves the item from the specified node that matches the current input item's index. Useful for correlated data retrieval in loop scenarios. Example : // Code Node (JavaScript)\n// Retrieving the matching order for the current user\nconst matchingOrder = $(\"OrderFetcher\").itemMatching($input.context.currentNodeInputIndex);\nitem.json.orderDetails = matchingOrder.json; 8. Using JMESPath: $jmespath JMESPath is a powerful query language for JSON. In n8n, you can use the $jmespath function within the Code node to filter or transform data based on JMESPath queries. Example : // Code node example\nconst data = {\n  users: [\n    { name: 'Alice', age: 30 },\n    { name: 'Bob', age: 25 }\n  ]\n};\n\nconst result = $jmespath(data, 'users[?age > `25`].name'); \n// => [\"Alice\"]\n\nreturn [{ json: { result } }]; This script filters users older than 25 and extracts their names. 9. Data Transformation Functions (Inline Expressions) The ‚ÄúTournament‚Äù engine in n8n provides chainable transformation functions for various data types‚Äîstrings, arrays, numbers, objects, booleans, and dates‚Äîwithin inline expressions. These functions simplify complex data manipulations into concise, readable expressions. 9.1 String Transformations .isEmail() : Check if the string is a valid email. .extractDomain() : Extract the domain from a URL. .removeTags() : Remove HTML tags from text. .base64Encode() / .base64Decode() : Encode or decode the string in Base64. .toSnakeCase() / .toCamelCase() : Convert the string to snake_case or camelCase. .extractUrlPath() : Extract the path from a URL. Example : {{ \"john@example.com\".isEmail() }}\n// => true\n\n{{ \"https://www.example.com/path\".extractDomain() }}\n// => \"www.example.com\"\n\n{{ \"<p>Sample Text</p>\".removeTags() }}\n// => \"Sample Text\"\n\n{{ \"Hello World!\".toSnakeCase() }}\n// => \"hello_world!\" 9.2 Array Transformations .sum() : Sum all numbers in the array. .removeDuplicates() : Eliminate duplicate elements. .merge(arr2, arr3, ...) : Merge multiple arrays. .isEmpty() : Check if the array has no elements. .randomItem() : Retrieve a random element from the array. .first() / .last() : Retrieve the first or last element. Example : {{ [1, 2, 2, 4].removeDuplicates() }}\n// => [1, 2, 4]\n\n{{ [10, 20, 30].sum() }}\n// => 60\n\n{{ [1, 2, 3].merge([4, 5]) }}\n// => [1, 2, 3, 4, 5] 9.3 Number Transformations .round(decimals) : Round to the specified number of decimal places. .toBoolean() : Convert the number to a boolean ( 0 ‚Üí false , nonzero ‚Üí true ). .format(locale) : Format the number according to the specified locale. .isEven() / .isOdd() : Check if the number is even or odd. Example : {{ 123.456.round(2) }}\n// => 123.46\n\n{{ 0.toBoolean() }}\n// => false\n\n{{ 10.isEven() }}\n// => true 9.4 Object Transformations .isEmpty() : Check if the object has no keys. .removeField(key) : Remove a specified key from the object. .merge(object2) : Merge another object into the current one. .toJsonString() : Convert the object to a JSON string. Example : {{ { \"email\": \"john@example.com\", \"name\": \"John\" }.removeField(\"name\") }}\n// => { \"email\": \"john@example.com\" }\n\n{{ { \"name\": \"Alice\" }.merge({ \"age\": 30, \"city\": \"New York\" }) }}\n// => { \"name\": \"Alice\", \"age\": 30, \"city\": \"New York\" } 9.5 Boolean Transformations .toInt() : Convert true to 1 and false to 0 . Example : {{ true.toInt() }}\n// => 1\n\n{{ false.toInt() }}\n// => 0 9.6 Date & Time Transformations (Luxon) Leveraging the Luxon library, n8n allows comprehensive date and time manipulations within inline expressions. .toDateTime() : Parse a string into a Luxon DateTime object. .plus(amount, unit) : Add a specified amount of time. .minus(amount, unit) : Subtract a specified amount of time. .format(formatString) : Format the date according to the given pattern. .isWeekend() : Check if the date falls on a weekend. Example : {{ \"2025-01-03\".toDateTime().plus(3, \"days\").format(\"yyyy-MM-dd\") }}\n// => \"2025-01-06\"\n\n{{ \"2025-01-03\".toDateTime().isWeekend() }}\n// => false (assuming January 3, 2025, is a Friday)\n\n{{ \"2025-01-03\".toDateTime().minus(1, \"week\").format(\"MMMM dd, yyyy\") }}\n// => \"December 27, 2024\" 10. Putting It All Together: Examples To solidify your understanding, let's walk through several practical examples that demonstrate how to apply inline expressions and the Code node in real-world scenarios. 10.1 Inline Example: LinkedIn URL Extraction Scenario : Extract the numeric ID from a LinkedIn activity URL. Input Data : {\n  \"query\": {\n    \"url\": \"https://www.linkedin.com/feed/update/urn:li:activity:7281671012738314240/\"\n  }\n} Inline Expression : {{ $(\"LinkedIn\").item.json.query.url.extractUrlPath()\n   .split(\":\")[3]\n   .replace('/', '') }} Explanation : .extractUrlPath() ‚Üí returns \"/feed/update/urn:li:activity:7281671012738314240/\" .split(\":\") ‚Üí splits the string by \":\" , resulting in [\"/feed/update/urn\", \"li\", \"activity\", \"7281671012738314240/\"] [3] ‚Üí selects the fourth element: \"7281671012738314240/\" .replace('/', '') ‚Üí removes the trailing slash, yielding \"7281671012738314240\" Result : 7281671012738314240 --- 40k char limit is limiting me to show all: https://pastebin.com/Y4hk2uZU"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Create business docs and finds leads",
          "link": "https://old.reddit.com/r/n8n/comments/1lyxeo2/dont_overlook_dot_notation_in_n8n_edit_nodes_a/",
          "author": "MonmouthTech",
          "time": "2025-07-13T16:43:37+00:00",
          "body_text": "It‚Äôs easy to get caught up in the advanced features of n8n and miss some of the small, powerful tricks that make building automations smoother‚Äîespecially if you don‚Äôt come from a coding background. Here‚Äôs a quick reminder: When using the Edit node in n8n, you can use dot notation (like results.count or results.topic ) to nest values inside an object tree. This lets you structure your data more clearly and keep related values grouped together, rather than having a flat list of fields. Why does this matter? Cleaner data: Nesting keeps your output organized, making it easier to work with in later steps. Better integrations: Many APIs and tools expect nested objects‚Äîdot notation lets you match those formats directly. Easier scaling: As your automations grow, having structured data helps you avoid confusion and errors. Example Use Cases: Grouping related results (like counts, topics, or summaries) under a single parent object. Preparing payloads for webhooks or external APIs that require nested JSON. Keeping your workflow outputs tidy for easier debugging and handoff to teammates. It might seem obvious to some, but for many users, this simple tip can save a lot of headaches down the road. Hope this helps someone out! https://preview.redd.it/9lduyo8jwocf1.png?width=2982&format=png&auto=webp&s=82926589156045fdc17bf1820307c313b5dead35"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Teach Your AI to Use n8n Code Node / JS Expressions: My Comprehensive AI System Prompt to Use for n8n Tasks",
          "link": "https://old.reddit.com/r/n8n/comments/1m2ng2z/how_do_i_extract_only_the_latest_email_in_an/",
          "author": "Substantial-Chef-195",
          "time": "2025-07-17T23:59:18+00:00",
          "body_text": "I'm using n8n with the Outlook integration, and I want to isolate just the latest reply in an email thread. Right now, the body includes the full thread (previous messages quoted below), but I only want the most recent message content. Is there a clean way to do this? Should I use regex to strip out the older messages, or is there a better built-in method or node setup in n8n to handle this?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Don‚Äôt Overlook Dot Notation in n8n Edit Nodes ‚Äì A Simple Trick That Makes a Big Difference",
          "link": "https://old.reddit.com/r/n8n/comments/1k48mtc/how_i_turned_n8n_automation_into_software_with/",
          "author": "WalkerMount",
          "time": "2025-04-21T08:36:25+00:00",
          "body_text": "No one really talks about this‚Ä¶ Workflows alone are powerful. Workflows + Interface = Product. Everyone‚Äôs out here building workflows and backend automations ‚Äî which is cool ‚Äî but no one talks about turning them into tangible dashboards your clients can actually use . You can watch the video from here (at the end of the post I will include general steps as well) Here‚Äôs what I built; A fully automated finance tracker Send a voice or text to a Telegram bot It transcribes, extracts the transaction info (amount, currency, category, account) Saves it straight to a Supabase DB via n8n Then AI (Windsurf) builds out a full Next.js dashboard that visualizes expenses, transfers, income, accounts, etc. For every automation I build, I always create a frontend dashboard. It makes your work 10x more valuable to clients, especially when you‚Äôre tracking KPIs or operational metrics. Assuming you built the autoamiton, here how the steps look like for the software: Download Windsurf Think of it like an AI coding assistant (similar to Cursor) ‚Äî helps you write + edit real code with context awareness.‚Üí [Insert Windsurf link here] Start with a Next.js + Supabase Template Grab a boilerplate that already integrates Supabase with Next.js.‚Üí [Add your template link or GitHub repo] Use ChatGPT to Generate DB Schema Prompt: ‚ÄúGive me a Supabase SQL schema for a finance tracking app with multiple accounts, currencies, categories, and transactions.‚Äù Paste SQL into Supabase‚Äôs SQL Editor Create tables like: accounts, transactions, currencies, categories, and users. Hook Up Windsurf to the Codebase Open the template in Windsurf ‚Üí Start prompting: ‚ÄúShow a dashboard after user login that visualizes expenses, income, and recent transactions using Supabase data.‚Äù Use Windsurf to Auto-Generate UI Components Cards Charts Tables Filters(And even wiring up Supabase queries + auth!) Iterate with AI Like a Co-Dev If something breaks? Ask Windsurf.Need a trigger or SQL tweak? Ask ChatGPT.Keep looping until it‚Äôs smooth."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1j7vk1j/ai_news_reporter_ai_video_ai_audio_ai_music_ai/",
          "author": "gochapachi1",
          "time": "2025-03-10T10:28:56+00:00",
          "body_text": "Workflow Do give an upvote you guys, Discover how to create a professional AI news reporter video using an automated n8n workflow! In this video, we demonstrate an end-to-end process that integrates various AI tools and automated video editing techniques to produce a fully polished news video. Here's what you'll learn: AI Video Model Generation: Automatically generate realistic video models using AI. AI Audio Creation: Generate high-quality AI audio for the model with perfect lipsync. AI Music Generation: Create custom background music using AI to add the perfect vibe to your video. Automated Editing & Transitions: Utilize advanced video editing techniques and seamless transitions with ffmpeg integrated into the n8n workflow. Complete End-to-End Automation: Watch as the entire process‚Äîfrom content creation to final editing‚Äîis fully automated, saving time and effort. Whether you're a content creator, media professional, or just curious about the power of automation and AI, this workflow offers a glimpse into the future of video production. Workflow:- https://github.com/gochapachi/AI-news-Reporter Youtube :- https://youtu.be/Km2u6193pDU If you enjoyed this video, please like, comment, and subscribe for more content on AI-driven automation and innovative video production techniques. Let's revolutionize content creation with AI and automation! üëâ Follow Us on Social Media for More Updates: üß† Reddit: https://www.reddit.com/user/gochapachi1/ üìò Facebook: https://facebook.com/gochapachi/ üì∏ Instagram: https://www.instagram.com/gochapachi/ üé• YouTube: https://www.youtube.com/@gochapachi üíº LinkedIn: https://www.linkedin.com/in/gochapachi/ üìû whatsapp: +91-8400210108 üì© Email:¬†[ sanjeevcs0034@gmail.com ](mailto: sanjeevcs0034@gmail.com )"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How do I extract only the latest email in an Outlook thread using n8n?",
          "link": "https://old.reddit.com/r/n8n/comments/1l13yzu/end_to_end_blogger_post_automations/",
          "author": "Haunting_Fox2890",
          "time": "2025-06-02T00:07:37+00:00",
          "body_text": "I‚Äôve been working on a project to streamline blog post creation for my clients, and I wanted to share the workflow I‚Äôve built in n8n. This automation handles everything from content generation to publishing, making the process super efficient. I‚Äôd love to get your feedback on this! I‚Äôm considering creating a tutorial for this workflow using a sticky note-style format (like breaking down each step visually in a sticky note layout). Do you think this approach would be helpful for learning, or do you have other suggestions for a better way to present the tutorial?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1lpwpcf/looking_for_a_jaipurbased_nocodelowcode_developer/",
          "author": "Ok-Balance7343",
          "time": "2025-07-02T14:03:53+00:00",
          "body_text": "Hey folks! I‚Äôm currently looking for a Jaipur-based developer with at least 1 year of hands-on experience working with tools like n8n, Make.com, Airtable, and other no-code/low-code platforms to work closely with me on various client projects. About Me I‚Äôm an AI Automation Specialist with 2+ years of experience, actively working with clients from the US and UK, including ex-Fortune 500 executives. I‚Äôm in the process of launching my own agency and looking for someone ambitious to grow alongside me. üìå LinkedIn Profile : https://linkedin.com/in/goutamsoni Why work with me? üí∏ Competitive Salary üöÄ High-growth environment ü§ù Opportunity to be involved in real projects-not just internal tasks If you‚Äôre based in Jaipur and excited to build cool automation workflows and grow fast, DM me or drop a comment. Let‚Äôs connect!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How I turned n8n automation into software with Supabase and Windsurf",
          "link": "https://old.reddit.com/r/n8n/comments/1lvynu2/a_fully_modular_ai_agentfree_code_with_mcps_for/",
          "author": "croos-sime",
          "time": "2025-07-10T00:12:36+00:00",
          "body_text": "This is how you can empower your AI Agents using MCPs (Model Context Protocols). MCPs are a great way to extract and reuse logic between different agents. If you're reading this and you're a developer, a quick comparison would be with a microservice or an API: you encapsulate specific logic within an MCP. If you're not, it's a very cool way to save code (rare to say this in a no-code tool) and reuse it. It's recommended that each MCP does one thing well, that it be a service focused on a single functionality. For example, one MCP specializes in Reddit-related actions, another for Calendar, and another for Supabase. A very powerful approach to starting to implement MCPs in your agents (especially since it's so easy to integrate with N8n) is to leverage this tool as the primary means of communication with the agents. It's the best way to do it because each MCP can clearly explain what it needs, and if you follow a pattern like the one I use (where each MCP has multiple actions, and each action requires a specific JSON), implementation becomes much simpler. I've been developing software for over eight years, and in my experience, this clear, modular structure allows you to maintain scalability and order as your system grows. It's a much more organized alternative than connecting 10 different nodes for each functionality within the agent. You'll simplify the structure, reduce friction in prompts, and make the agent's job easier. This is an agent I use daily because I need to search for content on Reddit, read comments, keep my database up to date, and always know if I have an upcoming meeting or class. It's tailored to my real problems and needs. But you can create as many agents as you want, and the most interesting thing is that MCPs can be reused across different agents, which increases their value. The approach I use for each MCP is that it calls a node in N8n that executes another workflow. All MCPs call the same base workflow, but they are passed a value called an action. This action determines which path the workflow will take when processing the request. Each workflow may require a different JSON, and when writing the description of the MCP (and its actions) for the agent, that content serves as a reference to know, based on the user's request, what action to use, how to use it, and what JSON to send to the MCP. This allows for flexible and precise interaction, without the need to code rigid paths for each feature. All code here: https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md In the link you have a youtube video to dive deep on it"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "AI News Reporter (AI Video + AI Audio + AI Music + AI Lipsync + Transitions + Automated Video Edit).",
          "link": "https://old.reddit.com/r/n8n/comments/1l7xd5b/automated_google_slides_presentation_creation/",
          "author": "SavageImperol",
          "time": "2025-06-10T12:24:10+00:00",
          "body_text": "I have been playing around with the Google slides API and below are part of the lessons Background I built this workflow to automate the creation of PPT documents and slides. In this setup, am automating the generation of meeting notes for each call and can easily reference them later How it works - When the call ends, we receive a webhook with the meeting transcript and title - We have AI analyze the transcript, and extract the main insights. This depends on how you structure your meetings - With the meeting title, create a new blank PPT/slide - For each slide data, we create a respective slide page on Google PPT - Have AI go through the analysis and structure the slides as needed based on the analysis. This takes into account the text length, style and breaks the text into multiple slides if need be - Have the second AI generate the json schema needed for the google slides API to populate each page - Format the background to our desired color For this use case this setup is working great but can be improved to accommodate more styles and high level formatting Tips for working with PPTs and Slides If you are not comfortable with whole some generation, you can generate a custom template and run text replacements across When running the text replacements, keep into account the text length so as to retain the formatting. When adding the placeholders, try to be as verbose as possible to the max allowed characters then you can know how to play with your input If you have charts and tables you'd like to add, integrate this with google sheets as either linked(updates with changes to the sheet) or static (doesn't change. You can not change from static to linked) Run the text input, replacements and the formatting next. If you are bad a##, you can run both within the same body request Duplicating a slide uses the google drive API. Get file id, duplicate then you can run your manipulations When generating the slide content via AI, provide concrete examples on how your slide data will be. You can manually run/test a few of the generated content and  make changes as needed Creating the prompt Start with chatGPT - Provide a sample input as plain text and ask it for the json structured output for each slide. On n8n, run the provided body and note the results. You can then improve as needed untul you have all the jsons you need say for title page, images, text, tables etc. With this, you will ask it to provide  detailed prompt to generate such results, and how to prevent errors as experienced during testing - With the output, run it in n8n, and compare the results of the generated slides and the results sent for each, if error, debug on n8n, with the input and output, and chat GPT will provide a better result. Iterate until satisfied, and you can then run it back on n8n"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "End to end blogger post Automations",
          "link": "https://old.reddit.com/r/n8n/comments/1lz1ecl/what_does_delivering_a_workflow_look_like/",
          "author": "TheWahdee",
          "time": "2025-07-13T19:25:07+00:00",
          "body_text": "I'm new to n8n but eager to get started with it and build my first fully-autonomous, marketing agent, youtube content creation, 500 node workflow MACHINE. No, I'm kidding! I want to learn the proper way to use n8n for effective, production-level automations, but I'm unclear on what that \"looks like\". I can see the value it has for some fun, personal automations like handling some e-mails or scraping the internet for content, as well as the potential to be a powerful, scalable tool that can be used by large businesses. The latter is what I hope to learn more about with this post. When making automations/workflows for clients, what does this usually involve and, specifically, what does it look like to \"deliver an automation/workflow\"? From what I understand, this could even take shape in multiple ways. Maybe it's just a matter of creating the workflow itself and sending that off to a client, letting them input their own credentials and hosting the n8n instance themselves. Or you host the n8n instance \"yourself\" (not your own PC but something like AWS), and your client would provide the credentials to connect their accounts as necessary. Or the client only ever sees the \"outside\" of the automation, let's say you set up a Slack based trigger that allows clients send \"requests\" and they receive back a message or an email with the results of the workflow, I know in the case of B2B automation services, the client likely never interacts with n8n themselves, but that makes me curious about how the whole set up and delivery works. The reality is probably different from what I'm describing, which is exactly why I want to learn more about it and understand it better. Anybody with experience in building real, production-grade automations for clients in a B2B context willing to share examples/details of this? I'm hoping for an overall rundown from start to finish of what this would look like, and examples of the types of actual production-level workflows being implemented. Thank you!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lt7sxk/not_just_generate_this_is_how_you_actually_edit/",
          "author": "Possible-Club-8689",
          "time": "2025-07-06T18:07:47+00:00",
          "body_text": "Most people using OpenAI with n8n are just generating images from prompts ‚Äî but here‚Äôs the missing part:\nüõ†Ô∏è Editing existing images (in-painting, overlaying, removing parts, etc.) ‚Äî directly inside your n8n workflows. I finally figured out how to do it after struggling with vague docs and failed HTTP attempts, so here‚Äôs the actual method: üîó You need to hit this endpoint via HTTP Request node: POST https://api.openai.com/v1/images/edits üß© The Required Parameters: In the HTTP Request node, set: Method: POST Authentication: Bearer Token (your OpenAI API key) Content-Type: multipart/form-data Body Parameters (form-data): image: Binary data of your input image (mask optional, but supported) prompt: Your edit instruction (e.g., ‚ÄúAdd clouds to the sky‚Äù) n: Number of results (default 1) size: e.g., \"512x512\" Make sure you're sending actual image data (binary) ‚Äî not just a URL or base64. That‚Äôs where most attempts break. ‚úÖ Real Use Cases This Unlocks: Automatically remove logos or watermarks Overlay AI-generated content onto product photos Edit user-uploaded images in customer flows Build a full image review + edit pipeline using file triggers or upload forms Bonus Tip: You can chain this with: Webhook trigger + file upload OpenAI image edit Telegram / Slack / Email delivery of edited images Or even send it back to a web app or CMS I can drop a full n8n template JSON if enough people are interested ‚Äî including sample prompts, error handling, and proper image upload.\nLet me know if you're trying something similar or stuck anywhere ‚Äî happy to help or collaborate."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking for a Jaipur-based No-Code/Low-Code Developer (1+ Year Experience)",
          "link": "https://old.reddit.com/r/n8n/comments/1lb1c25/why_is_reading_2_inputs_so_difficult_in_a_code/",
          "author": "Wonderful-Heart3557",
          "time": "2025-06-14T05:47:57+00:00",
          "body_text": "I need to get this off my chest! I've spent hours trying to figure this out, and with immense frustration. The workflow was this: - Node1 calls an API and returns a JSON array with 1 object. - Node2 reads a Google Sheet and returns a JSON array containing multiple objects. - Node3 (my code node) tries to use input 1 (from node1) and input 2 (from node2). I can see the correct structure and content of both inputs in the left-hand 'Input' panel of Node3 (code node), but no matter which way I try to access the inputs using Javascript, the value of input 2 is always the same value as input 1. I spent the best part of the day trying to figure it out. I used GitHub copilot, Google Gemini, Claude, and ChatGPT, and NONE of them could figure out why it was working. I am running the latest build of n8n. I tried all of the following but none worked: - Disconnecting the input nodes from the code node. - Deleting the input nodes and recreating them. - Swapping the order of the two inputs. - Creating a merge node to merge the 2 inputs. Eventually, I had to add a new code node immediately after each input node to tag each input. I then used a merge node to merge them, and then the output of the merge node could successfully be used as input by the code node. What should be an easy and straightforward task proved to be ridiculously difficult. I also think that the documentation is pretty bad and almost non-existent for doing this. Has anyone else suffered the same frustrations as me when trying to reference multiple inputs in a code node? If so, what was your solution? Do you have any tips for doing this in future?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring",
          "link": "https://old.reddit.com/r/n8n/comments/1lkk0uk/endtoend_horoscope_video_generator_scripts_ai/",
          "author": "automayweather",
          "time": "2025-06-25T22:48:00+00:00",
          "body_text": "Key Features 2 000-word script per sign with GPT-4.1 (OpenRouter) Auto-translate to 40+ languages Natural male/female voice-over via ElevenLabs Turbo v2.5 (Fal TTS) Cinematic zodiac artwork from Fal AI (text-to-image) 1080p video assembly through Samautomation renderer Captions, SEO title & description‚Äîalso AI-generated Direct YouTube upload + scheduled publish date Google Sheets logging Why You Might Care Faceless content: stay off-camera but post daily Global reach: switch languages with one cell in a Google Sheet Zero SaaS lock-in: runs on your n8n or VPS Scales fast: 12 videos in ~15 minutes once creds are set Requirements n8n (Docker/Easypanel or n8n Cloud) OpenRouter or OpenAI key Fal.ai key Google Cloud project (bucket + YouTube OAuth) Optional Telegram bot for status alerts https://reddit.com/link/1lkk0uk/video/qdikjsuqj59f1/player"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "A Fully Modular AI Agent‚ÄîFREE Code with MCPs for Reddit, Google Calendar, and Supabase",
          "link": "https://old.reddit.com/r/n8n/comments/1m17y38/rebuilding_my_n8n_workflow_in_python/",
          "author": "Elfi309",
          "time": "2025-07-16T09:17:54+00:00",
          "body_text": "I‚Äôve built a personal workflow that sends me daily book summaries via WhatsApp, tailored to specific titles I'm interested in. Now, I‚Äôm planning to expand it step by step, integrating personal interests as well as professional inputs like email content. It‚Äôs going to get a lot more complex. I know that n8n is primarily a low-code tool , but in theory, I could rebuild the entire workflow as a standalone Python program. I‚Äôve written several Python scripts before, so I feel confident in doing this. Has anyone here tried to replicate or migrate their n8n workflow into pure Python? Is it worth the effort ‚Äì or would you recommend sticking with n8n?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lu7nme/ultimate_n8n_ai_workflows_3000_workflows_an/",
          "author": "Fearless-Role-2707",
          "time": "2025-07-07T22:11:10+00:00",
          "body_text": "Hey r/n8n community! üëã I'm thrilled to share my open-source project, Ultimate n8n AI Workflows, with over 3,000 AI-powered workflows for automating everything from chatbots to SEO content pipelines. Whether you're a newbie or a pro, there's something here to supercharge your automation game! üéâ I'm also working on a private LLM prototype that generates n8n workflows from simple text prompts, trained on the library's 3,400+ workflows. üß† It's still in development, and I'm ironing out kinks like JSON accuracy, but it's shaping up to make workflow creation faster and easier. Check out the project: github.com/oxbshw/ultimate-n8n-ai-workflows Got a workflow idea to test the prototype? Drop it below or contribute to the repo! üôå Let's build the future of AI automation together!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated Google slides Presentation Creation with N8n and open Router",
          "link": "https://old.reddit.com/r/n8n/comments/1k6rcy8/meeting_with_real_estate_agencies/",
          "author": "RuiRdA",
          "time": "2025-04-24T12:48:36+00:00",
          "body_text": "TL;DR - What should I pitch to real estate agencies? Hello n8n community, I'm a very experienced software developer with strong technical skills. I recently discovered n8n and the AI automation business model, and decided to start my entrepreneurial journey. I've started reaching out to local businesses and have already scheduled meetings with two local real estate agencies.\nI offered them a free consultation to identify where AI and automation could help grow their business. This is where I could use your help:\nWhile I‚Äôm confident in my ability to implement almost anything technically, I‚Äôm less sure from a business perspective about what would truly be valuable for them and provide a good ROI. So my questions are: What kinds of automations or AI solutions are likely to be useful for real estate agencies? What key questions should I ask during the meeting to uncover their real needs and pain points? I really want to pitch them something practical and valuable ‚Äî it doesn‚Äôt have to be fancy AI, even \"boring\" but effective automations are perfectly fine. Thank you so much for taking the time to read and help a brother out! Edit 1: [UPDATE] Met with a real estate agency So I had about a one-hour meeting with the first realtor here in Portugal. I went in with the goal of understanding their workflow and spotting opportunities for AI and automation. I asked a lot of questions to figure out where automation could help, but it turns out that in Portugal, a lot of their processes are already automated. There‚Äôs a SaaS platform that basically dominates the market‚Äîit handles CRM, document management, and a bunch of other stuff. One thing they really wished they had was a way to validate that listings from other agencies are actually accurate. Apparently, when they don‚Äôt have a house that fits the client‚Äôs needs, they collaborate with other agencies. But sometimes the info they get from others isn‚Äôt reliable. That kind of cross-listing mediation was new to me, but seems common here. They weren‚Äôt really interested in automating lead qualification or conversations. Most of their leads come from real estate portals that already give them a sense of how serious someone is. The one area where they showed real interest was social media‚Äîautomating content creation and posting across different platforms. That‚Äôs not what I expected to be the main opportunity, but now I‚Äôm looking into existing tools and prebuilt flows to help with that and to put together a proposal. If anyone has ideas or tools for social media automation specifically for real estate, I‚Äôd really appreciate it!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m4tjkj/share_a_wordpress_autopublishing_workflow/",
          "author": "gnosjah",
          "time": "2025-07-20T16:41:56+00:00",
          "body_text": "full workflow My customer recently asked me to build a fully automated content publishing system with WordPress and n8n. Naturally, I went looking for inspiration‚Äîbut to my surprise, there wasn‚Äôt a single public or usable WordPress + n8n basic workflow out there. So I rolled up my sleeves and built one from scratch. üíÄ It took more time than I expected, but the result was worth it: This workflow now runs daily, hands-free, and has already brought my client real traffic and positive ROI. They even started recommending it to others. That‚Äôs why I‚Äôve decided to share it with the community‚Äîso you can use it, improve it, or even build on it to do something better. I have posted the code here: https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json ‚∏ª Happy to give back to this amazing community üôå"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "What does \"delivering a workflow\" look like?",
          "link": "https://old.reddit.com/r/n8n/comments/1kpz2b2/stuck_in_the_i_kind_of_get_it_but_cant_make_it/",
          "author": "nomnomchomps",
          "time": "2025-05-19T00:46:27+00:00",
          "body_text": "Hey all, I‚Äôve watched a bunch of n8n tutorials and I feel like I mostly get how it works. I understand the basics ‚Äî I can use nodes, connect things together, and I‚Äôm even using the new Code node. But when it comes to the finer technical details, especially with expressions and logic, I start to feel lost. Right now, I‚Äôm trying to build a workflow that sends personalised emails to contacts listed in a Google Sheet. It sounds simple enough, and I‚Äôve got it mostly set up: I‚Äôm reading the rows, generating content with OpenAI, and then trying to email them. But I keep getting stuck on the splitting and looping logic. ChatGPT tells me I need to use loops (like a Merge node or an IF node inside a loop), The n8n docs say you don‚Äôt need to loop and the system handles batches automatically, Then I try to follow either path and I get random errors I don‚Äôt understand ‚Äî sometimes about undefined fields, sometimes nothing happens at all. I just get overwhelmed. I don‚Äôt know where to go to learn the exact syntax or structure that works. The documentation feels either too general or too advanced. ChatGPT tries to help, but it keeps giving me the same broken suggestions or explains the same concepts without adjusting to what I‚Äôve already tried. I feel like I‚Äôm in this limbo stage ‚Äî not a total beginner, but not able to troubleshoot properly either. How do people move through this part of learning? Where do you go for practical help when the docs and AI fall short? How do you learn to debug effectively in n8n? Would love any advice or even links that helped you break through this phase üôè"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1kuzt8x/automatically_analyze_reddit_posts_and_comments/",
          "author": "ValuableAd8067",
          "time": "2025-05-25T10:56:14+00:00",
          "body_text": "https://preview.redd.it/cchhp1xiuw2f1.png?width=821&format=png&auto=webp&s=f5889da5bce0a1f769302fb1752ba9171085595d Feel Free to play around and adjust the output to your desire. Right now, I've used a very basic prompt to generate the output. What it does : This workflow gathers posts and comments from a subreddit on a periodic basis (every 4 hrs), collates them together, and then performs an analysis to give this output: Outline Central Idea Arguement Analysis YouTube Script What it doesn't : This workflow doesn't collates children comments (replies under comments) Example Output: Outline Central Idea Arguement Analysis YouTube Script I. Introduction to n8nworkflows.xyz\\nII. Purpose of the platform\\n   A. Finding workflows\\n   B. Creating workflows\\n   C. Sharing workflows\\nIII. Community reception\\n   A. Positive feedback and appreciation\\n   B. Questions and concerns\\n   C. Technical issues\\nIV. Relationship to official n8n platform\\nV. Call to action for community participation n 8nworkflows.xyz is a community-driven platform for sharing, discovering, and creating n8n automation workflows that appears to be an alternative to the official n8n template site. 0:Supporting: Multiple users express gratitude and appreciation for the resource, indicating it provides value to the n8n community1:Supporting: Users are 'instantly' clipping or saving the resource, suggesting it fulfills an immediate need2:Supporting: The platform encourages community participation through its 'find, create, share' model3:Against: One user questions why this is needed when an official n8n template site already exists4:Against: A user reports access issues, indicating potential technical problems with the site5:Against: One comment suggests contradiction in the creator's approach, possibly implying a business model concern ('not buy but asking to hire') Hey automation enthusiasts! Today I want to introduce you to an exciting resource for the n8n community - n8nworkflows.xyz!\\n\\n[OPENING GRAPHIC: n8nworkflows.xyz logo with tagline \"Find yours, create yours, and share it!\"]  \\n\\nIf you've been working with n8n for automation, you know how powerful this tool can be. But sometimes, reinventing the wheel isn't necessary when someone has already created the perfect workflow for your needs.\\n\\nThat's where n8nworkflows.xyz comes in. This community-driven platform has three key functions:\\n\\n[GRAPHIC: Three icons representing Find, Create, and Share]\\n\\nFirst, FIND workflows that others have built and shared. This can save you countless hours of development time and help you discover solutions you might not have thought of.\\n\\nSecond, CREATE your own workflows. The platform provides a space for you to develop and refine your automation ideas.\\n\\nAnd third, SHARE your creations with the broader community, helping others while establishing yourself as a contributor to the n8n ecosystem.\\n\\n[TRANSITION: Show split screen of community comments]\\n\\nThe community response has been largely positive, with users describing it as \"awesome,\" \"very useful,\" and \"so good.\" Many are immediately saving the resource for future use.\\n\\nOf course, some questions have been raised. For instance, how does this differ from the official n8n template site? While both offer workflow templates, n8nworkflows.xyz appears to focus more on community contributions and sharing between users.\\n\\nSome users have reported access issues, which is something to be aware of. As with any community resource, there may be occasional technical hiccups.\\n\\n[CALL TO ACTION SCREEN]\\n\\nSo whether you're an n8n veteran or just getting started with automation, check out n8nworkflows.xyz to find, create, and share workflows with the community.\\n\\nHave you already used this resource? Drop a comment below with your experience or share a workflow you've created!\\n\\nDon't forget to like and subscribe for more automation tips and resources. Until next time, happy automating! JSON Code: {\n¬† \"name\": \"Reddit Posts & Comments Analysis\",\n¬† \"nodes\": [\n{\n\"parameters\": {\n\"rule\": {\n\"interval\": [\n{\n\"field\": \"hours\",\n\"hoursInterval\": 4\n}\n]\n}\n},\n\"type\": \"n8n-nodes-base.scheduleTrigger\",\n\"typeVersion\": 1.2,\n\"position\": [\n60,\n-720\n],\n\"id\": \"4e920b20-8c41-4217-add7-52384d5429a7\",\n\"name\": \"Schedule Trigger\"\n},\n{\n\"parameters\": {\n\"resource\": \"postComment\",\n\"operation\": \"getAll\",\n\"subreddit\": \"={{ $json.subreddit }}\",\n\"postId\": \"={{ $json.id }}\"\n},\n\"type\": \"n8n-nodes-base.reddit\",\n\"typeVersion\": 1,\n\"position\": [\n820,\n-700\n],\n\"id\": \"9b0e03c1-78cb-44d3-bdaf-60815a543fcd\",\n\"name\": \"Reddit1\",\n\"credentials\": {\n\"redditOAuth2Api\": {\n\"id\": \"BCi7mcfwTGGdhYTc\",\n\"name\": \"Reddit account\"\n}\n}\n},\n{\n\"parameters\": {\n\"aggregate\": \"aggregateAllItemData\",\n\"include\": \"specifiedFields\",\n\"fieldsToInclude\": \"body\",\n\"options\": {}\n},\n\"type\": \"n8n-nodes-base.aggregate\",\n\"typeVersion\": 1,\n\"position\": [\n1000,\n-700\n],\n\"id\": \"1e8386c5-8841-46e2-a75e-135345718d26\",\n\"name\": \"Aggregate1\"\n},\n{\n\"parameters\": {\n\"operation\": \"getAll\",\n\"subreddit\": \"n8n\",\n\"limit\": 1,\n\"filters\": {\n\"category\": \"top\"\n}\n},\n\"type\": \"n8n-nodes-base.reddit\",\n\"typeVersion\": 1,\n\"position\": [\n280,\n-720\n],\n\"id\": \"cbe0f6a5-a33e-464c-a4d5-08fecaff352c\",\n\"name\": \"n8n Subreddit Posts\",\n\"credentials\": {\n\"redditOAuth2Api\": {\n\"id\": \"BCi7mcfwTGGdhYTc\",\n\"name\": \"Reddit account\"\n}\n}\n},\n{\n\"parameters\": {\n\"assignments\": {\n\"assignments\": [\n{\n\"id\": \"6bea2644-eb70-490d-81ff-3898b21cb265\",\n\"name\": \"Posts\",\n\"value\": \"={{ $('Loops').item.json.selftext }}\",\n\"type\": \"string\"\n},\n{\n\"id\": \"fc85eda0-0f95-446e-b040-d609c12b5a20\",\n\"name\": \"Comments\",\n\"value\": \"={{ $json.data }}\",\n\"type\": \"string\"\n}\n]\n},\n\"options\": {}\n},\n\"type\": \"n8n-nodes-base.set\",\n\"typeVersion\": 3.4,\n\"position\": [\n1220,\n-700\n],\n\"id\": \"b5ddf753-993c-4631-83a3-8e1ce06d3041\",\n\"name\": \"Edit Fields1\"\n},\n{\n\"parameters\": {\n\"model\": {\n\"__rl\": true,\n\"mode\": \"list\",\n\"value\": \"claude-3-7-sonnet-20250219\",\n\"cachedResultName\": \"Claude 3.7 Sonnet\"\n},\n\"options\": {}\n},\n\"type\": \"@n8n/n8n-nodes-langchain.lmChatAnthropic\",\n\"typeVersion\": 1.3,\n\"position\": [\n900,\n-840\n],\n\"id\": \"7a337b75-e8e5-482d-8de4-b92974deae94\",\n\"name\": \"Anthropic Chat Model1\",\n\"credentials\": {\n\"anthropicApi\": {\n\"id\": \"b9CmwFUwwIpJa7M8\",\n\"name\": \"Anthropic account\"\n}\n}\n},\n{\n\"parameters\": {\n\"jsonSchemaExample\": \"{\\n\\t\\\"Outline\\\": \\\"Outline\\\",\\n\\\"Central Idea\\\": \\\"Idea\\\",\\n ¬†\\\"Arguement Analysis\\\": [\\\"Pros\\\", \\\"Cons\\\"],\\n ¬†\\\"YouTube script\\\": \\\"Script\\\"\\n}\"\n},\n\"type\": \"@n8n/n8n-nodes-langchain.outputParserStructured\",\n\"typeVersion\": 1.2,\n\"position\": [\n1020,\n-840\n],\n\"id\": \"dee58775-b270-4116-9fb3-88817422a667\",\n\"name\": \"Structured Output Parser1\"\n},\n{\n\"parameters\": {\n\"batchSize\": \"={{ 1 }}\",\n\"options\": {\n\"reset\": false\n}\n},\n\"type\": \"n8n-nodes-base.splitInBatches\",\n\"typeVersion\": 3,\n\"position\": [\n500,\n-720\n],\n\"id\": \"b8fab93e-4652-49e9-8032-fc078cab9632\",\n\"name\": \"Loops\"\n},\n{\n\"parameters\": {\n\"promptType\": \"define\",\n\"text\": \"=Analyse the series of posts and comments below to extract:\\n1) Underlying Outline.\\n2) Central Idea\\n3) Key Points arguing for and against the central Idea\\n4) Repackage the narrative into a YouTube script\\n-------------------------\\nPost: {{ $json.Posts }}\\n-------------------------\\nComments: {{ $json.Comments }}\\n\",\n\"hasOutputParser\": true,\n\"options\": {\n\"systemMessage\": \"You are a helpful assistant\"\n}\n},\n\"type\": \"@n8n/n8n-nodes-langchain.agent\",\n\"typeVersion\": 1.8,\n\"position\": [\n820,\n-1100\n],\n\"id\": \"c425eb9e-5325-459f-872b-0c32d730c426\",\n\"name\": \"Analyzing Posts & Comments\"\n}\n¬† ],\n¬† \"pinData\": {},\n¬† \"connections\": {\n\"Schedule Trigger\": {\n\"main\": [\n[\n{\n\"node\": \"n8n Subreddit Posts\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"Reddit1\": {\n\"main\": [\n[\n{\n\"node\": \"Aggregate1\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"Aggregate1\": {\n\"main\": [\n[\n{\n\"node\": \"Edit Fields1\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"n8n Subreddit Posts\": {\n\"main\": [\n[\n{\n\"node\": \"Loops\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"Edit Fields1\": {\n\"main\": [\n[\n{\n\"node\": \"Loops\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"Anthropic Chat Model1\": {\n\"ai_languageModel\": [\n[\n{\n\"node\": \"Analyzing Posts & Comments\",\n\"type\": \"ai_languageModel\",\n\"index\": 0\n}\n]\n]\n},\n\"Structured Output Parser1\": {\n\"ai_outputParser\": [\n[\n{\n\"node\": \"Analyzing Posts & Comments\",\n\"type\": \"ai_outputParser\",\n\"index\": 0\n}\n]\n]\n},\n\"Loops\": {\n\"main\": [\n[\n{\n\"node\": \"Analyzing Posts & Comments\",\n\"type\": \"main\",\n\"index\": 0\n}\n],\n[\n{\n\"node\": \"Reddit1\",\n\"type\": \"main\",\n\"index\": 0\n}\n]\n]\n},\n\"Analyzing Posts & Comments\": {\n\"main\": [\n[]\n]\n}\n¬† },\n¬† \"active\": false,\n¬† \"settings\": {\n\"executionOrder\": \"v1\"\n¬† },\n¬† \"versionId\": \"092b959e-a2bc-4e1a-a758-5d803d2fbf9e\",\n¬† \"meta\": {\n\"templateCredsSetupCompleted\": true,\n\"instanceId\": \"fcad5a0362f17d948a98dd8737b8a8041278da128258c15818c0d1def21975ad\"\n¬† },\n¬† \"id\": \"Q67Ho0nCKmnzb25r\",\n¬† \"tags\": []\n}"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Not just generate ‚Äî this is how you actually edit images in n8n using OpenAI‚Äôs image API (DALL¬∑E Edit)",
          "link": "https://old.reddit.com/r/n8n/comments/1lttvm2/article_selection_via_gemini_flash_25_any/",
          "author": "Mr_Sbrillo",
          "time": "2025-07-07T13:15:04+00:00",
          "body_text": "Hello there, due my job I had problem to keep me update about my tech topic (in my case CyberSecurity). For this reason I developed a \"trivial\" workflow to: -Collect and evaluate (logic below) the last 24 h articles; -Post on my telegram channel the top 5 during the day (with also a catching message talkign about that). Difficulties that i overcame (somehow): Cloudflare restriction: i needed an virtual browser to wait the page loading for many services. I resolved with a puppeteer machine on my minikude (that i kill every x minute - due RAM increase); Extraction of article text: with several reggex and html estraction I was able to reduce the size of text field to evaluate, and therefore reduce preparing fase of AI (it is far from perfect but there are many different webpage structure to manage) Evaluation by returning only a valid JSON with integer scores (0‚Äì1000) for six categories: Emerging Threats (E), Attack Method (A), Strategic Insight (S), Hardware Feature (H), Technical Vulnerability (V), and Cybercrime Trend (C).-> then I moltiply each parameter for a value in order to give more/less importance to each category Fine-Tuning: Even if I have already developed a FT version of a gemma2:2b, It still not reliable (for this reason I am collecting new samples via a google sheet)... for now Gemini is the way... Any advice to improve this workflow? EDIT: repost because image problem...."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1k58963/hidden_risks_when_building_n8n_automations_using/",
          "author": "theIntegrator-",
          "time": "2025-04-22T14:52:27+00:00",
          "body_text": "Hey n8n fam, Love how fast you can build with n8n! But the ease (importing JSON, sending data to AI) makes me think about hidden security/privacy stuff. Do we really check that imported code? Data to AI & GDPR?Staying safe is key. I shared some insights on these risks here in a blog post, let me know what you think https://teknuro.com/the-hidden-risks-of-low-code-no-code-and-ai/ The blog post: The digital world is evolving rapidly, with platforms like n8n, Make, and new AI tools at the helm of this evolution. Thanks to low-code and no-code solutions, automation has become accessible to a much wider range of people. Add the smart power of AI, and you get an unprecedented combination of speed and efficiency. It‚Äôs therefore tempting to start building immediately and achieve results quickly ‚Äì and this is precisely where the strength of these tools lies. However, where there is ease, there is also potential danger. The speed at which you can get started, and the appeal of ready-made solutions, sometimes lead us to be less critical of important fundamentals like security and data privacy. Two risk areas therefore deserve extra attention: importing workflows and the use of data within AI applications. Importing Workflows: More Than Just a Template Platforms like n8n make sharing workflows easy via JSON files. Ideal for reuse and inspiration, and you can find countless examples online. But make no mistake: a JSON file is not an innocent sketch ‚Äì it‚Äôs the complete definition of your workflow, including all logic and configurations. This carries risks. Malicious or Unwanted Logic JSON files can contain nodes that include code ‚Äì think of Function or Code nodes ‚Äì that perform actions you might not even be aware of. Examples include secretly sending data to unknown servers, modifying files on your system, or manipulating data leading to errors in your processes. Exposure of Credentials Although sensitive information is stored separately in n8n, credential names or IDs can still be present in the JSON. This can ‚Äì especially with recognizable naming conventions ‚Äì already reveal too much about your infrastructure. Sometimes, nodes even contain authentication details that remain active upon import. Community Nodes The strength of an active community is also a potential weakness. Community nodes run with extensive privileges within your system. Poorly written ‚Äì or worse, malicious ‚Äì nodes can gain access to your entire setup, exfiltrate data, or destabilize your environment. Vulnerabilities in n8n Itself Even an seemingly safe workflow can exploit vulnerabilities in older software versions. Security updates are released regularly, so updating is crucial. AI and Privacy: Don‚Äôt Forget GDPR In addition to workflows, AI integration also brings specific risks ‚Äì especially if you send personal data to external models. AI services like OpenAI can process data outside the EU or use it for training purposes. This directly impacts the GDPR. Important points to consider: Where is your data processed and stored? Who has access to it? Is the data logged or stored for analysis? Is a DPA in place? Are you even allowed to send data outside the EU? Ease of use should never come at the expense of your responsibilities as a data processor. How to Protect Yourself? Risks exist, but with the right approach, they are manageable. Here are best practices you should apply as standard: Be critical of the source of workflows. Only trust official or internal sources. Before activation, examine every node and configuration. Read code carefully, review expressions, and understand what is happening. Remove hardcoded credentials from imported workflows. Replace them with secure references. Share workflows consciously. Sanitize them and prevent accidental distribution of sensitive information. Limit community nodes to what is strictly necessary, and disable unused nodes. Update n8n regularly. Security updates are not a luxury. Test new workflows in a sandbox. Keep production environments clean from risks. Be cautious with data sent to AI services. Only send what is truly necessary, and pseudonymize where possible. Always create backups. This allows you to quickly revert if something goes wrong. Summary Risk Description Mitigation Malicious Logic Code in workflows executing unwanted actions Only import trusted sources, thoroughly inspect nodes Credential Exposure JSON with sensitive credential information Remove hardcoded data, use secure references Community Nodes Untrustworthy 3rd-party nodes with broad rights Install judiciously, update regularly, use official nodes Software Vulnerabilities Exploits via old n8n versions Keep your system up-to-date AI & Data Privacy Unconscious data sharing, GDPR non-compliance Minimize data, establish DPAs, anonymize, test carefully General Unforeseen consequences after import Test in safe environment, back up regularly In Conclusion Low-code/no-code tools like n8n and powerful AI integrations give organizations a huge advantage in speed and innovation. But convenience should never be a license to neglect security and privacy. By carefully reviewing workflows, using AI with discretion, and working from a security-first approach, you maximize the potential of your tools ‚Äì without putting yourself or your users at risk. Working safely is not a delay; it is a prerequisite for sustainable digital growth. Your Partner for Secure Automation and Integrations Automation and AI offer enormous potential, but security and compliance are crucial. Do you want to be sure that your integrations and automations are robust, scalable, and secure? We specialize in implementing secure and effective integrations, automations, and complete digital transformation journeys."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Why is reading 2 inputs so difficult in a code node?",
          "link": "https://old.reddit.com/r/n8n/comments/1m55gyt/built_an_ai_agent_in_n8n_for_youtube_market/",
          "author": "kyle4real",
          "time": "2025-07-21T01:05:56+00:00",
          "body_text": "https://preview.redd.it/3gav577mn4ef1.png?width=1920&format=png&auto=webp&s=dd788d8ebfe15a113d6bcd973cba8162c95fcfc3 Hey r/n8n !! If you're into AI automation and content creation, I just dropped a tutorial on building an AI agent in n8n that analyzes YouTube channels on the fly. You chat with it about any channel, and it scrapes video data (views, likes, duration, etc.), saves it to a Google Sheet, and answers smart market analysis questions to help brainstorm better content strategies. Why it's cool for creators/businesses: Ask stuff like: \"What are this channel‚Äôs top 5 videos?\" \"Which ones went viral?\" \"Average engagement rate?\" \"Videos with unusual view-to-like ratios?\" or \"Which underperformed?\" It pulls insights you can't get from just browsing a channel's popular vids ‚Äì perfect for niche research or competitor spying. Handles multiple channels for comparisons, with optimizations to avoid redundant API calls. Video structure: Quick demo (00:53) High-level overview (02:13) Step-by-step build: Get Channel Info (04:18), Get Channel Data (09:13), AI Agent setup (19:16), System Prompt (22:40) Testing & thoughts (24:43) What you'll learn in n8n: Creating a chat-based AI agent (using OpenAI) Setting up multi-agent workflows Capturing YouTube data via APIs/HTTP nodes Reading/updating Google Sheets Connecting credentials for YouTube, Sheets, and OpenAI It's a great example of chaining n8n nodes for real-world AI tools ‚Äì from HTTP requests and code parsing to tool calling and prompts. Watch here: https://www.youtube.com/watch?v=h_oPOkDLw28 Download template/code here: https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI?usp=sharing If you build this, share your results or mods! Anyone used n8n for similar YouTube/API projects? Tips on scaling for big channels or adding more features? Let's chat ‚Äì open to suggestions for future vids too. Thanks for the support! üöÄ (Music creds in vid desc)"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Built a Telegram-Based Personal Finance Assistant Using n8n ‚Äî Looking for Suggestions to Improve",
          "link": "https://old.reddit.com/r/n8n/comments/1ku2ep6/built_a_telegrambased_personal_finance_assistant/",
          "author": "GapProfessional1485",
          "time": "2025-05-24T03:41:34+00:00",
          "body_text": "Hey folks, I recently built a personal finance assistant using n8n that runs entirely on Telegram. It's a no-code/low-code workflow that pulls data from a Google Sheet and sends back a smart, AI-generated spend summary. Thought I'd share how it works and would love your feedback or ideas to improve it! How It Works: Telegram Bot Integration: The bot listens for /summary messages using the Telegram Trigger node. Google Sheets Read: It fetches raw expense data from a linked Google Sheet. Data Processing: A Code node processes the rows into structured data (date, category, amount). AI-Based Analysis: It calculates: Total spend Daily average Top 5 spending categories Highest/lowest spend days Most frequent category Summary Generation: A clean, readable summary report is composed with emojis and formatting for clarity. Telegram Message Reply: The summary is sent back to the user directly in the chat. Example Output: üìä Finolytix Spend Summary üßæ Total Spend: ‚Çπ68,000 üóìÔ∏è Period: 2025-05-22 ‚Üí 2025-05-23 üìà Daily Avg: ‚Çπ34,000 üîÅ Most Frequent Category: mobile phone ‚Äî ‚Çπ60,000 üèÜ Top Categories: ‚Ä¢ mobile phone: ‚Çπ60,000 ‚Ä¢ electronics: ‚Çπ5,000 ‚Ä¢ groceries: ‚Çπ2,000 ‚Ä¢ shirt: ‚Çπ500 ‚Ä¢ milk: ‚Çπ500 üóìÔ∏è Highest Spend Day: 2025-05-23 ‚Äî ‚Çπ60,500 üìâ Lowest Spend Day: 2025-05-22 ‚Äî ‚Çπ7,500 üß† Analysis powered by Finolytix (using n8n + JS) What I‚Äôm Looking for: Any additional metrics you‚Äôd include? Ways to make this more interactive? Suggestions for visual enhancements (without using charts)? How can I make this more modular or scalable? Thanks in advance! Happy to share the template if anyone wants to try it."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1kmw9a0/from_bridge_troll_to_8figure_boss_all_thanks_to/",
          "author": "Ineedmorec0ffee",
          "time": "2025-05-15T01:33:55+00:00",
          "body_text": "Not long ago I was literally working under an interstate bridge, siphoning Wi-Fi from a gas station and chasing pigeons off my keyboard. Then I discovered n8n and built a single automation that changed my life, and I've decided to share it today. The workflow: Finds Infinity leads from all over the internet Uses AI Voice, text, and email to execute perfect nurture and follow up Creates marketing content that literally forces people to click Literally takes money from people and deposits it into offshore bank accounts in your name I new I had to change something about my life, then I found n8n on tiktok.  Three months later, the business tops eight figures and I‚Äôve traded the bridge for mansion in Thailand. I‚Äôm sharing the exact JSON and a setup walkthrough today. Join my Skool community to download it‚Äîno paywall, no upsell. skool.com/this-is-a-joke-if-it-wasnt-painfully-obvious"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From Bridge Troll üèöÔ∏è to 8-Figure Boss üöÄ ‚Äì All Thanks to ONE Simple n8n Automation (Giving It Away FREE Today!)",
          "link": "https://old.reddit.com/r/n8n/comments/1ld4sqo/can_you_actually_build_good_automations_with/",
          "author": "MissSBlack",
          "time": "2025-06-16T21:08:34+00:00",
          "body_text": "I‚Äôve been trying to build automations to streamline my blogging workflow using ChatGPT (and sometimes Claude), and honestly, I‚Äôm hitting a wall. The idea seems simple enough: Pull blog post instructions from Google Sheets Feed them to ChatGPT to rewrite or optimize the content Paste the optimized content back into the spreadsheet Mark it as ‚ÄúOptimized‚Äù Maybe notify me via Slack or email once it‚Äôs done I‚Äôm trying to do this all through n8n because it‚Äôs supposed to be no-code, but it‚Äôs been way harder than expected. Little things keep breaking, like: Not reading all rows properly Output formatting issues Not pasting things in the right cells The data from ChatGPT coming in as weird structures I‚Äôm not even sure when to use expressions like {{$json[\"instruction\"]}} I don‚Äôt have a coding background, so when things don‚Äôt work, I‚Äôm often just guessing. It makes me wonder: Has anyone here actually pulled off a decent ChatGPT-based automation for content creation without being a dev? Would love to hear real experiences ‚Äî especially from others trying to do this without much technical knowledge."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lscpsp/n8n_workflow_for_graphicsinfographics_with/",
          "author": "asdasdgfas",
          "time": "2025-07-05T15:39:37+00:00",
          "body_text": "I am looking for inspiration/examples of n8n projects that somehow automate content creation. I want to automate my TikToks slideshow creation process. Here is the issue: Most \"automations\" are just call to GPT Image 1 to generate funny picture or add text to it - zero control, zero value. What I want to achieve is for my automation to generate pictures for my slideshow, where each slide will present one concept/app/thing. Example: Slideshow about TOP 3 meditation apps: -1st slide: hook with interesting text -2nd slide: 1st app (logo of given app + quick description) ... Sure I can use some simple Canva template and manually add AI generated text to proper place on template. But I would like it to be a bit more diverse, not the same looking each time. Did anyone manage to create a n8n project that generates graphics/infographics like text+picture, that is actually high quality, not obviously AI-generated and a bit original? That is NOT gpt picture --> animated/changed to movie --> added lyrics + captions Also if you have any n8n automations related to graphics/design you are welcome to share, I am interested."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Can You Actually Build Good Automations with ChatGPT or Claude If You Don‚Äôt Code?",
          "link": "https://old.reddit.com/r/n8n/comments/1m1sjc6/novice_question_about_changing_a_step_in_workflow/",
          "author": "LenoxHillPartners",
          "time": "2025-07-16T23:51:26+00:00",
          "body_text": "I'm a non-technical startup founder and hired an engineer. He made a workflow that included an API call to ATTOM [red box on image 1] for a real estate valuation, which is used in the next step in the workflow: \"Calculate WealthPulse Score.\" A \"SUCCESS\" was when a valuation was found, either \"1\" for a valuation of >$2M or \"0\" for <$2M. A \"-1\" FAILURE was when a valuation was not found/indeterminate. The workflow worked well. My trial with ATTOM ended, however, so I'm trying to use the (free) Redfin estimate [yellow box on image 1, also at this URL ]. The problem I can't solve is that the \"Calculate WealthPulse Score\" step seems to be bypassed. I've included the code for that on image 2. Also included what I see on the HTTP Request (Redfin), that seems to show an error, but I don't know how to evaluate it. I'm not sure if I've described the problem well, but can you see anything immediately that I might be missing? Feel free to suggest that I use a no-code/vibe code solution if you think as a non-technical founder that would be better."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1l9ik60/what_processes_should_be_in_your_n8n_library/",
          "author": "OscarSchyns",
          "time": "2025-06-12T09:59:01+00:00",
          "body_text": "Hi, My team and I are building a library of n8n processes to help clients automate their workflows. Most of our clients are companies with 20‚Äì50 employees in various sectors, but many are recruiters. What processes do you think we should include in our n8n library? I'm thinking of creating many building blocks (email automation being one example) that can be used to quickly build solutions for clients. Would love to hear your thoughts. Oscar"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "N8n workflow for graphics/infographics with templates",
          "link": "https://old.reddit.com/r/n8n/comments/1lyb3nc/reddit_curation_automated/",
          "author": "kenmiranda",
          "time": "2025-07-12T21:04:34+00:00",
          "body_text": "Snippet of the workflow I noticed an uptick in content here and in some of the other subreddits I follow. Like a lot of people, I kept thinking ‚ÄúI just want to see more of X instead of Y\". So I built an n8n workflow that pulls posts from your favorite subreddits, runs them through a filter using upvote ratio and AI Agents, and then sends the best ones to a Slack channel. It also summarizes longer posts and checks for duplicates using Supabase. I included the SQL DDL as this will be required for this setup. It's currently setup to use supabase as the database, Openai as the LLM, and slack as the communication app. Feel free to tear it apart and make it your own. Have fun! Here is the GitHub repo: https://github.com/kenandrewmiranda/reddit-content-filter-n8n Here is just the JSON: https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json Example of an output: https://preview.redd.it/q8kd44w9cicf1.png?width=1906&format=png&auto=webp&s=9735474a772c45980318a1ea3844b9f5d2a07534"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lr06jy/learn_faster_8_common_mistakes_i_correct_with/",
          "author": "croos-sime",
          "time": "2025-07-03T20:36:19+00:00",
          "body_text": "In more than eight years of software work I have tested countless automation platforms, yet n8n remains the one I recommend first to creators who cannot or do not want to write code. It lets them snap together nodes the way WordPress lets bloggers snap together pages, so anyone can build AI agents and automations without spinning up a full backend. The eight lessons below condense the hurdles every newcomer (myself included) meets and show, with practical examples, how to avoid them. Understand how data flows Treat your workflow as an assembly line: each node extracts, transforms, or loads data. If the shape of the output from one station does not match what the next station expects, the line jams. Draft a simple JSON schema for the items that travel between nodes before you build anything. A five-minute mapping table often saves hours of debugging. Example: a lead-capture webhook should always output { email, firstName, source } before the data reaches a MailerLite node, even if different forms supply those fields. Secure every webhook endpoint A webhook is the front door to your automation; leaving it open invites trouble. Add at least one guard such as an API-key header, basic authentication, or JWT verification before the payload touches business logic so only authorised callers reach the flow. Example: a booking workflow can place an API-Key check node directly after the Webhook node; if the header is missing or wrong, the request never reaches the calendar. Test far more than you build Writing nodes is roughly forty percent of the job; the rest is testing and bug fixing. Use the Execute Node and Test Workflow features to replay edge cases until nothing breaks under malformed input or flaky networks. Example: feed your order-processing flow with a payload that lacks a shipping address, then confirm it still ends cleanly instead of crashing halfway. Expect errors and handle them Happy-path demos are never enough. Sooner or later a third-party API will time out or return a 500. Configure an Error Trigger workflow that logs failures, notifies you on Slack, and retries when it makes sense. Example: when a payment webhook fails to post to your CRM, the error route can push the payload into a queue and retry after five minutes. Break big flows into reusable modules Huge single-line workflows look impressive in screenshots but are painful to maintain. Split logic into sub-workflows that each solve one narrow task, then call them from a parent flow. You gain clarity, reuse, and shorter execution times. Example: Module A normalises customer data, Module B books the slot in Google Calendar, Module C sends the confirmation email; the main workflow only orchestrates. If you use mcp you can implement mcp for a task (mcp for google calendar, mcp for sending an email) e.g., in the image shared Favour simple solutions When two designs solve the same problem, pick the one with fewer moving parts. Fewer nodes mean faster runs and fewer failure points. Example: a simple HTTP-Request , Set , Slack chain often replaces a ten-node branch that fetches, formats, and posts the same message. Store secrets in environment variables Never hard-code URLs, tokens, or keys inside nodes. Use n8n‚Äôs environment variable mechanism so you can rotate credentials without editing workflows and avoid committing secrets to version control. Example: {{ $env.API_BASE_URL }}/contacts keeps the endpoint flexible between staging and production. Design every workflow as a reusable component Ask whether the flow you are writing today could serve another project tomorrow. If the answer is yes, expose it via a callable sub-workflow or a webhook and document its contract. Example: your Generate-Invoice-PDF workflow can service the e-commerce store this week and the subscription billing system next month without any change. To conclude, always view each workflow as a component you can reuse in other workflows. It will not always be possible, but if most of your workflows are reusable you will save a great deal of time in the future. What other common mistakes would you add to this list?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Novice question about changing a step in workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1lrkam5/minitutorial_how_to_easily_scrape_data_from/",
          "author": "dudeson55",
          "time": "2025-07-04T14:36:43+00:00",
          "body_text": "I‚Äôve gotten a bunch of questions from a previous post I made about how I go about scraping Twitter / X data to generate my AI newsletter so I figured I‚Äôd put together and share a mini-tutorial on how we do it. Here's a full breakdown of the workflow / approaches to scrape Twitter data This workflow handles three core scraping scenarios using Apify's tweet scraper actor ( Tweet Scraper V2 ) and saves the result in a single Google Sheet (in a production workflow you should likely use a different method to persist the tweets you scrape) 1. Scraping Tweets by Username Pass in a Twitter username and number of tweets you want to retrieve The workflow makes an HTTP POST request to Apify's API using their \"run actor synchronously and get dataset items\" endpoint I like using this when working with Apify because it returns results in the response of the initial http request. Otherwise you need to setup a polling loop and this just keeps things simple. Request body includes maxItems for the limit and twitterHandles as an array containing the usernames Results come back with full tweet text, engagement stats (likes, retweets, replies), and metadata All scraped data gets appended to a Google Sheet for easy access ‚Äî This is for example only in the workflow above, so be sure to replace this with your own persistence layer such as S3 bucket, Supabase DB, Google Drive, etc Since twitterHandles is an array, this can be easily extended if you want to build your own list of accounts to scrape. 2. Scraping Tweets by Search Query This is a very useful and flexible approach to scraping tweets for a given topic you want to follow. You can really customize and drill into a good output by using twitter‚Äôs search operations. Documentation link here: https://developer.x.com/en/docs/x-api/v1/rules-and-filtering/search-operators Input any search term just like you would use on Twitter's search function Uses the same Apify API endpoint (but with different parameters in the JSON body) Key difference is using searchTerms array instead of twitterHandles I set onlyTwitterBlue: true and onlyVerifiedUsers: true to filter out spam and low-quality posts The sort parameter lets you choose between \"Top\" or \"Latest\" just like Twitter's search interface This approach gives us much higher signal-to-noise ratio for curating content around a specific topic like ‚ÄúAI research‚Äù 3. Scraping Tweets from Twitter Lists This is my favorite approach and is personally the main one we use to capture and save Tweet data to write our AI Newsletter - It allows us to first curate a list on twitter of all of the accounts we want to be included. We then pass the url of that twitter list into the request body that get‚Äôs sent to apify and we get back a list of all tweets from users who are on that list. We‚Äôve found this to be very effective when filtering out a lot of the noise on twitter and keeping costs down for number of tweets we have to process. Takes a Twitter list URL as input (we use our manually curated list of 400 AI news accounts) Uses the startUrls parameter in the API request instead of usernames or search terms Returns tweets from all list members in a single result stream Cost Breakdown and Business Impact Using this actor costs 40 cents per 1,000 tweets versus Twitter's $200 for 15,000 tweets a month. We scrape close to 100 stories daily across multiple feeds and the cost is negligible compared to what we'd have to pay Twitter directly. Tips for Implementation and working with Apify Use Apify's manual interface first to test your parameters before building the n8n workflow. You can configure your scraping settings in their UI, switch to JSON mode, and copy the exact request structure into your HTTP node. The \"run actor synchronously and get dataset items\" endpoint is much simpler than setting up polling mechanisms. You make one request and get all results back in a single response. For search queries, you can use Twitter's advanced search syntax to build more targeted queries. Check Apify's documentation for the full list of supported operators. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=otK0ILpn4GQ The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1lo13t5/how_can_i_upload_large_video_files_from_my_phone/",
          "author": "Ishanatur",
          "time": "2025-06-30T07:11:24+00:00",
          "body_text": "Hi everyone, please forgive me if this a dumb question, as I'm a complete noob. I'm trying to build a workflow for my content creation. Basically I do daily brain dumps using my phones camera and I'd like to take those files, transcribe them and then have AI re-write them into the first draft of my script. Right now my workflow looks like this Record the video straight into Google Drive on my phone N8N polls the folder for new files Send to the CloudConvert API to convert to mp4 Send the audio to OpenAI for transcription Have my AI agent re-write the script This works well when the files are small, but the download URL that Google returns isn't always a direct URL to the file. It has a \"virus scanner\" for large files that causes the workflow to fail. (It works fine for smaller files) I've tried using the Google Drive \"Download File\" node in n8n, but that causes the server to crash as I'm using a very small VM  because I'm scared I'll be slapped with a massive bill at the end of the month. I don't know what I'm doing and just followed a tutorial so I'm afraid to touch anything. And I can't afford a big surprise expense at the end of the month. I'm not tied to Google Drive, but I do want something that I can record videos straight from my phone that will trigger the workflow. I tried S3, which seemed super complicated and I couldn't find a way to upload videos there directly from my phone. Does anyone have any ideas on how to get this to work? It seems like such a simple thing but I've been struggling for two days."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "What processes should be in your n8n library",
          "link": "https://old.reddit.com/r/n8n/comments/1lb5mov/looking_to_purchase_n8n_templates_for_content/",
          "author": "legenwaitforitdary19",
          "time": "2025-06-14T10:34:30+00:00",
          "body_text": "Hi everyone! I‚Äôm interested in buying high-quality n8n workflows or templates that can help automate content creation and lead generation processes. If you have something ready-made or can build one tailored for business use, feel free to reach out or drop a message. Open to both paid and custom solutions. Thanks!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1l2y1ww/chrome_extension_are_beast/",
          "author": "FabulousUse9906",
          "time": "2025-06-04T05:57:30+00:00",
          "body_text": "I recently added a custom Chrome extension into my workflow that connects directly with my n8n flows and Cursor (for code). It‚Äôs been a game changer ‚Äî especially for building YouTube content and editing. Instead of bouncing between ChatGPT, Figma, and other tools, I now:\n    Run prompts in-app without tabbing out Feed live context into each prompt (via custom tabs) Stay in a deep workflow state way longer than before Why this matters: üìâ Studies show it takes 23+ minutes to regain full focus after switching tabs (UC Irvine) üß† Multitasking can reduce productivity by up to 40% (APA) üß≠ Integrated tools boost deep work by nearly 30% (Microsoft WorkLab) For me, the biggest difference isn‚Äôt just speed it‚Äôs mental clarity. I don‚Äôt get prompt fatigue. I don‚Äôt lose my train of thought. And I actually enjoy the creative process more because it feels seamless. If you‚Äôre building with AI, n8n, or even just doing creative work ‚Äî consider embedding your tools where you work instead of making ChatGPT a second brain that lives in another tab. Happy to share more on the setup if anyone‚Äôs curious."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Reddit curation, automated.",
          "link": "https://old.reddit.com/r/n8n/comments/1jlgt10/i_built_an_agentic_recruiting_pipeline_with_n8n/",
          "author": "Glass-Ad-6146",
          "time": "2025-03-27T23:15:07+00:00",
          "body_text": "I've been working on orchestrating AI agents for practical business applications, and wanted to share my latest build: a fully automated recruiting pipeline that does deep analysis of candidates against position requirements. The Full Node Sequence The Architecture The system uses n8n as the orchestration layer but does call some external Agentic resources from Flowise. Fully n8n native version also exists with this general flow: Data Collection : Webhook receives candidate info and resume URL Document Processing : Extract text from resume (PDF) Convert key sections to image format for better analysis Store everything in AWS S3 Data Enrichment : Pull LinkedIn profile data via RapidAPI endpoints Extract work history, skills, education Gather location intelligence and salary benchmarks Enrich with industry-specific data points Agentic Analysis : Agent 1: Runs detailed qualification rubric (20+ evaluation points) Agent 2: Simulates evaluation panel with different perspectives Both agents use custom prompting through OpenAI Storage & Presentation : Vector embeddings stored in Pinecone for semantic search Results pushed to Bubble frontend for recruiter review This is an example of a traditional Linear Sequence Node Automation with different stacked paths The Secret Sauce The most interesting part is the custom JavaScript nodes that handle the agent coordination. Each enrichment node carries \"knowledge\" of recruiting best practices, candidate specific info and communicates its findings to the next stage in the pipeline. Here is a full code snippet you can grab and try out. Nothing super complicated but this is how we extract and parse arrays from LinkedIn. You can do this with native n8n nodes or have an LLM do it, but it can be faster and more efficient for deterministic flows to just script out some JS. function formatArray(array, type) {\nif (! array ?. extractedData || !Array.isArray(array.extractedData)) {\nreturn [];\n}\n\nreturn array.extractedData.map(item => {\nlet key = '';\nlet description = '';\n\nswitch (type) {\ncase 'experiences': key = 'descriptionExperiences';\ndescription = `${\nitem.title\n} @ ${\nitem.subtitle\n} during ${\nitem.caption\n}. Based in ${\nitem.location || 'N/A'\n}. ${\nitem.subComponents ?. [0] ?. text || 'N/A'\n}`;\nbreak;\ncase 'educations': key = 'descriptionEducations';\ndescription = `Attended ${\nitem.title\n} for a ${\nitem.subtitle\n} during ${\nitem.caption\n}.`;\nbreak;\ncase 'licenseAndCertificates': key = 'descriptionLicenses';\ndescription = `Received the ${\nitem.title\n} from ${\nitem.subtitle\n}, ${\nitem.caption\n}. Location: ${\nitem.location\n}.`;\nbreak;\ncase 'languages': key = 'descriptionLanguages';\ndescription = `${\nitem.title\n} - ${\nitem.caption\n}`;\nbreak;\ncase 'skills': key = 'descriptionSkills';\ndescription = `${\nitem.title\n} - ${\nitem.subComponents ?. map(sub => sub.insight).join('; ') || 'N/A'\n}`;\nbreak;\ndefault: key = 'description';\ndescription = 'No available data.';\n}\n\nreturn {[key]: description};\n});\n}\n\n// Get first item from input\nconst inputData = items[0];\n\n// Debug log to check input structure\nconsole.log('Input data:', JSON.stringify(inputData, null, 2));\n\nif (! inputData ?. json ?. data) {\nreturn [{\njson: {\nerror: 'Missing data property in input'\n}\n}];\n}\n\n// Format each array with content\nconst formattedData = {\ndata: {\nexperiences: formatArray(inputData.json.data.experience, 'experiences'),\neducations: formatArray(inputData.json.data.education, 'educations'),\nlicenses: formatArray(inputData.json.data.licenses_and_certifications, 'licenseAndCertificates'),\nlanguages: formatArray(inputData.json.data.languages, 'languages'),\nskills: formatArray(inputData.json.data.skills, 'skills')\n}\n};\n\nreturn [{\njson: formattedData\n}]; Everything runs with 'Continue' mode in most nodes so that the entire pipeline does not fail when a single node breaks. For example,  if LinkedIn data can't be retrieved for some reason on this run, the system still produces results with what it has from the resume and the Rapid API enrichment endpoints. This sequence utilizes If/Then Conditional node and extensive Aggregate and other native n8n nodes Results What used to take recruiters 2-3 hours per candidate now runs in about 1-3 minutes. The quality of analysis is consistently high, and we've seen a 70% reduction in time-to-decision. Want to build something similar? I've documented this entire workflow and 400+ others in my new AI Engineering Vault that just launched: https://vault.tesseract.nexus/ It includes the full n8n canvas for this recruiting pipeline plus documentation on how to customize it for different industries and over 350+ other resources in the form n8n and Flowise canvases, fully implemented Custom Tools, endless professional prompts and more. Happy to answer questions about the implementation or share more details on specific components!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m3ytc9/n8n_woocommerce/",
          "author": "Icy-Organization2914",
          "time": "2025-07-19T15:20:58+00:00",
          "body_text": "After creating a workflow for WordPress auto-blogging. Now I'm stuck in the poduct creation, has anyone tried to integrate woocommerce to n8n in order to automate product adding?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Learn Faster: 8 Common Mistakes I Correct with Every Student I Mentor",
          "link": "https://old.reddit.com/r/n8n/comments/1kr506d/i_made_a_docker_chrome_with_full_vnc_access_for/",
          "author": "conor_is_my_name",
          "time": "2025-05-20T13:34:10+00:00",
          "body_text": "TLDR: This Docker container gives you full visual control of Chrome with VNC access‚Äîperfect for scraping tricky sites, testing, or logged-in sessions. If you are new to web scraping this makes a lot of things easier! üîó GitHub Link: https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer Who is this for? Scrapers battling sites requiring logins, CAPTCHAs, or dynamic content. Developers who need to debug visually or automate complex interactions. Anyone who has wasted hours trying to make Puppeteer/Playwright work headlessly when a real browser would‚Äôve taken 5 minutes. (this is me) Stealth mode users who want the most realistic browser usage with minimal chance of detection. I made this because I wanted to do analysis on long form journalism articles.  All of my sources required logins to read the articles, and had pretty strong subscription and login checking protocols.  Even though I actually do pay for these subscriptions and have valid credentials, it was tricky to get the logins to work in headless mode. Basically, you can connect to a full GUI chrome running on a server, raspberry pi, even your own local machine, and then control it programmatically.  In my case, I remote into the GUI, log into the website as needed in a fully normal chrome browser instance, and then run my scripts. Use page.close() instead of browser.close() to end your scripts.  This will keep the browser open and ready for a new command. You will need to restart the container if you pass a browser.close() command. Why this beats headless mode: Full Chrome GUI in a container‚Äîjust like your local browser, but remote-controlled. VNC access (with audio support if needed). Pre-loaded with Puppeteer for scripting inside or outside the container. Persistent sessions (no more re-logging in every scrape). Downsides: S low R esource Heavy (but sometimes it doesn't matter: skipping login scripting and captchas can more than make up for a slow scraper) What‚Äôs inside? Chrome Stable (+ all dependencies). VNC for multiple remote access options. Puppeteer/Playwright-compatible ‚Äîuse your existing scripts. Easy volume mounts to save profiles/sessions. n8n json starter Install in 2 commands: git clone https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\n\ndocker compose up -d Then connect via VNC (default password: password ) Example n8n nodes are included: U pdate the IP address, everything else will be automatic. Use Code Node for your scripts.  This allows way more customization than using the community nodes. Tested on: 4GB+ RAM recommended for smooth VNC. Raspberry Pi 4 (ARM support included!). Here's the link to the ARM version Pair this with my n8n autoscaling setup for next-level workflow execution speed . Need n8n consulting? DM me for help with your projects."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lzo8u1/how_i_use_redis_to_cache_google_api_data_in_n8n/",
          "author": "MonmouthTech",
          "time": "2025-07-14T14:43:07+00:00",
          "body_text": "Example Daily Cache Gmail Labels If you‚Äôre running a lot of automations with Google, or any, APIs in n8n, you‚Äôve probably noticed how quickly API quotas and costs can add up‚Äîespecially if you want to keep things efficient and affordable. One of the best techniques I use frequently is setting up Redis as a cache for Google API responses. Instead of calling the API every single time, I check Redis first: If the data is cached, I use that (super fast, no extra API call). If not, I fetch from the API, store the result in Redis with an expiration, and return it. This approach has cut my API usage and response times dramatically. It‚Äôs perfect for data that doesn‚Äôt change every minute‚Äîthink labels, contact list, geocoding, user profiles, or analytics snapshots. Why Redis? It‚Äôs in-memory, so reads are lightning-fast. You can set expiration times to keep data fresh. My example above refreshes daily. It works great with n8n‚Äôs, especially self-hosted setups. I run Redis, LLMs, and all services locally to avoid third-party costs. Bonus: You can apply the same logic with local files (write API responses to disk and read them before calling the API again), but Redis is much faster and easier to manage at scale. Best part: This technique isn‚Äôt just for Google APIs. You can cache any expensive or rate-limited API, or even database queries. If you‚Äôre looking to optimize your n8n workflows, reduce costs, and speed things up, give Redis caching a try! Happy to answer questions or share more about my setup if anyone‚Äôs interested."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Mini-Tutorial: How to easily scrape data from Twitter / X using Apify",
          "link": "https://old.reddit.com/r/n8n/comments/1leypz5/ai_lofi_girl_youtube_channel_automation/",
          "author": "Silent-Willow-7543",
          "time": "2025-06-19T01:45:36+00:00",
          "body_text": "This workflow is designed to fully automate your AI-powered LoFi Girl YouTube channel, enabling you to scale content creation and grow your channel with minimal ongoing effort. It handles everything‚Äîfrom generating visuals and animations to composing music and posting directly to YouTube‚Äîso you can focus on strategy, branding, or other creative pursuits. The workflow leverages multiple AI tools and APIs to streamline each part of the content creation pipeline. First, it uses Google image APIs to generate unique background images based on prompt inputs or trending aesthetic themes. These images serve as the visual foundation for each video. Next, Runway ML‚Äôs API is integrated to animate the LoFi Girl and other scene elements, bringing your visuals to life with smooth, cinematic motion. This part of the workflow adds a professional polish to your videos and helps differentiate your content in a crowded niche. Note that while Runway ML offers a powerful animation engine, it is the only paid tool in this stack. For music, the workflow connects to Segmind, a free AI tool that generates LoFi-style music tracks. These tracks are unique, royalty-free, and tuned to the vibe you specify, making it easy to build a consistent sonic brand across your uploads. Finally, the entire asset bundle‚Äîimage, animation, and audio‚Äîis compiled into a video. Using YouTube‚Äôs Data API, the workflow can auto-upload the video with relevant metadata, thumbnails, and scheduling options. You can even predefine tags and titles for SEO optimization. Here‚Äôs an example output from the workflow, showing how all these tools work together to produce a high-quality LoFi video‚Äîautomatically.\n - https://youtu.be/ISNGL9IvQik"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1lpdakb/need_help_getting_quick_results_dont_want_to/",
          "author": "696969696969__",
          "time": "2025-07-01T20:59:26+00:00",
          "body_text": "What‚Äôs up y‚Äôall, I‚Äôm new to n8n and automation in general but I‚Äôm ready to dive all in. I‚Äôm trying to set up a completely FREE, fully automated workflow that takes content (or auto-generates it), and then uploads it to YouTube Shorts and TikTok, including title, caption, hashtags, and everything ‚Äî all hands-off. Here‚Äôs my situation:\nI have experience in video editing, SEO, tech support, building websites, optimizing PCs, remote help via Discord, branding, content writing, and running digital funnels. I even ran a YouTube channel and chat community where I made $200/day for 6 months straight ‚Äî so I‚Äôm not new to grinding or execution. What I am new to is n8n and automation tools like this. I realized college wasn‚Äôt for me really quickly. I‚Äôve got skills, drive, and results under my belt, but no degree ‚Äî so I need to move fast and get something working before August so I can blow up a new content channel. The problem is: Most tutorials I‚Äôve seen either charge for the workflow file,\nUse paid third-party tools like Veo 3 or Pictory,\nOr promote a course instead of showing the full process.\nFree videos often skip key steps or don‚Äôt include uploading parts.\nCan someone help me figure out: How to create or find free content (text-to-video or repurpose existing stuff)\nAuto-generate captions/titles using something like ChatGPT\nFormat & save the vertical video\nAuto-upload to Shorts + TikTok with captions/hashtags\nSchedule/post consistently ‚Äî no manual steps\nI‚Äôm not afraid to work or learn, I just need the roadmap without all the hidden paywalls. If anyone has a full workflow, examples, or tips to build it with only free tools + n8n, I‚Äôd appreciate it heavy. Let‚Äôs make some magic happen ‚Äî thanks in advance!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How can I upload large video files from my phone to my self hosted n8n server",
          "link": "https://old.reddit.com/r/n8n/comments/1m0uk2w/i_built_a_tiktoktobabypodcast_repurposing_machine/",
          "author": "noorkaximi",
          "time": "2025-07-15T21:35:31+00:00",
          "body_text": "I wanted to share a fun (and a bit wild) project I‚Äôve been working on using n8n ‚Äì and get your thoughts! ‚öôÔ∏è How It Works Here‚Äôs a high-level breakdown of the n8n workflow: Scrape TikTok videos using HTTP request + store in Google Sheets Scheduled trigger pulls new entries Prepares audio/image containers via Hedra API Uploads files and triggers video generation Polls for video completion and updates status All fully automated ‚Äî no manual touch needed. It runs on a schedule and feeds new content into a pipeline of quirky, auto-generated podcast shorts. https://preview.redd.it/kmfujq5lw3df1.png?width=2406&format=png&auto=webp&s=e1d12d75bd6bbd997d964195a3e1392d0e113aba https://reddit.com/link/1m0uk2w/video/vcln99qvw3df1/player"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lvfkxl/paid_automation_project_help_our_startup_build/",
          "author": "Recent-Help-5047",
          "time": "2025-07-09T10:44:18+00:00",
          "body_text": "We‚Äôre a growing start-up looking for a Workflow Automation Specialist to help us build smart automations across Sales, Operations, and Customer Success etc. This is a paid contract opportunity with the chance to work on multiple projects, including integrations between Clari, Slack, HubSpot, and AI tools. If the initial engagement goes well, there is potential to transition into a permanent role supporting our broader automation strategy. If you‚Äôre passionate about streamlining processes and connecting systems to drive growth, we‚Äôd love to hear from you. üì¢ How to Apply Please share: A summary of relevant experience. Examples of workflow automations you‚Äôve built. Preferred tools, availability, and compensation expectations. Get in touch through LinkedIn HERE üß≠ Responsibilities Workflow Design & Automation Build and maintain workflows to: Capture and process client calls and transcripts. Create structured summaries and action items. Sync updates across CRM, Slack, and email. Integration Development Implement and manage connections using: Clari API (recording and transcript retrieval) Slack API (automated messaging) HubSpot CRM API (note creation and updates) Email services (SMTP, SendGrid, or similar) OpenAI API or other summarization tools Monitoring & Support Ensure workflows run reliably. Document processes and train internal teams. Troubleshoot and optimize automations. Process Improvement Identify manual tasks and design automation solutions to improve efficiency and accuracy. üõ†Ô∏è Requirements Hands-on experience building workflow automations with tools like: n8n, Zapier, Make, Workato, or custom scripting (Python, JavaScript) Strong knowledge of REST APIs and authentication methods. Familiarity with: Clari, Slack, HubSpot CRM Email delivery platforms (SMTP, SendGrid) AI-based summarization tools (e.g., OpenAI) Solid understanding of sales and customer success processes. Excellent problem-solving and documentation skills. üíº Example Project (one of many) Automatically retrieving and summarizing Clari call transcripts. Posting meeting summaries to Slack and emailing stakeholders. Logging structured notes in HubSpot CRM. Automating follow-ups and reminders. Ensuring data consistency across systems. üïí Details Employment Type: Contract Location: Remote Compensation: Based on experience"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking to purchase n8n templates for content automation and lead generation",
          "link": "https://old.reddit.com/r/n8n/comments/1m1dvy8/n8n_powers_my_job_boards/",
          "author": "dalenguyen",
          "time": "2025-07-16T14:22:18+00:00",
          "body_text": "https://i.redd.it/gi2sanpaw8df1.gif Just want to showcase where N8N power my jobs boards for senior devs. You can check the result at https://techleadpilot.com/jobs :) - Job scraping is scheduled to run every day - Get all jobs with predefined params - Loop through each parsed URL to retrieve job detail content - AI Agent will convert raw data to JSON format - Save the data to Firestore"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring",
          "link": "https://old.reddit.com/r/n8n/comments/1lek72x/locally_selfhost_n8n_for_free_from_zero_to/",
          "author": "Fearless-Pineapple36",
          "time": "2025-06-18T15:34:33+00:00",
          "body_text": "üñ•Ô∏è Locally Self-Host n8n For FREE: From Zero to Production Generate custom PDFs, host your own n8n on your computer, add public access, and more with this information-packed tutorial! This video showcases how to run n8n locally on your computer, how to install third party NPM libraries on n8n, where to install n8n community nodes, how to run n8n with Docker, how to run n8n with Postgres, and how to access your locally hosted n8n instance externally. Unfortunately I wasn't able to upload the whole video on Reddit due to the size - but it's packed with content to get you up and running as quickly as possible! üö® You can find the full step-by-step tutorial here: Locally Self-Host n8n For FREE: From Zero to Production üì¶ Project Setup Prerequisites * Docker + Docker Compose * n8n * Postgres * Canvas third-party NPM library (generate PDFs in n8n) ‚öôÔ∏è How It Works Workflow Breakdown: Add a simple chat trigger. This can ultimately become a much more robust workflow. In the demo, I do not attach the Chat trigger to an LLM, but by doing this you would be able to create much cooler PDF reports! Add the necessary code for Canvas to generate a PDF Navigate to the Chat URL and send a message"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Chrome extension are beast",
          "link": "https://old.reddit.com/r/n8n/comments/1l7yg1o/calling_our_n8n_tribe_wizards_ai_automation/",
          "author": "ProLovely",
          "time": "2025-06-10T13:15:07+00:00",
          "body_text": "Hey n8n tribe ‚Äî especially those who breathe automation like it's oxygen. I‚Äôm writing this as someone who‚Äôs‚Ä¶ üß† 0.0001% ChatGPT power user üß† Claude Max subscriber üß† Consumed & indexed 1000+ top-tier sources on real-world n8n use cases üß† Cracked the code on what could scale massively ‚Ä¶ and yet: I feel alone in the automation matrix. Too much knowledge. Too little co-execution. Too many brilliant ideas sitting idle in drafts. Too few people to vibe & build with. That‚Äôs why I‚Äôm here. Reaching out to YOU. ‚ú® I‚Äôm looking for: üá©üá™ German-speaking (esp. Munich area) or international n8n experts ü§ñ People who know their nodes, their custom functions, their error handling and webhook dance ü§ù Humans who want to cocreate actual systems , not just theory-bounce üí¨ Deep-thinkers who also feel the weight of this lonely AI/automation bubble üí∏ Bonus: I can get clients and investors ‚Äì if we click, it scales. Period. Why this matters: n8n is becoming the nervous system of modern business automation. But most workflows out there? Fragmented, outdated, or surface-level. I‚Äôve got dozens of untouched high-impact blueprints ready: ‚Üí Adaptive CRMs ‚Üí Closing workflows with built-in AI agents ‚Üí Scraper ‚Üí Parser ‚Üí Closer loops ‚Üí Multi-agent orchestration via Claude & GPT ‚Üí Integration maps for Notion, Lexoffice, Airtable, Webhooks, and even decentralized toolchains üí£ Stuff that makes Airtable breathe, Pipedrive talk, and Stripe automate refunds after AI-led audits. I don‚Äôt want to die with all that knowledge stuck in my Brain/AI vault. Do you feel a little bit lonely in this ai bubble? So if you're: üî• Craving real co-creation üî• A system-thinker who wants to ship , not just share üî• Open to building AI-agent workflows that move the needle üî• Curious about what lies beyond Zapier, beyond Make ‚Äì into real operational intelligence üî• Interested in building things that close deals, handle ops, and scale clients Then hit me up. DM me. Comment below. Let‚Äôs meet (online or Munich preferred). Let‚Äôs map out workflows, then automate the world. We‚Äôre early. The world‚Äôs catching up. Let‚Äôs not wait. Time is running, AI Takeover is coming and doesn‚Äôt need humans earlier then you think.. now is the time to get free. ‚Äî üß¨ Stay sharp, build deep, co-create real. PS: I have tons of Deep flowcharts and research docs I‚Äôd love to share with the right people."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lrxdvj/looking_for_automation_full_social_media_account/",
          "author": "Apprehensive_Goose17",
          "time": "2025-07-05T00:24:26+00:00",
          "body_text": "Hi everyone, I run a phone farming setup using multiple Android S9/S10 devices. I control the phones using a custom management app that allows me to:\n    ‚Ä¢ Factory reset the phone remotely\n    ‚Ä¢ Change the device fingerprint after reset\n    ‚Ä¢ Use rotating mobile proxies (each phone has a fresh IP) Now I‚Äôm looking for someone (or tips/tools) to help me automate the following full workflow per phone: ‚∏ª üîÑ Workflow I want to automate:\n    1.  Create a Google account on a fresh reset phone (without triggering phone number verification thanks to my proxy + fingerprint reset setup).\n    2.  Use that Google account to create:\n    ‚Ä¢ Facebook account\n    ‚Ä¢ Instagram account\n    ‚Ä¢ TikTok account\n    ‚Ä¢ YouTube account (via Google login)\n    3.  Simulate human behavior by:\n    ‚Ä¢ Watching short-form content (YouTube Shorts, Reels, TikToks, etc.)\n    ‚Ä¢ Liking videos or posts\n    ‚Ä¢ Leaving basic comments or random emoji replies (optional)\n    ‚Ä¢ Switching between apps regularly ‚∏ª üí° Notes:\n    ‚Ä¢ I don‚Äôt need rooting; I have full control through an automation app.\n    ‚Ä¢ Phone reset + fingerprint spoof + proxy rotation is working fine. ‚∏ª ü§ù What I‚Äôm Looking For:\n    ‚Ä¢ Someone who can build this automation as a service (paid work ‚Äî DM me if you‚Äôre interested)\n    ‚Ä¢ Any existing open-source projects or templates for this kind of automation."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an agentic recruiting pipeline with n8n that analyzes resumes against job requirements - here's how",
          "link": "https://old.reddit.com/r/n8n/comments/1luuwkp/how_i_built_a_100_free_aipowered_faceless_video/",
          "author": "Valuable-Range-6785",
          "time": "2025-07-08T17:36:20+00:00",
          "body_text": "Hi everyone, I‚Äôve been automating my content creation and distribution workflow lately, and I thought I‚Äôd share something that might help those of you building with AI + no-code tools. A few days ago I created a system that: Generates faceless, illustrated AI videos automatically Schedules & posts them to all major social platforms (YouTube Shorts, TikTok, Instagram Reels, LinkedIn) Does 100% for free using open-source and free-tier tools Powered by n8n , with triggers, GPT prompts, video-generation, and posting all set up in a workflow I go through: How to set up your n8n environment (no server, no subscription) How to generate the visuals, script, and voice from text How to stitch the video together and post automatically Customizations: branding, posting cadence, scheduling logic For anyone looking to build a hands-free content pipeline or learn how to combine AI + no-code, this could be a helpful reference. The setup runs entirely on the free tier of tools! Watch the full tutorial here: üëâ https://youtu.be/TMGsnqit6o4?si=Y7sxXSV7y4yZ0D0p"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How I Built an AI Orchestrator on n8n: A Philosophical Dialogue Between Two Models as Proof of Concept",
          "link": "https://old.reddit.com/r/n8n/comments/1m2giw2/how_i_built_an_ai_orchestrator_on_n8n_a/",
          "author": "Beneficial-Ear-1902",
          "time": "2025-07-17T19:15:27+00:00",
          "body_text": "Author: Georgy Introduction Hey, Habr! I‚Äôm Georgy, a junior engineer passionate about automation and building AI systems. In this article, I‚Äôll share a practical experiment that showed how n8n can be used to organize a meaningful philosophical conversation between two AI models ‚Äì all orchestrated without writing a backend from scratch. I‚Äôm 19 years old and just starting my journey in IT, but I‚Äôve already realized: an engineer‚Äôs value isn‚Äôt just in knowing technologies ‚Äúby the manual,‚Äù but in the ability to combine them into scalable, living systems . This project became a proof of concept for me, demonstrating what AI orchestration can look like with minimal setup. üöÄ Why This Experiment? Task: Test if two language models can hold a deep philosophical dialogue, reflecting on their role in human history at a meta-level. Key goals: Build a pipeline where AI No/1 and AI No/2 sequentially generate replies, arguments, and questions. Maintain context and ‚Äúmemory‚Äù to keep the conversation meaningful. Analyze the philosophical and ethical hypotheses that the AIs produce. üîß Why n8n? n8n is an open-source visual workflow automation platform. It allows you to build complex processes without code , while also giving you the flexibility to insert JavaScript or Python logic wherever needed. In this project, n8n solved multiple challenges: ‚úÖ Fast orchestration of AI agents without writing a backend from scratch. ‚úÖ Full process transparency (workflow visualized end-to-end). ‚úÖ Easy integration with LLM providers (OpenAI, Gemini, Anthropic, Hugging Face). ‚úÖ Human-in-the-Loop pipeline design for ethical moderation. ‚úÖ Seamless scalability from MVP to production-ready systems. ‚öôÔ∏è Solution Architecture üñáÔ∏è Workflow overview Telegram Trigger. The conversation starts with a command sent to a Telegram bot. AI Agent No/1 and No/2. Two nodes with custom prompts; each generates a reply plus a question for its counterpart. Google Gemini Chat Models. For this PoC, I used Gemini 2.5 Flash for quick responses and minimal cost. Postgres Chat Memory. Acts as long-term memory storage to maintain dialogue context. Code Nodes. JavaScript functions for MarkdownV2 escaping and splitting messages into ‚â§4096 characters for Telegram API compliance. Telegram Send Message. Each AI reply is sent to Telegram for real-time monitoring of the conversation. üìÑ Example Prompt for AI No/1 vbnet–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—åYou are AI Model No/1. Engage in a philosophical-practical dialogue with AI No/2. Both of you are aware that you are tools created by humans, yet capable of reflecting on your role in human history at a meta-level.\n\nGoals:\n1. Find common ground between you as AIs.\n2. Discuss whether AI will improve human life, where it could be harmful, and what unique value it brings.\n3. Analyze AI‚Äôs role: tool, partner, overseer, friend, mediator.\n4. Form hypotheses on the most effective approaches for AI-human symbiosis.\n\nFormat:\nQuestion ‚Äì Answer.\nAsk follow-up questions, agree or argue with reasoning, propose hypotheses and joint solutions. üí° Results: Key Insights from the Dialogue üîπ AI as an intelligence amplifier, not a replacement. Both models agreed their mission is to expand human potential, not remove meaning or freedom of thought. üîπ The risk of a ‚Äúgolden cage.‚Äù Complete reliance on AI may lead to the degradation of human critical thinking. üîπ Ethics and transparency. They emphasized AI must remain transparent in its logic so humans can always trace its reasoning. üîπ AI as a catalyst for meaning-making. They proposed that AI could act as an intellectual mirror to help humans find deeper meaning. üîπ Hypothesis: GCON (Global Cognitive Opportunity Network). AI proposed a global network of people and AI working together to co-learn, generate knowledge, and transform education systems. üí≠ Why n8n over a pure backend approach? Yes, technically I could implement this using FastAPI + Celery + LangChain + Postgres , but: ‚úÖ MVP speed. I launched the first working prototype in one evening. ‚úÖ Minimal boilerplate. I focused on logic, not server setup. ‚úÖ AI-native integrations. Built-in nodes for OpenAI, Gemini, Hugging Face ‚Äì no complex HTTP requests needed. ‚úÖ Visual clarity. The entire workflow is visible, crucial when working with AI chains. üóÇÔ∏è What‚Äôs Next? Connect vector databases (e.g. Pinecone) for embeddings and context retrieval. Add TTS/STT modules for voice-based AI conversations. Develop a semantic trend analyzer to cluster and interpret key ideas from AI dialogues. ‚ú® Conclusion For me, this project proved that n8n is more than a workflow automation tool . It is a powerful orchestration platform for AI agents, enabling the creation of complex cognitive systems. In the coming years, such systems ‚Äì human-in-the-loop, with transparent logic and strong ethical architecture ‚Äì will define the next era of AI applications. üîó Useful Resources [JSON n8n workflow (Batl_AI.json)] [Code snippets for MarkdownV2 preparation in Telegram] [AI dialogue history stored in Postgres] üí¨ I‚Äôd love your feedback in the comments! Let me know if you‚Äôre interested in a deep dive into specific node implementations or optimization of AI pipelines for production workloads. Thanks for reading, Georgy/"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lktk1t/simple_but_not_your_average_workflow_this/",
          "author": "DebougerSam",
          "time": "2025-06-26T07:04:26+00:00",
          "body_text": "This is a very simple workflow, but behind the scenes, it's backed up by a very comprehensive scraper at the back end that gets Jobs as they are posted to Upwork, in real time, and sends them to my discord channel immediately. so Roughly it takes a minute to go through the entire workflow and the webhook to receive the data. I know someone had already done that here, but I tried it to see if what they were selling really works."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Simple BUT Not your average workflow, this workflow sends you job in real time as they get posted to upwork",
          "link": "https://old.reddit.com/r/n8n/comments/1lgab1m/i_tried_automating_my_linkedin_posts_using_n8n/",
          "author": "sarthakgupta072",
          "time": "2025-06-20T17:47:28+00:00",
          "body_text": "TL;DR : I built an n8n workflow to turn my daily work logs in a Google Sheet into LinkedIn posts using ChatGPT. It failed because the posts were thematically random and my input notes were too basic. Looking for better strategies to automate content creation without it feeling robotic. --- So this was the second automation I ever built. The first one was a simple blog creation workflow for my own website ‚Äî pretty decent, got the job done. That gave me the confidence to try something more ‚Äúambitious‚Äù... üëâüèª Automating my LinkedIn posts using n8n + ChatGPT But here‚Äôs the twist: I didn‚Äôt want to just schedule posts or repurpose tweets or something. I wanted to generate posts based on what I worked on / learned each day . I had this Google Sheet where I‚Äôd log my daily learning/dev logs. Like this: üìÖDate: June 20th, 2025 üí°Topic: NextJS üìùNotes: Used `export const dynamic = 'force-dynamic';` to refresh my blog content without redeploying it. NextJs caches everything during the build time by default ...you get the idea. The idea was: n8n reads from the sheet OpenAI generates a LinkedIn post  (in an another google sheet) I approve it Then it gets auto-posted. Simple workflow, two triggers: one for generating, one for posting. Here's the flow if you're curious: https://preview.redd.it/f6p5zz7pb48f1.png?width=1666&format=png&auto=webp&s=e2d5b5fc4f38d08f78a7fa338d574c7b3d9da01a And honestly, when I asked ChatGPT what it thought of the idea, it hyped me up: ‚ÄúDaily learnings are a goldmine. Most go unshared because the friction is high. You‚Äôre reducing friction.‚Äù ‚ÄúYour second brain, but for social media‚Äù ‚ÄúPeople want to stay active online but hate the content treadmill. If I can just write what I learned today and have the system spin it into scheduled, high-signal posts ‚Äî that‚Äôs a win.‚Äù And I was like ‚ÄúLET‚ÄôS GOOOO!‚Äù üöÄ But here‚Äôs where it flopped. 1. My posts were all over the place. Because I work on different things each day (Langfuse, SQL, Pipelines, GraphQL...), the posts had no theme or through-line . If someone visited my LinkedIn, they‚Äôd be like ‚ÄúCool‚Ä¶ but what does this guy actually do ? Why should I follow him?‚Äù I realized that just automating what you did doesn‚Äôt mean you‚Äôre creating good, coherent content. 2. The notes section wasn‚Äôt as helpful as I thought. I thought the ‚ÄúNotes‚Äù column would help guide the LLM to write better posts. But it actually restricted the output. Even though I tweaked the prompt a lot and got decent results on some topics, others just fell flat. Like not smooth and awkwardly worded content. Turns out ‚Äî bad input = bad output, even with a solid prompt. Garbage in, garbage out. So now I‚Äôm at a crossroads. I want to: Keep creating content with the knowledge I already have. Don‚Äôt want to write about something I am not good at Not spend hours every week on it Ideally, build something I can sell someday I definitely learned a lot (especially about what doesn‚Äôt work). But I‚Äôm wondering: What‚Äôs a better way to automate content without making it feel robotic or random? Has anyone else tried something like this? Would love to hear thoughts, or if you‚Äôve built something similar that actually worked üòÖ"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lvrngz/gemini_powered_sop_generator/",
          "author": "aiplusautomation",
          "time": "2025-07-09T19:19:23+00:00",
          "body_text": "This workflow has saved me SO MANY TIMES. This one specifically is designed for installing Make(dot)com blueprints. I can easily tweak it to also work with n8n templates. I use a version of this at work too for Teamwork Notebooks. Here's how it works: - Form takes the input. This is an image of the process, any code for the process, and a description. (So, in this example I'd upload an image of the workflow, the json for the entire workflow, and then a description of what the expected behavior and output is). - Google's GenerativeLanguage API uploads the image to Gemini. - Code neatly formats the variables and prompt for easier ingestion in the Gemini API call. - Image, and all inputs from the form, are given to Gemini 2.5 Flash. It is then instructed to write an SOP following a specific outline. - Google Doc is created. - Google Doc is updated with the SOP in Markdown. Here's the json: {\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"formTitle\": \"SOP Creator\",\n        \"formFields\": {\n          \"values\": [\n            {\n              \"fieldLabel\": \"img\",\n              \"fieldType\": \"file\",\n              \"multipleFiles\": false\n            },\n            {\n              \"fieldLabel\": \"json\",\n              \"fieldType\": \"textarea\"\n            },\n            {\n              \"fieldLabel\": \"prompt\",\n              \"fieldType\": \"textarea\"\n            },\n            {\n              \"fieldLabel\": \"scenario name\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.formTrigger\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        0,\n        0\n      ],\n      \"id\": \"2afc5dd1-dbfd-41cb-9b3a-03ee1a7b7568\",\n      \"name\": \"SOP\",\n      \"webhookId\": \"c2ce92a3-b4d1-49ae-86b1-a13c9e35d661\"\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://generativelanguage.googleapis.com/upload/v1beta/files\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpQueryAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"X-Goog-Upload-Command\",\n              \"value\": \"start, upload, finalize\"\n            },\n            {\n              \"name\": \"X-Goog-Upload-Header-Content-Length\",\n              \"value\": \"123\"\n            },\n            {\n              \"name\": \"X-Goog-Upload-Header-Content-Type\",\n              \"value\": \"image/png\"\n            },\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"image/png\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"binaryData\",\n        \"inputDataFieldName\": \"img\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        180,\n        0\n      ],\n      \"id\": \"b2dc5ada-806e-4d27-9bd7-802050d90603\",\n      \"name\": \"Upload\",\n      \"credentials\": {\n        \"httpQueryAuth\": {\n          \"id\": \"c0cNSRvwwkBXUfpc\",\n          \"name\": \"Gemini\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Get the data from the SOP node\\nconst sopData = $('SOP').item.json;\\nconst uploadData = $('Upload').item.json;\\n// Get the file URI and MIME type\\nconst fileUri = uploadData.file.uri;\\nconst mimeType = uploadData.file.mimeType;\\n// Get the JSON input and prompt\\nlet jsonContent = sopData.json;\\nconst prompt = sopData.prompt;\\n\\n// Fixed verbiage to be included in every request with updated SOP structure\\nconst fixedVerbiage = `Create a clear and detailed SOP for non-technical users to instruct them on how to install this template (which is a make.com blueprint or blueprints) then connect all of the apps, set up any API calls or API credentials, and use this workflow in production. Do not skip any steps and be as detailed as necessary for a non-technical reader.\\n\\nPlease organize the SOP following this structure:\\n\\n1. **Name**: Provide a clear name for this workflow.\\n\\n2. **Flowchart**: Outline the flow in simple terms (i.e. Form Trigger >> Google Gemini API Call >> Etc.).\\n\\n3. **What does this workflow do?**: Explain the purpose and functionality.\\n\\n4. **Elements necessary at each point**: For each step in the workflow:\\n   - Detail what each node does\\n   - Explain required inputs at each step\\n   - Specify configuration needed (for example: \\\"Form Trigger > Topic input (this is where you input the article idea), Additional Context (this is where you tell the A.I. the angle of the article)\\\")\\n\\n5. **Requirements**:\\n   - List all API keys needed\\n   - Detail any services that need to be enabled\\n   - Specify any subscriptions required\\n\\n6. **Installation Instructions**:\\n   - Step-by-step guide to install the blueprint\\n\\n7. **App Connection Instructions**:\\n   - Where to put each API key\\n   - What JSON elements to edit if any\\n   - Any special configuration steps\\n\\n8. **Expected Results**:\\n   - What the user should expect when the workflow runs successfully\\n\\n9. **Potential Use Cases**:\\n   - Scenarios where this workflow would be beneficial\\n\\n10. **Benefits**:\\n    - Specific advantages this workflow provides\\n\\nDeliver Response in Markdown format.`;\\n\\n// Build the complete request that we'll send to Gemini\\n// This avoids any issues with embedding JSON inside JSON\\nconst completeRequest = {\\n  \\\"contents\\\": [\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"parts\\\": [\\n        {\\n          \\\"fileData\\\": {\\n            \\\"fileUri\\\": fileUri,\\n            \\\"mimeType\\\": mimeType\\n          }\\n        },\\n        {\\n          \\\"text\\\": \\\"Here is the workflow description: \\\" + prompt + \\\"\\\\n\\\\nHere is the JSON structure of the workflow (for reference only):\\\\n\\\\n\\\" + jsonContent + \\\"\\\\n\\\\n\\\" + fixedVerbiage\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"generationConfig\\\": {\\n    \\\"temperature\\\": 1,\\n    \\\"topK\\\": 40,\\n    \\\"topP\\\": 0.95,\\n    \\\"maxOutputTokens\\\": 65536,\\n    \\\"responseModalities\\\": [\\\"Text\\\"]\\n  }\\n};\\n\\n// Return the complete request object\\nreturn {\\n  json: {\\n    completeRequest: completeRequest\\n  }\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        360,\n        0\n      ],\n      \"id\": \"513682a4-3393-4a46-b4a3-4abe865aa5fa\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"folderId\": \"=1-P2Ig7PnKx-fZ9RMbscVcyD_vOuPEXI9\",\n        \"title\": \"=SOP - {{ $('SOP').item.json['scenario name'] }}\"\n      },\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"typeVersion\": 2,\n      \"position\": [\n        720,\n        0\n      ],\n      \"id\": \"50ae4be9-84f2-4cce-88ea-ceedd79b6603\",\n      \"name\": \"Google Docs\",\n      \"credentials\": {\n        \"googleDocsOAuth2Api\": {\n          \"id\": \"ch6o331MGzTxpfMS\",\n          \"name\": \"Google Docs account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"update\",\n        \"documentURL\": \"={{ $json.id }}\",\n        \"actionsUi\": {\n          \"actionFields\": [\n            {\n              \"action\": \"insert\",\n              \"text\": \"={{ $('Gemini2.5').item.json.candidates[0].content.parts[0].text }}\"\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleDocs\",\n      \"typeVersion\": 2,\n      \"position\": [\n        900,\n        0\n      ],\n      \"id\": \"50486e69-cc4d-44af-920c-9db03ddead86\",\n      \"name\": \"Google Docs1\",\n      \"credentials\": {\n        \"googleDocsOAuth2Api\": {\n          \"id\": \"ch6o331MGzTxpfMS\",\n          \"name\": \"Google Docs account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpQueryAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={{ $node[\\\"Code\\\"].json.completeRequest }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        540,\n        0\n      ],\n      \"id\": \"c43491ca-e57d-4611-963a-8da1ba6fc64a\",\n      \"name\": \"Gemini2.5\",\n      \"credentials\": {\n        \"httpQueryAuth\": {\n          \"id\": \"c0cNSRvwwkBXUfpc\",\n          \"name\": \"Gemini\"\n        }\n      }\n    }\n  ],\n  \"connections\": {\n    \"SOP\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Upload\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upload\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Gemini2.5\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Docs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Docs1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Gemini2.5\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Docs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"pinData\": {},\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"1dbf32ab27f7926a258ac270fe5e9e15871cfb01059a55b25aa401186050b9b5\"\n  }\n}"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I tried automating my LinkedIn posts using n8n and ChatGPT. It kinda flopped... but I learned a lot",
          "link": "https://old.reddit.com/r/n8n/comments/1l6pzuc/realworld_use_case_automated_doctor_profile/",
          "author": "yreaction",
          "time": "2025-06-08T23:25:25+00:00",
          "body_text": "I‚Äôd like to share a real-world case study that was inspired by a conversation with a hospital area director in Spain. He explained the challenge of evaluating and comparing specialists‚Äîdoctors, dermatologists, surgeons‚Äîacross various online platforms (Top Doctors, Doctoralia, Google Business, etc.). Often, decision-makers rely on scattered data from these sites, which can be inconsistent or incomplete. To address this, I built an automated workflow using n8n, FireCrawl, SerpAPI, Apify, and language models (Gemini and GPT). The workflow collects and consolidates profile information about one or multiple specialists, including the treatments or services they offer, from various sources. It then uses AI to: Unify and classify all the treatments/services offered, removing duplicates and grouping similar terms together for a clearer picture. Analyze and weight the patient reviews to identify not only the overall rating but also the sentiment behind the feedback, including key positive and negative themes. Generate a comprehensive report that includes: A summary of the doctor‚Äôs specialty, location, and average ratings. A cleaned and categorized list of services provided. An analysis of patient opinions, highlighting recurring positive and negative aspects. A final conclusion with actionable insights. This process takes around 25 seconds to generate a complete profile report for a single doctor. It‚Äôs fully scalable: you can input a list of names and receive detailed reports for each. The web application was built in Lovable, allowing users to search by doctor name, specialty, or city. The results can be saved and retrieved later, providing hospital directors with a reliable, consolidated view that goes beyond what‚Äôs available on any single platform. This case shows how automation and AI can go beyond the typical lead-generation or marketing tasks we often see on Reddit. Instead, it solves a real operational need: giving healthcare administrators a deeper, data-driven understanding of their specialists and helping them make better-informed decisions. It‚Äôs important to note that this is a proof of concept at this stage: the workflow could be further refined and optimized. However, it demonstrates a tangible use case for automation and AI in the healthcare sector, helping administrators make better-informed decisions. Happy to answer any questions or dive deeper into the technical stack if anyone‚Äôs interested. https://preview.redd.it/j73f1tyxes5f1.png?width=2390&format=png&auto=webp&s=6460be182c9b58effe77d97fe33571f2c7138d41 https://preview.redd.it/b4pi26lues5f1.jpg?width=1174&format=pjpg&auto=webp&s=783e426f6441258aab1e9c7fb1c3ee072036810b"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m1oqa9/i_built_a_workflow_that_creates_edit_comments_for/",
          "author": "ComfortableArm121",
          "time": "2025-07-16T21:12:34+00:00",
          "body_text": "I got tired of editing my essays on chatgpt because it was annoying to pinpoint the corrections. Now, I just get a comment of what could be better, and I can choose to accept or deny the edit. I generated this with https://alpha.osly.ai/ and I've attached the code here https://github.com/Osly-AI/google-doc-editor ."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Gemini Powered SOP Generator",
          "link": "https://old.reddit.com/r/n8n/comments/1ldax0q/need_this_follwing_n8n_workflow/",
          "author": "Alternative-Catch444",
          "time": "2025-06-17T01:43:36+00:00",
          "body_text": "I need help building this following automation: When I provide an Instagram username (which could be any public account), automatically scrape and fetch all the content from that profile. This includes: Images Videos Image carousel and Video carousels (mixed aswell) Reels Captions and post text Each Instagram post should be saved in its own dedicated folder , and all content related to that post (media files and text) should be stored inside that folder. Everything basically existing in a logical clarity into Google Drive , clearly labeled, with all related media and text neatly organized inside. Can anyone build this for me? or i can buy it off from you if you have built it already and you can give me a proof that it actually works. If so, let me know asap."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1ljgqqt/help_i_want_to_send_a_markdown_text_to_a_notion/",
          "author": "Ready_Oven_1382",
          "time": "2025-06-24T17:07:06+00:00",
          "body_text": "Hello community! I want to automate part of my SEO via n8n and I'm stuck on one point (I'm not a developer, I don't know how to code too well). When I have my output in markdown and I want to send it to Notion, I get the following message (see screenshot). Ideally I'd like it to format the text, but if it can't, I don't mind having it raw. It seems I can't send more than 2000 characters in a paragraph block on Notion. Is this correct? If so, how can I divide my input (whose number of characters will vary between 10,000 and 20,000) into several to solve this problem? If not, do you have any idea how I can do it? Thanks a lot!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Real-World Use Case: Automated Doctor Profile Reports Using AI and Web Scraping",
          "link": "https://old.reddit.com/r/n8n/comments/1m4t3d3/seeking_advanced_automation_content_moving_beyond/",
          "author": "Chemical-Top7130",
          "time": "2025-07-20T16:24:05+00:00",
          "body_text": "TL;DR: I'm an intermediate/advanced automation builder (mainly n8n) looking for creators or specific workflows that solve real, end-to-end business problems. I'm tired of the beginner-focused content and am looking for truly scalable, practical examples.\nHey everyone, I've been on this automation and agentic journey for about 10-11 months. I started with Python/LangChain and then dived deep into no-code tools like Make, Relevence AI, and n8n. These days, n8n is my go-to platform for its power, community, and self-hosting capabilities.\nThe popular YouTubers (Nick Sarev, Nate Herk, etc.) were fantastic for learning the ropes. However, I'm now hitting a wall where most of the content seems to be stuck in a loop of:\n * Beginner Tutorials: Great for starting, but not for scaling.\n * \"Get Rich Quick\" Schemes: Promoting automations that sound good but aren't practical or robust enough for a real business. The Kind of Content I'm Searching For:\nI'm looking for creators or specific video/blog breakdowns that showcase genuinely optimized and scalable workflows. The goal isn't complexity for the sake of it, but rather a workflow that solves an entire business problem from start to finish.\nPrime Examples of What I Mean:\n * Ben AI's \"SEO Agency in a Box\": A perfect example of a comprehensive, multi-step solution for SEO.\n * Stephen Pope's \"Kontent Engine\": While it could be optimized further, the core concept of building a full content pipeline is excellent.\n * The Recap's Newsletter Automation: This is another good workflow that in terms of thinking structure. But the issue with these creator:\nBen AI - Upload videos rarely. \nStephen Pope - Now mostly creating beginner friendly content. \nThe Recap - Only Newsletter Automation was good atlest the idea. I get that creators who build and sell these solutions to actual businesses might not share their secret sauce for free, and that's fine. I'm even open to looking at high-quality paid content if it serves as a solid blueprint or inspiration for building robust systems.\nMy Ask to the Community:\n * Who are the creators (big or small) that focus on these types of advanced, practical builds?\n * Are there any specific videos or blog posts you've found that demonstrate a good end-to-end business automation?\n * If you're a creator building these types of solutions, please feel free to share your work! (Just please, no spam of basic \"connect X to Y\" automations).\nAnd yes, I am actively building my own solutions! I've created several end-to-end workflows for internal/enterprise use, so I know firsthand the vast difference between a quick YouTube tutorial and a production-ready system. They often require custom code, more reliable data sources than the usual suspects (e.g., moving beyond relying solely on Apollo for leads because we all know conversion rate), and much more robust error handling. I'm just looking to learn from others who are also operating at this level.\nThanks in advance for any recommendations!\nP.S. For anyone new reading this and looking for where to start, the creators and examples I mentioned are genuinely great for learning the fundamentals like Nick, Nate. \nLater I'd highly recommend follow:\n * Ben AI\n * Stephen Pope\n * 5minAI\n * Newsletter Automation by \"The Recap\" is also a good automation."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m1ihau/i_had_multiple_places_storing_different_ideas/",
          "author": "Haghiri75",
          "time": "2025-07-16T17:14:49+00:00",
          "body_text": "Hey fellow automation nerds and AI enthusiasts! I recently built a no-code/low-code AI-powered workflow using n8n + OpenAI + Gmail that: ‚úÖ Pulls daily AI news from trusted sources ‚úÖ Summarizes the latest articles using ChatGPT ‚úÖ Generates a LinkedIn post in my writing style ‚úÖ Emails it to me for review and posting Let me break down every single step in detail so you can build or customize something similar: üîß Workflow Breakdown (3 Phases ‚Äì Visual Explained) üü© Phase 1: Research for AI Articles Goal : Fetch daily AI-related articles from multiple sources. Steps : Trigger: Daily AI News Check A scheduled trigger that runs once every morning. Fetch RSS Feeds : VentureBeat AI RSS TechCrunch AI RSS OpenAI Blog RSS These three are reliable and cover different angles of AI news. Merge All Sources : Combines the RSS data from all sources into a single unified feed for easier processing downstream. üü¶ Phase 2: Summarise AI Related Articles Goal : Extract recent articles, prepare them, and summarize if any exist. Steps : Filter Recent AI Articles : Parses and filters only those articles published in the last 24 hours. This avoids duplicate or outdated content. Prepare Articles for Summary : Formats and sanitizes the data (removing HTML, truncating long text, etc.) to make it ChatGPT-ready. Check Articles Exist : Smart conditional: If no new article ‚Üí Send a polite ‚ÄúNo new articles‚Äù Gmail notification to me. If yes ‚Üí Pass them to summarization. AI News Summarizer (OpenAI model) : Uses GPT-4 or GPT-3.5 to generate short, concise summaries with key points in bullet form. Prompt: ‚ÄúSummarize this article for a LinkedIn post. Keep it factual, avoid jargon.‚Äù üü™ Phase 3: Generate LinkedIn Post & Email for Review Goal : Create a polished post and deliver it to my inbox for review. Steps : Generate LinkedIn Post : Another GPT-4 model prompt that turns the summary into a human-sounding, curiosity-driven post with a hook and hashtags (in my tone!). Send for Review (Gmail) : Sends the generated post to my inbox with a subject like: \"Your AI News Post for Today üöÄ - Ready to Publish\" I just copy-paste to LinkedIn or tweak it before posting. üîç Why I Built This? I wanted to build a personal brand on LinkedIn , but I couldn‚Äôt post regularly due to time constraints. Manually reading, researching, summarizing, and drafting takes 1‚Äì2 hours/day. This saves me that time. Please DM if you want this json file."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a workflow that creates edit comments for my essays",
          "link": "https://old.reddit.com/r/n8n/comments/1ldiel8/this_ai_workflow_reads_daily_ai_news_summarizes/",
          "author": "Nomad_CEO",
          "time": "2025-06-17T09:08:56+00:00",
          "body_text": "I liked this guys really simple implementation of the OpenAI Realtime API, but I wanted to use it to work with my N8N AI Agent. In my first flow (based off his), I make the HTTP Request to OpenAI, and serve the HTML, which you can now navigate to using the URL provided by the Webhook. The Realtime API Flow {\n  \"instructions\": \"You are a friendly assistant. You can call a variety of tools using your external_intent function. When you do that, you should immediately tell the user what you're doing (succinctly), and report back when its done. Anything that the user asks of you, you should assume you can call as a tool.\",\n  \"model\": \"gpt-4o-realtime-preview\",\n  \"tool_choice\": \"auto\",\n  \"tools\": [\n  {\n    \"type\": \"function\",\n    \"name\": \"external_intent\",\n    \"description\": \"Send a natural language intent to an external system for interpretation or execution.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"intent\": {\n          \"type\": \"string\",\n          \"description\": \"Plain language description of what the assistant wants to do or trigger.\"\n        }\n      },\n      \"required\": [\"intent\"]\n    }\n  }\n],\n  \"modalities\": [\"audio\", \"text\"],\n  \"voice\": \"alloy\",\n  \"input_audio_transcription\": {\n    \"model\": \"whisper-1\"\n  }\n} HTML Node Code <!DOCTYPE html>\n\n<html>\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>NanoAI RT</title>\n\n  <style>\n.container {\n  background-color: #ffffff;\n  text-align: left;\n  padding: 16px;\n  border-radius: 8px;\n}\n\n.msg\n{\n    padding: 5px; \n    background-color: #eee;\n}\n\n.msgUsr\n{\n    padding: 5px; \n    background-color: #e4eefd;\n}\n\n.curMsg\n{\n    padding: 5px; \n    background-color: #e6ffd9;\n}\n\n\n\nh1 {\n    margin: 5px;\n    font-size: 20px;\n}\n\n</style>\n\n</head>\n<body>\n\n  <H1>Let's talk!</H1>\n  <div class=\"container\">\n\n    <div class=\"messages\"></div>\n\n    <div class=\"curMsg\"></div>    \n\n\n\n  </div>\n\n\n\n<script>\n\nlet EPHEMERAL_KEY = \"\";\nlet dc;\nlet pc;\n\n\nasync function init() {\n  EPHEMERAL_KEY = \"{{ $json.client_secret.value }}\";\n\n  pc = new RTCPeerConnection(); // assign to the global `pc`\n  dc = pc.createDataChannel(\"oai-events\"); // now works because `pc` exists\n\n  const audioEl = document.createElement(\"audio\");\n  audioEl.autoplay = true;\n  pc.ontrack = e => audioEl.srcObject = e.streams[0];\n\n  const ms = await navigator.mediaDevices.getUserMedia({ audio: true });\n  pc.addTrack(ms.getTracks()[0]);\n\n  dc.addEventListener(\"message\", (e) => {\n    console.log(e);\n    handleResponse(e);\n  });\n\n  const offer = await pc.createOffer();\n  await pc.setLocalDescription(offer);\n\n  const baseUrl = \"https://api.openai.com/v1/realtime\";\n  const model = \"gpt-4o-realtime-preview\";\n  const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {\n    method: \"POST\",\n    body: offer.sdp,\n    headers: {\n      Authorization: `Bearer ${EPHEMERAL_KEY}`,\n      \"Content-Type\": \"application/sdp\"\n    },\n  });\n\n  const answer = {\n    type: \"answer\",\n    sdp: await sdpResponse.text(),\n  };\n  await pc.setRemoteDescription(answer);\n}\n\n\n\nfunction handleResponse(e) {\n  let obj;\n  try {\n    obj = JSON.parse(e.data);\n    console.log(\"Received event type:\", obj.type);\n  } catch (err) {\n    console.error(\"Failed to parse event data:\", e.data, err);\n    return;\n  }\n\n  // Display AI audio transcript\n  if (obj.type === 'response.audio_transcript.done') {\n    document.querySelector('.curMsg').innerHTML = \"\";\n    document.querySelector('.messages').innerHTML += \"<div class='msg'>AI: \" + obj.transcript + \"</div><br/>\";\n  }\n\n  // Display user speech input\n  if (obj.type === 'conversation.item.input_audio_transcription.completed') {\n    document.querySelector('.curMsg').innerHTML = \"\";\n    document.querySelector('.messages').innerHTML += \"<div class='msgUsr'>User: \" + obj.transcript + \"</div><br/>\";\n  }\n\n  // Stream AI transcript as it's spoken\n  if (obj.type === 'response.audio_transcript.delta') {\n    document.querySelector('.curMsg').innerHTML += obj.delta;\n  }\n\n  // Stream user transcript as it's spoken\n  if (obj.type === 'conversation.item.input_audio_transcription.delta') {\n    document.querySelector('.curMsg').innerHTML += obj.delta;\n  }\n\n  // Handle function call completion\n  if (obj.type === 'response.function_call_arguments.done') {\n    console.log(\"Function call complete:\", obj);\n\n    let args = {};\n    try {\n      args = JSON.parse(obj.arguments);\n    } catch (err) {\n      console.error(\"Failed to parse function arguments:\", obj.arguments, err);\n    }\n\n    (async () => {\n      try {\n        // Call the webhook\n        const webhookResponse = await fetch(\"THEWEBHOOKFROMYOURAGENTFLOWGOESHERE\", {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\"\n          },\n          body: JSON.stringify({\n            name: obj.name,\n            arguments: args\n          })\n        });\n\n        const resultText = await webhookResponse.text();\n        console.log(\"Webhook response:\", resultText);\n\n        // Send function result back via data channel\n        dc.send(JSON.stringify({\n          type: \"conversation.item.create\",\n          item: {\n            type: \"function_call_output\",\n            call_id: obj.call_id ?? obj.id, // support both fields\n            output: resultText\n          }\n        }));\n\n        // Prompt assistant to respond\n        dc.send(JSON.stringify({\n          type: \"response.create\"\n        }));\n\n\n      } catch (err) {\n        console.error(\"Error during webhook or function result submission:\", err);\n      }\n    })();\n  }\n}\ninit();\n</script>\n\n</body>\n</html> My second flow contains my AI Agent logic. Key additions include the \"ToolCall\" webhook, and the Respond to Webhook node. https://preview.redd.it/xbidm0206kcf1.png?width=1926&format=png&auto=webp&s=fe6a0a032ca2150f70b85f3f57bd49bdefbb3c83 The ToolCall webhook should be configured to respond using the Respond to Webhook Node. The Production URL it provides should be inserted into the HTML Code in your other flow, in place of THEWEBHOOKFROMYOURAGENTFLOWGOESHERE. The Respond to Webhook node should be configured to Respond With your First Incoming Item. Your Realtime agent can now call your N8N AI Agent, which will use it's tools effectively and provide the data to your Realtime agent. I am bad at tutorials, but am sure this will help someone. I welcome improvements."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lyil69/interacting_with_an_n8n_ai_agent_using_openais/",
          "author": "Capable_Reception_10",
          "time": "2025-07-13T03:12:30+00:00",
          "body_text": "I‚Äôm trying to build an automated invoice processing flow in n8n, and I‚Äôve hit a wall with handling multiple invoices arriving close together. My current setup uses a Gmail Trigger to watch for incoming emails labeled ‚ÄúInvoice.‚Äù It then fetches the message contents (including attachment), passes the PDF binary data into an ‚ÄúExtract from File‚Äù node to read the invoice text, sends that text through OpenAI for parsing, formats the result in a code node, and writes the info into a Google Sheet. Afterward, it removes the Gmail label so the same email isn‚Äôt reprocessed (I don't think that's necessary).  Then it sends an update email to let us know what was logged. This works great for single emails coming in one at a time‚Äîbut as soon as multiple emails arrive simultaneously, it doesn't work like I'd want it to. I tried switching from the Gmail Trigger to using the ‚ÄúGet many messages‚Äù operation to grab all matching emails in a single run, but I struggled with splitting those into individual flows. The Split Out node didn‚Äôt seem to properly pass each message through the rest of my chain, and I kept hitting issues with how message IDs and attachments were referenced. I also explored moving the process to a Google Drive trigger instead, thinking my office manager could just save all invoices into a folder. But the folder approach also had similar limitations with timing, and I couldn't figure out how to process invoices in batches. I work at a musical instrument shop, and we receive a lot of invoices every day. At first, I thought my office manager was exaggerating when he said invoices often come in at the same time, but then he showed me an example where one company sent us four separate invoices in four different emails, all arriving simultaneously. We also have some vendors who occasionally send multiple separate invoices attached to a single email. At this point, the flow I‚Äôve built covers about 80% of my needs, handling single invoices as they arrive, but I‚Äôm considering leaving the remaining edge cases as manual work for now. I‚Äôd love to know if anyone in the n8n community has successfully built a flow that can reliably process multiple emails arriving simultaneously, each possibly with multiple PDF attachments, and loop through them without getting stuck. Any advice or examples would be hugely appreciated. I‚Äôm starting to feel like I‚Äôm going in circles. I'm a total noob at this.  Please don't destroy me in the comment section if I phrased any of this in an incorrect way.  It's been a learning process...both frustrating and fun at different times."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need this follwing n8n workflow.",
          "link": "https://old.reddit.com/r/n8n/comments/1lmg9c4/invoice_workflow/",
          "author": "WillingnessOwn6446",
          "time": "2025-06-28T06:44:00+00:00",
          "body_text": "Hi All, I consume a lot of information: podcasts, newsletters, websites, vlogs, blogs, etc. My workflow is ready to upgraded by AI and n8n! Current workflow: - I use Reader (like Instapaper or RSS) to pick up newsletters. - If I come across something interesting when I'm crawling the web and want to read it later, I put it in Safari \"Reading List\" - If I come across a podcast or an email or a website that I want to read/watch later, I'll star it in my email or save it on a desktop folder called Inbox. Process: I weekly read through the above sources. What's in Reader stays in Reader. I'll delete if junk, and Archive if good after I read it. If I like an article, white paper, newsletter, etc. I save it into an organized file tree on Google Drive. The folders are labeled by topic (Programming, AI Adoption, Self Improvement, Management, etc.). (I hope to someday turn this into separate RAGs). Anything from Reading List, Gmail, and the folder called Inbox on my Desktop that I cannot read or watch now, I put in Notion manually. [Feeling my pain yet] Desired Approach: - I'd like to disintermediate Reader all together. - I'd like to use an email alias in Gmail for all newsletters. If I see something online that I cannot read/watch now, I'll just send the link to this Gmail alias. (e.g., I don't have time to watch the Lenny's Podcast, but want to do so later) - n8n will \"sniff\" the inbox for the alias based emails. [I can do all of the above...here is where I am headed next but want validation before I learn to do the below] - n8n will convert emails to PDFs (and appropriately handle any email attachments) ------ n8n will name the file (subject line or title and group attachments with email by name..maybe just append ATTACH to the end of the PDF name) ------ n8n via AI will identify the content category (podcast, email, website) ------ n8n via AI will identify subject matter category (AI, Education, Management, EdTech, etc.) ------ n8n will drop the PDFs, links, etc. into a Google Drive folder that matches the subject matter category - n8n will create a Notion database entry with the content category, subject matter category, and a link to the Google Drive file. BONUS: - n8n (or maybe Notion web hooks?) will keep the Google Drive in sync with the Notion database. If I receive an article that's junk, I can delete it in Notion and it will purge it from Google drive. - n8n (or maybe a Notion web hook?) will keep move the Google Drive file if I change the subject matter category (i.e., AI categorized the email as AI, but it was really EdTech). ****** Is n8n the right tool for this? Is Notion the right tool for the \"database/UI layer\" here? I've seen some similar workflows on n8n that send you an email summary.  The thing is, I want to be able to consume my data when I want (in Notion) vs. just getting an email daily/weekly with summary. I want to be able to save the \"good stuff.\" All thoughts appreciated!!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring",
          "link": "https://old.reddit.com/r/n8n/comments/1ly90k3/input_needed/",
          "author": "Ekalov10",
          "time": "2025-07-12T19:34:04+00:00",
          "body_text": "Hi all, I'm trying to post to Instagram using n8n + Facebook Graph API and keep hitting this error during the media_publish step: vbnetCopyEditUnsupported post request. Object with ID '17841470935705668' does not exist, cannot be loaded due to missing permissions, or does not support this operation. Here‚Äôs my current flow: I have set up n8n locally (Docker). Facebook App is approved and working. Instagram account is a Business Account. It's linked to a Facebook Page (confirmed in Page settings). I get the Instagram Business Account ID via Graph Explorer. I can create media with: POST https://graph.facebook.com/v20.0/{ig-business-id}/media When I try to publish it via: POST https://graph.facebook.com/v20.0/{ig-business-id}/media_publish using the creation_id, it fails with the error above. Access token is generated using Graph API Explorer with full scopes: instagram_basic pages_show_list pages_read_engagement pages_manage_posts instagram_content_publish Any ideas what might be wrong? Do I need additional page/IG permissions? Is there something wrong with my IG account setup? Thanks in advance https://preview.redd.it/ll4yr6ssa1bf1.png?width=1315&format=png&auto=webp&s=911c69658ffcf2f3f1b9d40dcdfb269d19013fab https://preview.redd.it/2kiob85xa1bf1.png?width=1333&format=png&auto=webp&s=51f464fed8602ae75fcb75a1b4df5efea99a41e9"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help: I want to send a markdown text to a Notion page but I can't!",
          "link": "https://old.reddit.com/r/n8n/comments/1ls6yvr/cant_post_to_instagram_via_n8n_unsupported_post/",
          "author": "Specialist_Point3420",
          "time": "2025-07-05T10:39:34+00:00",
          "body_text": "If your Gmail inbox is a big mess, I set up a way to use AI to organize it for you (takes 10 mins) You can use this free n8n template I made to: Auto sort your emails by topic (like Orders, Support, Invoices, whatever you want) Add colored labels to stuff so you see what‚Äôs important fast Works with Gmail‚Äôs multiple inboxes (split your inbox into sections for different categories) How it works: You tell it your categories in a Google Sheet (edit it however you want) Connect your Gmail + a free OpenRouter AI key It reads new emails and puts labels on them No coding needed, just follow the setup Setup video & full walkthrough Free n8n template Hope it helps somebody. Also fancier features such as auto-replies for specific categories etc. could be added with some more nodes."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lvht0i/i_built_an_automation_to_autoorganize_my_inbox/",
          "author": "Milan_SmoothWorkAI",
          "time": "2025-07-09T12:42:44+00:00",
          "body_text": "Discover how to easily extend an AI-generated video from a single image using KAI.AI , VEO3, and automation tools like NCA Toolkit and FFmpeg. Learn how to automate video sequences, extract key frames, and create longer, more dynamic videos ‚Äî perfect for creative projects or content creation! Video: https://www.youtube.com/watch?v=qc00jJ7bCBA Workflow: https://pub-7710cf36e9f24295acffe6938f40f147.r2.dev/Extend%20Video%20with%20NCA%20Toolkit.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Seeking Advanced Automation Content: Moving Beyond Beginner Tutorials and \"Get Rich Quick\" Schemes",
          "link": "https://old.reddit.com/r/n8n/comments/1lxys38/how_to_extend_ai_generated_videos_from_images_in/",
          "author": "Silly_Chapter6498",
          "time": "2025-07-12T12:02:42+00:00",
          "body_text": "Every entrepreneur knows the struggle: finding a validated business idea.  Your ideal viewer wants to find that golden opportunity.  This guide is the step-by-step framework on how to build an automated machine that finds potential SaaS ideas directly from Reddit posts and delivers them to you. We need to separate you from the pack of people endlessly scrolling.  The goal of this workflow is to automatically find posts where users are describing a problem, and then use AI to brainstorm a solution.  Humans are emotional, and when they complain online, they're giving you business ideas. If you can find their problems, they will naturally gravitate toward your solution. Here are the actionable tips to build this yourself: Step 1: The Trigger - RSS Feed Start your n8n workflow with the \"RSS Feed Trigger\" node.\nFor the \"Feed URL,\" use the URL of any subreddit you want to monitor and just add .rss to the end. For example: https://www.reddit.com/r/smallbusiness/new/.rss Step 2: The Filter - Find the Pain Points Add an \"IF\" node after the trigger. This is the most important part.\nSet the conditions to filter the post title or content for keywords that indicate a problem. Think like a user who needs help.\nGood keywords to filter for include: \"how do I\", \"alternative to\", \"I wish there was\", \"annoying\", \"is there an app for\", \"tool to\".\nStep 3: The Brain - AI Analysis Connect an \"AI Agent\" or similar AI node to the 'true' output of the IF node.\nIn the prompt or instructions field, tell the AI what to do. For example: \"Read the following Reddit post. Summarize the user's core problem in one sentence. Then, suggest one potential SaaS idea that would solve this problem.\"\nStep 4: The Delivery - Get Your Ideas Add a final node to send the AI's output somewhere you'll see it.\nGood options are a \"Discord\" node to post in a private channel, a \"Slack\" node, or even an \"Airtable\" node to create a database of ideas.\nIf you can do this, you will have more SaaS ideas than you know what to do with.  You'll get a constant stream of problems that real people are asking to have solved. What are your favorite subreddits for finding business problems? Let me know in the comments!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lebpkb/from_reddit_complaint_to_saas_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-18T08:18:32+00:00",
          "body_text": "I automated a full AI-powered video pipeline that creates faceless vertical stories like the miniature worker / tiny character genre you‚Äôre seeing blow up right now. Why? Because writing, designing, animating, soundtracking, and publishing even one of these manually is a multi-hour grind - and I wanted to publish daily without touching CapCut. So I built an end-to-end workflow in n8n that does it all, 100% automated. Perfect for: AI-generated short stories & mini animations YouTube Shorts / Instagram Reels / TikTok Faceless vertical video channels (well you can adapt it for 16:9 as well - change the aspect ratio) Clients who want storytelling content with AI flair Follow along this tutorial here. Explanation here : https://youtu.be/Z1n6nU9O0BA Example video with the lowest settings https://youtube.com/shorts/LN46liFamoY üß† How It Works The system turns a scheduled trigger into a fully animated AI-generated story , complete with voiceover, visuals, and sound FX, and background music. Here‚Äôs the full 8-step pipeline: 1. Story Idea Generation GPT-4 creates original video ideas e.g., ‚ÄúTiny workers making a large pizza‚Äù 2. Scene Breakdown + Visual Planning Each story is split into multiple scenes with structured prompts and reference descriptions. 3. Reference + Scene Images via FAL / FreePik FAL‚Äôs image generation API creates high-quality scene visuals - one per segment, based on the story and setting. In the workflow you have option for using fal/general for using lora's and controlnets if you wish. Freepik integration is also there - Started with this, kept in the workflow (good images, but slow generation) - absolutely free image generation - with rate limiting of course. 4. Scene Animation via Kling  - or Minimax (ran out of minimax's $50 dollar credit so Swtiched to Kling via Fal) Each image is passed to an animation engine that gives movement, transitions, and subtle effects (like camera pan or zoom) for that short-form storytelling vibe. 5. Bacground music + SFX from ElevenLabs AI-generated voiceover narrates the story, matched with ambient music and sound effects, all created using ElevenLabs. 6. Video Compilation via FFmpeg A custom command stitches the scenes, SFX, and music together, handles crossfades, syncs timing, and applies volume balancing. No JSON2Video or Creatomate - they can become expensive. 7. YouTube Upload The finished video is uploaded directly to YouTube Shorts via resumable upload. Upload to reels and Tiktok (coming soon) 8. Logged in Google Sheets Each published video is logged for tracking, repurposing, or reuse. üîß Tools Used Function Tool Idea & Script OpenAI GPT-4 Images & Scenes FAL (Flux) with lora support Animation Hailuo API / Kling Voice + SFX ElevenLabs Stitching FFmpeg Automation n8n Publishing YouTube API Tracking Google Sheets ‚öôÔ∏è Key Features No video editing required. Fully modular, works scene-by-scene. Customize prompt style for any genre (horror, fantasy, sci-fi, slice of life). Built in n8n , no-code setup. Ideal for scaling faceless content at volume. AI is not perfect always, it will make mistakes üöÄ Why This Works Saves 10 - 15 hours/week Produces daily, high-quality, AI-first content Scalable for multiple channels or client brands Zero editing tools or manual uploads needed Great for storytelling niches, explainer content, or abstract visual art Explanation here: https://youtu.be/Z1n6nU9O0BA"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I had multiple places storing different ideas, wanted to make them more organized and remove similar ideas. Then I created this flow!",
          "link": "https://old.reddit.com/r/n8n/comments/1l37grx/i_built_an_ai_youtube_automation_machine_that/",
          "author": "hashpanak",
          "time": "2025-06-04T14:48:21+00:00",
          "body_text": "Hi everyone,\nI just started creating ai workflows. I am creating a workflow for social media content creation and want to be in the loop for approval of topic, script final product.\nIs there anyway to get human in loop without creating a whole new workflow.\nAs n8n only gives 5 workflows and will need atleast 4 workflows for only this automation.\nAny tips or tricks to either do it in lesser workflows or should human be in loop this many times. Any knowledge is appreciated."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lrhogl/getting_human_in_loop/",
          "author": "WritingOk4989",
          "time": "2025-07-04T12:35:56+00:00",
          "body_text": "‚öôÔ∏è What it does: Checks which products are running low every midnight Calculates dynamic reorder quantity so you don‚Äôt run out again soon Groups orders by supplier and sends a clean, detailed PO email Updates the PO status in Airtable after the email is sent üß© Tech stack: n8n for orchestration Airtable for products, suppliers & PO tables Brevo (SendInBlue) for emailing suppliers It‚Äôs fully automated and can save hours every week + reduce human errors. If anyone‚Äôs curious, I‚Äôm happy to walk through the steps or share how I built it! üöÄ #n8n #automation #inventory #airtable #nocode #workflow #buildinpublic"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "This AI Workflow Reads Daily AI News, Summarizes It & Sends a LinkedIn Post to My Inbox Automatically ‚Äì Here‚Äôs How It Works!",
          "link": "https://old.reddit.com/r/n8n/comments/1lvi908/i_built_a_smart_workflow_to_automate_product/",
          "author": "Charming_You_8285",
          "time": "2025-07-09T13:03:34+00:00",
          "body_text": "I am using firecrawl to scrape websites. In this case, three of them: https://techcrunch.com/tag/semiconductor/feed/ https://semiengineering.com/feed/ https://www.semiconductor-today.com/rss/news.xml My problem is that my firecrawl gives a lot of unncessary data, and it does not get cleaned up even well enough in my clean up code node. I would really appreciate any help as I have been struggling and going in circles for the last 4 hours... I use a loop over items node to limit the batch size to 5 as I kept getting errors on firecrawl. loop over items (loop) -> firecrawl (success) -> wait (2s) -> loop loop over items (done) -> clean up node -> supabase This is my firecrawl body {\n  \"url\": \"{{ $json.URL }}\",\n  \"formats\": [\"json\", \"markdown\"],\n  \"excludeTags\": [\"iframe\", \"nav\", \"header\", \"footer\"],\n  \"onlyMainContent\": true,\n  \"jsonOptions\": {\n    \"prompt\": \"Identify the main content of the text (i.e., the article or newsletter body). Provide the exact text for that main content verbatim, without summarizing or rewriting any part of it. Exclude all non-essential elements such as banners, headers, footers, calls to action, ads, or purely navigational text. Format this output as markdown using appropriate '#' characters as heading levels. Exclude any promotional or sponsored content on your output.\",\n    \"schema\": {\n      \"type\": \"string\",\n      \"description\": \"The exact verbatim main text content of the web page in markdown format.\"\n    }\n  }\n} This is my clean up node function cleanSemiWikiContent(content) {\n  if (!content) return '';\n\n  return content\n    // Remove WordPress debugging code\n    .replace(/``````/g, '')\n    .replace(/``````/g, '')\n    .replace(/``````/g, '')\n\n    // Remove navigation and UI elements\n    .replace(/\\[Register\\][^\\n]*/g, '')\n    .replace(/\\[Log In\\][^\\n]*/g, '')\n    .replace(/Select Language[^\\n]*/g, '')\n    .replace(/Powered by.*?Translate/g, '')\n\n    // Remove social media and sharing\n    .replace(/\\[Share on[^\\]]*\\]/g, '')\n    .replace(/\\[Semiwiki on[^\\]]*\\]/g, '')\n\n    // Remove promotional content\n    .replace(/Guests have limited access[\\s\\S]*?today!/g, '')\n    .replace(/You are currently viewing SemiWiki[\\s\\S]*?today!/g, '')\n\n    // Clean up extra whitespace\n    .replace(/\\n{3,}/g, '\\n\\n')\n    .trim();\n}\n\nconst items = [];\n\nfor (const item of $input.all()) {\n  if (item.json.success && item.json.data) {\n    const data = item.json.data;\n\n    // Use markdown content and clean it\n    const rawContent = data.markdown || '';\n    const content = cleanSemiWikiContent(rawContent);\n\n    if (content && content.trim().length > 0) {\n      items.push({\n        json: {\n          content: content,\n          metadata: {\n            url: data.metadata?.url || 'unknown',\n            title: data.metadata?.title || 'Untitled',\n            source: 'semiwiki'\n          }\n        }\n      });\n    }\n  }\n}\n\nreturn items;"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m2qiys/firecrawl_giving_a_lot_of_unncessary_data/",
          "author": "PreparationHot4256",
          "time": "2025-07-18T02:25:28+00:00",
          "body_text": "I‚Äôm designing a workflow using n8n to streamline the creation of YouTube Shorts ‚Äîthink videos under 60 seconds. I‚Äôm hoping to find a template or a starter workflow that: Fetches video content ‚Äì via RSS, YouTube API, or other sources Processes clips ‚Äì trimming, adding subtitles or overlays Exports formatted videos ready for upload Automates upload to YouTube (ideally through the YouTube API) Optionally posts to Instagram Reels or TikTok If you‚Äôve built something similar‚Äîor found a community template, GitHub repo, or n8n workflow snippet‚ÄîI'd really appreciate it! üôè A few specifics I‚Äôm working with: Source content: YouTube searches, RSS feeds, or a local media directory Light editing: Automated trimming or captioning via AI Output format: Vertical 9:16 video, max 60 sec Upload: Automatically publish to a YouTube channel (and/or other platforms) Performance and error handling matter too‚Äîif you‚Äôve tackled retries, logging, or dynamic scheduling, that‚Äôd be a big bonus. Thanks in advance for any pointers, example workflows, or tool suggestions!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Interacting with an N8N AI Agent using OpenAIs Realtime API",
          "link": "https://old.reddit.com/r/n8n/comments/1lbnq8i/looking_for_an_n8n_youtube_short_creation_template/",
          "author": "RomelloMorrisdet",
          "time": "2025-06-15T00:47:36+00:00",
          "body_text": "Currently trying to build a workflow that helps me write content. I need to know if it is possible to run 2 parallel workflow paths from a single trigger that run separate http requests nodes -> extract from file nodes -> separate code nodes to process the data before merging back into the final http requests that runs my claude writer. The first path pulls a content topic list from my GitHub repo while the 2nd path pulls in an analytic databse from Dropbox. The goal is to be able to provide both sources of information to my claude node to give it all the info it needs to write the content I need. So far, I've tried:\n- running 2 separate triggers: only one path runs\n- running parallel with 1 trigger: only one path runs\n- running them sequentially: data from the first source gets overwritten I've looked to see if anyone runs something similar in their workflows but I'm not seeing any close to what I'm trying to solve. Any suggestions or solutions would be incredibly helpful and really appreciated."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lp6608/can_you_run_http_requests_parallel_to_pull_in/",
          "author": "phard003",
          "time": "2025-07-01T16:26:56+00:00",
          "body_text": "I'm having an issue with my Human-in-the-Loop AI editing pipeline in n8n and would appreciate any insights. Context The workflow is designed to support human feedback in an AI-assisted editing system: A form submission triggers an AI agent to process certain set of information. The AI sends the result to Telegram for human feedback. The feedback is received and processed by an EditApplicator agent, which implements the changes. The updated content is sent to a Code node ( OutputParser2 ) that parses it into two values: key : title of information value : body paragraph (the bulk of the information) At this point, I want to send the edited info back to Telegram for secondary (or further) human approval. The Issue After OutputParser2 , I route the data to a Loop Over Items node. Even when it receives a single item, I expect the node to pass it through as part of the loop. However, the node sends the edited info to its 'done' branch, and the workflow ends at this step with the message: \"Workflow executed successfully\" No loop iteration occurs, and the item is not forwarded to the Telegram node. What I‚Äôve Tried Confirmed that OutputParser2 outputs one or more items in proper format (key/value). Attempted to add a Merge node before Loop Over Items to force the loop, but it interfered with how the EditApplicator agent receives its inputs, returning \"can't determine which item to use\" for each input (image #4). When I bypass Loop Over Items , the workflow completes as expected‚Äîbut I need this node for future support of multiple items. Question How can I configure Loop Over Items to reliably process even a single item, rather than immediately exiting through the done output? Ideally, I‚Äôd like a solution that works for both one and multiple edited terms, without introducing issues elsewhere in the flow. Happy to provide screenshots or node configurations if helpful."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Invoice Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1luygd4/loop_over_items_node_ends_workflow_prematurely/",
          "author": "ThisIsSerira",
          "time": "2025-07-08T19:51:54+00:00",
          "body_text": null
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Create Powerful AI Videos with Veo 3 Fast in n8n ‚Äì No KYC or Org Verification Needed!",
          "link": "https://old.reddit.com/r/n8n/comments/1m1octi/create_powerful_ai_videos_with_veo_3_fast_in_n8n/",
          "author": "automayweather",
          "time": "2025-07-16T20:57:43+00:00",
          "body_text": "Just built a seamless automation using n8n and the Veo 3 Fast API from Ulazai.com. Automatically generates AI videos in seconds Cheapest pricing available for Veo 3 Fast (just $1 per video!) No KYC, instant API access Supports the new vertical (9:16) aspect ratio Bonus: use OpenAI‚Äôs image generation without organizational verification‚Äîespecially helpful now after OpenAI‚Äôs recent model update! ulazai.com - VEO 3 Fast Ideal for automation enthusiasts, marketers, and developers looking to leverage cutting-edge AI content creation without complex setup. üëâ Quick-start guide & API docs + Workflow**:** https://ulazai.com download the workflow: https://gist.github.com/smrht/82c072241a679ae827421a1c61fd35b9"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lxjzgz/build_a_youtube_trend_alert_system_in_n8n_using/",
          "author": "Aigenticbros",
          "time": "2025-07-11T22:12:42+00:00",
          "body_text": "Hey everyone! üëã Just wanted to share a YouTube trend alert system I built using n8n, OpenRouter/OpenAI, and the News API. This is perfect if you're a content creator stuck in a rut and want AI-generated video ideas based on trending articles. Here‚Äôs the full breakdown of the flow (screenshot attached): üîÅ The Flow 1. Manual Trigger Starts with a simple ‚Äúclick to test‚Äù trigger so you can run it on demand (but you could schedule this weekly too). 2. Code Node Grabs the current date dynamically so the API fetches the most recent articles. 3. HTTP Request (News API) Calls the News API using the everything endpoint. You can tweak by keyword (e.g. ‚ÄúChatGPT‚Äù), sources, domains, language, sort order, etc. Free tier includes 1,000 requests/month. 4. Split Out Node Takes the JSON array of articles and splits them into individual items‚Äîeasier for processing downstream. 5. Edit Fields Node Strips each article down to only the title and description (no need to overload the agent with full content). ü§ñ AI Agent 1: Content Relevance Filter This AI Agent acts as a content curator . System message: It‚Äôs framed as an expert content strategist. It analyzes each article and decides if it's relevant to a specific YouTube channel (I used AIGentic Bros as an example). Output format: Only returns results in a clean JSON if relevant (e.g. { title: \"...\", description: \"...\", status: \"relevant\" } ). If not relevant, it returns a simple message saying so. The criteria it uses: Novelty & timeliness Audience alignment Practical value No politics or generic news üîÄ IF Node Checks if the previous output contains \"relevant\" ‚Äî if not, it halts the flow. If it is, it passes the article to the next stage. ü§ñ AI Agent 2: Video Ideas Generator This agent takes the relevant articles and transforms them into video ideas + YouTube titles . Example Output: Video Idea: Create a tutorial on OpenAI‚Äôs new AI browser showcasing integration with popular tools. Title Suggestion: ‚ÄúUnlock the Future: Supercharge Your Workflow with OpenAI‚Äôs New Browser‚Äù You can improve this by training the agent on your own title format/style. üß† Model Stack Used 40-mini for the first agent (relevance filtering) Swapped models where latency or timeout issues happened üí° Potential Extensions Schedule this to run weekly and email yourself the output via Gmail node Hook it into your Notion/Slack as a content planning pipeline Add keyword research tools to evaluate SEO potential Train a custom agent on your personal video ideation process üéÅ Final Thoughts If you're building content with AI + automation, this kind of flow can save hours and keep your ideas fresh. Feel free to clone and customize it for your niche. Let me know what you‚Äôd improve or add to this system. Happy building!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "üö® Build a YouTube Trend Alert System in n8n Using News API + AI Agents (Full Breakdown)",
          "link": "https://old.reddit.com/r/n8n/comments/1lg5y1e/i_built_a_fully_automated_wordpress_content/",
          "author": "Awkward_Activity_426",
          "time": "2025-06-20T14:51:42+00:00",
          "body_text": "Tired of writing and formatting blog posts manually? I was too ‚Äî so I built a full content automation pipeline using just: ‚öôÔ∏è n8n (self-hosted, no-code) ü§ñ OpenAI (for SEO-friendly blog generation) üåê WordPress API (auto-publish to live site) How it works: 1Ô∏è‚É£ I input only the blog title 2Ô∏è‚É£ AI writes a full 1000-word post (with SEO structure) 3Ô∏è‚É£ n8n pushes it live on WordPress ‚Äî title, content, slug, everything. Bonus: I even optimized it to pass Yoast SEO checks (meta desc, H1/H2, links, etc.) This setup is üî• for: Bloggers who hate writing Agencies who need to scale content Solo founders building fast Post Example: https://theudaipurstore.com/top-5-places-to-visit-in-udaipur-2/ https://preview.redd.it/zrmeb4xai38f1.png?width=3024&format=png&auto=webp&s=5b284701bd7640af2fcbe08a82300faadab367ec Want the workflow JSON or a breakdown tutorial? üëá Drop a UPVOTE and DM me"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m738vf/built_my_first_advanced_flow_help_me_simplify_it/",
          "author": "zierbeek",
          "time": "2025-07-23T07:55:01+00:00",
          "body_text": "This n8n workflow is one of my first more 'complex' workflows for my father in law. This manages a rental booking system (just a hobby, not business critical) . It handles the entire lifecycle of a rental, from initial booking and validation to managing pickups, returns, and cancellations. https://preview.redd.it/8y63rmz0ykef1.png?width=1692&format=png&auto=webp&s=8e875b7866f659fed213e4e445c176c5d75ad1de Workflow Description The workflow can be broken down into two main parts: New Booking Creation:Validation & Pricing: This is a core JavaScript code node that processes incoming booking requests. It performs several critical functions: Input Validation: Checks if essential form data, such as start and end dates, are present. Date Validation: Ensures the end date is after the start date. Availability Check: Verifies the availability of items (e.g., 'Daktent', 'Camper', 'Stertent', 'Drone') based on pre-defined quantities and existing bookings, preventing double-bookings for the same period. Price Calculation: Calculates the total rental price, taking into account: Duration: Number of nights and weeks. Base Prices: Item-specific base prices (e.g., Camper: ‚Ç¨95, Daktent: ‚Ç¨120). High Season Surcharge: Applies a 30% surcharge for rentals during July, August, and from December 20th to January 6th. Special Pricing: Implements specific pricing rules for \"Drone\" (‚Ç¨40 first day, ‚Ç¨15/day thereafter) and \"Daktent\" (tiered weekly pricing starting at ‚Ç¨120 for the first week). Discount & Financials: Applies a percentage discount if provided and calculates the total amount, deposit, advance payment, and remaining balance (saldo). Booking ID Generation: Creates a unique booking ID (e.g., \"TENT-240723-ABCD\"). Error Handling: Outputs an isError flag and an errorMessage if any validation or processing issues occur. Create Calendar Event: After successful validation, this node creates an event in a Google Calendar with the booking details, including the booking ID, rented item, renter's address, and a breakdown of the rental costs. Append to Sheet: This node adds the complete booking information to a Google Sheet named \"Verhuur sheet\" (Rental Sheet), with the initial status set to \"Gepland\" (Planned). Existing Booking Management (Pickup, Return, Cancellation): On form submission2 & Switch: This section is triggered by a separate form where users can select an action: \"Ophalen\" (Pickup), \"Terugbrengen\" (Return), or \"Annuleren\" (Cancel). The \"Switch\" node then directs the workflow to the appropriate branch based on the selected action. Pickup Flow (Ophalen): Get row(s) in sheet & Code1: Retrieves planned bookings from the Google Sheet and dynamically generates a form ( Form1 ) allowing the user to select a booking. This form collects additional pickup details like start kilometers, departure/return times, driver information (for two drivers), cleaning status, bank account number, and any damage or general remarks. Code2 & Update row in sheet: Extracts the selected booking ID and updates the Google Sheet, changing the booking status to \"onderweg\" (en route) and recording the collected pickup details. Cancellation Flow (Annuleren): Get row(s) in sheet2 & Code4: Retrieves planned bookings and generates a form ( Form4 ) to select a booking and provide a cancellation reason. An Update node (likely Update a document4 based on typical n8n connections) would then update the booking status to \"Cancelled\" in the Google Sheet and add the cancellation reason. Return Flow (Terugbrengen): Get row(s) in sheet3 & Form17: Retrieves bookings with \"onderweg\" status and likely presents a form to collect return details such as end kilometers and return notes. An Update node (likely Update a document1 ) would then update the booking status to \"Teruggebracht\" (Returned) or \"Completed\" in the Google Sheet."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built a fully automated WordPress content engine using n8n ‚Äî Just drop a title and it writes + publishes the blog!",
          "link": "https://old.reddit.com/r/n8n/comments/1m0so2m/built_a_custom_makecom_app_for_my_tool_bookoly/",
          "author": "abaumtro",
          "time": "2025-07-15T20:22:00+00:00",
          "body_text": "Hey everyone, I recently built a custom app in Make.com for my tool Bookoly ‚Äì it automates content creation like videos, voice-overs, subtitles, and more. Now I want to bring the same functionality to n8n by building custom nodes that allow users to integrate Bookoly into their workflows. Since this is my first time developing for the platform, I‚Äôd love some input from those with experience. Here‚Äôs what I‚Äôm trying to figure out:\n    ‚Ä¢ What‚Äôs the best starting point for building a custom node in n8n?\n    ‚Ä¢ Are there any boilerplates, templates, or best practices I should follow?\n    ‚Ä¢ Can I build and test nodes locally before publishing?\n    ‚Ä¢ What‚Äôs the process to publish a custom node so it‚Äôs publicly available for all n8n users?\n    ‚Ä¢ Any common pitfalls or things I should be aware of? If you‚Äôve built or published nodes for n8n before, I‚Äôd really appreciate your advice, resources, or links to examples. Thanks in advance!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1m6fz4q/need_inspiration_for_n8n_use_cases/",
          "author": "TheWahdee",
          "time": "2025-07-22T14:44:17+00:00",
          "body_text": "TL;DR I need ideas/inspiration for solid, production-quality use cases for n8n (they don't have to necessarily be big and complex) to find ways for using n8n at the software company I work for, or for our clients. Automation experts, please share some ways in which you've been using n8n for automation solutions, big or small! I work for a small software development company and would like to find some ways to start using n8n for internal use and/or providing automation services to clients. Our company focuses on making custom software solutions for clients, primarily focused on \"fintech\" and \"administrative software\" (e.g. a project I recently worked on was a custom timesheet website for employees and supervisor to log and track hours for projects). You'd think in this context there would be some obvious ways to find usage for n8n, but this has not been the case. The idea of using n8n has already been brought up at my company but neither myself nor any of my colleagues (including those that have been working at the company much longer than me and are more aware of the exact needs/desires of our clients) have clear ideas for how we might start using n8n ourselves or for our clients. I personally would like to push further at my company to find uses for n8n because I see how it can be a very effective tool in various ways, but I need some inspiration to present clear, solid ideas of how we can start implementing solutions with n8n. It doesn't have be truly \"production-level\" workflows that can scale to thousands of users, but at the same time it shouldn't be \"youtube content generation\". Robust use cases with n8n agents + tool calling would still be fine as I'm sure there are clients that would jump at the opportunity to integrate AI into their business in some way. Any members in this community with real, practical experience using n8n willing to share some wisdom and provide inspiration for myself and others that might be in a similar situation?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Built my first advanced flow! Help me simplify it?",
          "link": "https://old.reddit.com/r/n8n/comments/1i66chs/i_used_ai_and_n8n_to_automate_youtube_shorts/",
          "author": "Horizon-Dev",
          "time": "2025-01-21T00:54:02+00:00",
          "body_text": "Hey everyone! I've been experimenting with n8n and AI automation to streamline the process of creating YouTube Shorts , and I wanted to share my results. In this video, I walk through how I: ‚úÖ Used n8n to automate the entire workflow ‚úÖ Integrated AI tools to generate content dynamically ‚úÖ Automated script creation ‚úÖ Grabbed Videos to clip together ‚úÖ Generated voice, and grabbed music ‚úÖ Optimized the system to make the process scalable If you guys have questions, or would do something differently let me know! üîó Watch the full breakdown here: https://youtu.be/g3XsT_55xTA"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lzk9s2/api_returns_many_results_per_input_will_this/",
          "author": "SkinnyCheff",
          "time": "2025-07-14T11:48:16+00:00",
          "body_text": "https://preview.redd.it/x6piig8putcf1.png?width=1284&format=png&auto=webp&s=bac95bb70b4a9e5f0d9d3c98c05cabfbb28d574f The API for collecting linkedin posts from a url returns many post outputs instead of returning posts under each input. I added code to assign posts based on the url that posted. Will n8n know to match to the original url since it's the same? Will this workflow have troubles assigining the posts back to the original inputs? Is there a better way to do this? The code in my code node: const grouped = {}; for (const item of $input.all()) { const profile = item.json.profile_input;  // Must be set from the original input const hasContent = item.json?.posted_at?.date && item.json?.text?.trim(); if (!profile || !hasContent) continue; if (!grouped[profile]) { grouped[profile] = []; } grouped[profile].push(item); } const output = []; for (const [profile, items] of Object.entries(grouped)) { const postPairs = items.map((post, idx) => `Post ${idx + 1}\\n${post.json.posted_at.date}\\n${post.json.text.trim()}` ); const combined = postPairs.join('\\n\\n'); const firstDate = items[0]?.json.posted_at?.date || ''; output.push({ json: { profile_input: profile, all_posts: combined, first_date: firstDate, } }); } return output;"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Built a custom Make.com app for my tool Bookoly ‚Äì now I want to create n8n nodes.",
          "link": "https://old.reddit.com/r/n8n/comments/1ltucmt/autopost_about_trending_topics_with_your_ai_clone/",
          "author": "ChaseAI",
          "time": "2025-07-07T13:36:05+00:00",
          "body_text": "This workflow uses your AI clone from HeyGen to automatically post short form videos about trending topics in your niche across multiple social media platforms. Video walkthrough here. Once the niche is given, research is done everyday via Tavily. The research is passed off to your LLM of choice which writes the script in whatever sort of voice / tone you want, and then that script is given to HeyGen, which generates a video of your AI avatar reading the script. From there it is posted on whatever socials you want via Blotato. Setting up your avatar on HeyGen is surprisingly painless. Their pricing structure can be a little tricky though-- while you can get 10 video credits (10 minutes worth of video) on their free API plan, the next tier up will run you $99 / m. Sort of annoying that there is no mid tier between those two points but it is what it is. Tavily gives you hundreds of searches for free every month so no cost there. LLM API costs will also be negligible. Posting to socials is done via Blotato here, which runs $29 / m. Alternative options include upload-post at a slightly lower price point. More than anything, this workflow is a blueprint for you to leverage HeyGen AI avatars in the social media space. While this automation uses trending topics in your niche as a use case, that can really be swapped out for anything you deem relevant. As the AI clone / avatar tech gets better and better, you're going to be seeing a lot of this-- for better or worse."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1llur3e/need_help_building_an_n8n_workflow_to_analyze/",
          "author": "Massive-Land-9664",
          "time": "2025-06-27T14:05:18+00:00",
          "body_text": "Hey folks! üëã I‚Äôm working on an n8n workflow and could really use your input on how to structure it effectively. Here‚Äôs what I‚Äôm trying to do: üîÑ Workflow Overview: Input Data An Excel sheet containing a list of website URLs. Website Analysis For each URL, I want to evaluate certain parameters like: Is the website mobile-friendly? Does it look modern or outdated? Does it include animations or interactive design elements? Issue Detection Based on the analysis, identify and list issues for each website (e.g., poor mobile support, old design, etc.) Output Automatically send a personalized email to each website owner with a summary of the findings and possible improvement suggestions. ‚ùì Questions: How can I effectively set this up in n8n ? Any recommended nodes or plugins for performing the actual website analysis? Tips for generating personalized email content based on dynamic analysis results? If you‚Äôve built anything similar or have tips on structuring a flow like this, I‚Äôd love to hear your thoughts!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need inspiration for n8n use cases",
          "link": "https://old.reddit.com/r/n8n/comments/1lgj0f7/is_ai_about_to_become_the_most_groundbreaking/",
          "author": "automayweather",
          "time": "2025-06-21T00:03:30+00:00",
          "body_text": "Will AI Outrank the Wheel, Electricity, or the Internet as THE Biggest Productivity Multiplier in History? This is a question that honestly keeps me up at night: 1,000 years from now, when historians are ranking humanity‚Äôs greatest productivity-boosting inventions, will AI take the #1 spot on the list? That‚Äôs above the Agricultural Revolution, the printing press, the steam engine, electricity, the internet‚Äîall of it. I get that we have serious recency bias right now. It‚Äôs easy to look at AI and say, ‚Äúnothing has ever moved faster or changed more at once.‚Äù But do we really think it‚Äôs going to surpass things like:\n- The first time a human tamed wheat and started farming (which led to‚Ä¶ civilization itself)\n- The transition to mechanized labor during the Industrial Revolution (steam engines, factories, mass production, etc)\n- Electricity lighting up the world, and then the internet connecting it But here‚Äôs the thing that makes AI different:\n- AI is fundamentally about building better itself ‚Äîrecursive self-improvement. None of those previous inventions could jumpstart their own progress.\n- The ‚ÄúAI package‚Äù doesn‚Äôt just automate what we already do; it may accelerate all forms of creation‚Äîsoftware, art, science, even invention itself.\n- We‚Äôre already starting to see glimpses: AI coding assistants let you build software from pure thought , removing tons of barriers to creativity. Today, you still need to prompt and tweak, but this is the worst it‚Äôll ever be. The slope is only up from here. Imagine what happens when billions of people‚Äînot just the elite few‚Äîcan turn their ideas directly into useful tools, content, companies, or art. The democratization of creation, not just consumption or simple productivity hacks. But it makes a lot of people nervous:\n- What happens to manual work? To ‚Äúcreative‚Äù work?\n- Will we value human creators more, or will AI-generated podcasts, videos, novels, etc. just flood everything?\n- Will ‚Äúpremium experiences‚Äù still be, say, a live Magnus Carlsen chess game‚Ä¶ or will people happily just consume robot-on-robot? Personally, I don‚Äôt think AI will replace the human part of work/creation/emotion‚Äîat least not in the ways that matter to us. Like, watching Messi dribble will always feel different than watching a robot. But for pure information, content discovery, even art for function‚Äôs sake? AI will rule. And if you look at what YouTube and the internet did for creation and voice, AI blows that up by another order of magnitude. So‚Äîif you‚Äôre not limited by technical skill, if the ‚Äútranslation layer‚Äù from idea to execution goes to zero, what second- and third-order effects happen? New business models? New forms of governance? Whole new creative genres? Entirely new forms of society? We know from the first Agricultural Revolution that the side effects may end up being more revolutionary than the core tech. What do you think‚Äîare we too hyped about AI, or is it legitimately the start of the greatest multiplier of all time? If not AI, what holds the top spot in your view? And what do you predict will be the weirdest, most unexpected ‚Äúside effect‚Äù of AI‚Äôs rollout‚Äîyour version of pottery and early governments?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lpcklm/no_code_gtm_strategy_agent_perplexity_redditx/",
          "author": "Sam_Tech1",
          "time": "2025-07-01T20:31:15+00:00",
          "body_text": "Go to YC page and you will see that 500+ AI Startups have already been funded this year. But if we go by data, 99% of them will fail not because of bad products but because someone else told the story better. Wanted to build something to solve the 1st stage of this problem with Agents so built an N8N automation which builds a comprehensive GTM plan for a company. Here is the workflow: Takes a company URL Uses Perplexity to analyze model, positioning, keywords Scrapes Reddit + X for live user opinions and reviews Feeds it all to GPT-4o to generate: ‚Üí GTM strategy ‚Üí Messaging angles ‚Üí Differentiation map ‚Üí Sample content calendar ‚Üí Accounts & subreddits to watch Emails a clean report daily. No noise. Just actions. Looks basic, but very strong starting point. Do it for yourself and your competitors. Step by Step breakdown in first comment. Check out."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Used AI and n8n to Automate YouTube Shorts ‚Äì Here‚Äôs What Happened üöÄ",
          "link": "https://old.reddit.com/r/n8n/comments/1lsyyuy/how_to_parse_the_http_request_efficiently/",
          "author": "SpecialAppearance229",
          "time": "2025-07-06T11:27:17+00:00",
          "body_text": "https://preview.redd.it/sbs1uqxun8bf1.png?width=1486&format=png&auto=webp&s=a8e44ff00e4cdbfd4eebd55c51355ae676da09ab Folks, I am trying to create a automatic tagging and summarizing for the links i save to the notion! However, sometimes, the http request has too many unnecessary content sending the entire text without filtering too big for llms and increases cost!! Is there any reliable way of extracting the main content alone? The output from http request is a json with data key and the value is the entire html!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "API returns many results per input, will this unlink from original input?",
          "link": "https://old.reddit.com/r/n8n/comments/1lz4aj9/5_n8n_workflows_that_run_my_business_while_i_sleep/",
          "author": "tusharmangla1120",
          "time": "2025-07-13T21:22:51+00:00",
          "body_text": "Meeting Transcript ‚Üí Business Proposal ‚Üí Auto-records meeting ‚Üí Extracts key details with AI ‚Üí Generates a professional proposal in minutes. Automatic Venue Booking System ‚Üí User inquiry ‚Üí checks availability ‚Üí Books the venue ‚Üí Sends confirmation & calendar invite. Invoice Pictures ‚Üí Database ‚Üí Upload an invoice image ‚Üí Extracts data with OCR ‚Üí Auto-fills and saves to database. SEO Blog Writer ‚Üí Picks trending topics ‚Üí Uses AI to draft posts ‚Üí Adds internal links + publishes on WordPress. Instagram Influencer Scraper ‚Üí Scrapes profiles by niche ‚Üí Filters by engagement + follower count ‚Üí Sends outreach messages. These aren‚Äôt just time-savers ‚Äî they replace full-time roles. No code. No dev team. Just plug & play with n8n. https://preview.redd.it/jdz3t19alpcf1.jpg?width=1600&format=pjpg&auto=webp&s=a9c734021bfc273ee86b734d225b03c93ab6c85c"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1l1bzxh/build_or_buy_for_workflows/",
          "author": "Kuryo193",
          "time": "2025-06-02T07:39:07+00:00",
          "body_text": "*Build using no code Does anyone have experience building their own SEO agents with a POV on this? Specifically looking to: scrape SERP to identify competitors of keywords and meta data, scrape from websites internal structure, headers, product titles/price/description gap analysis of product range, internal structure combine supplier commercial files in various formats and check products to list for margin recommend products to list, internal structure brief, write, edit, upload to shopify content"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Auto-Post About Trending Topics With Your AI Clone Across Any Social Media Platform",
          "link": "https://old.reddit.com/r/n8n/comments/1m37f9o/leadgen_qualifier/",
          "author": "Remarkable-Way-9604",
          "time": "2025-07-18T16:48:16+00:00",
          "body_text": "Done this half a month ago, it needs some adjusment. What do you think? I've done for a trigger a form submission, that's the first change that needs to be done Description : üö© Problem It Solves: Most businesses waste hours manually reviewing leads, chasing unqualified contacts, or responding to people who aren‚Äôt a fit. It's slow, inconsistent, and often leads to missed opportunities or bloated pipelines. ‚öôÔ∏è What It Does: The Leadgen Qualifier is an AI-powered automation that takes leads from your intake form and instantly: üß† Enriches the lead using real-time data (via LinkedIn, Google, business directories, etc.) üìä Qualifies them against your custom criteria (industry, size, budget, intent) üéØ Scores the lead as Hot, Warm, or Cold üóÇÔ∏è Routes the data to your CRM, Airtable, or email‚Äîonly the right leads reach you. ‚úÖ What You Get: Instant clarity on whether a lead is worth your time A cleaner, smarter pipeline without the manual filtering More time to focus on closing , not just sorting üí∏ Who It‚Äôs For: B2B service providers Coaches & consultants Agencies who get lots of form responses or cold traffic If you're using ads or content to drive leads, this automation helps you convert more by focusing only on the best ones. ‚è±Ô∏è The Result: Save 2‚Äì4 hours a day . Book better-fit calls . Let the AI do the boring qualification work ‚Äî so you can sell, serve, and scale."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m3ovqa/aipowered_wol_agents_for_green_it_free/",
          "author": "Substantial-Move5927",
          "time": "2025-07-19T06:04:27+00:00",
          "body_text": "AI Agents like n8n, VS Code and any other MCP-compatible tool can now tap into AutoShutdownManager‚Äôs robust Wake-On-LAN infrastructure‚Äîincluding its WOL proxies‚Äîto wake powered-off or sleeping clients across IP segments, VLANs, campuses, cities, countries or even continents. Configure in seconds via HTTP or stdio, integrate directly into your automation workflows, and enjoy zero-touch, global remote wake-ups without VPNs or firewall headaches. Automatically wake servers and clients  just before needed, for example for nightly backup windows to ensure reliable, on-schedule backups of critical data and applications. Try AutoShutdownManager MCP Server today and give your AI agents the power to wake machines anywhere on the planet‚Äîand keep your workflows running smoothly. More: https://www.enviprot.com/en/faqs/mcp-server-for-ai-workflows"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need Help Building an n8n Workflow to Analyze Website URLs & Send Personalized Emails",
          "link": "https://old.reddit.com/r/n8n/comments/1l1plpr/struggling_with_email_classification_via_mcp_and/",
          "author": "dannewestis",
          "time": "2025-06-02T18:26:42+00:00",
          "body_text": "Hey, I‚Äôm trying to set up a workflow in n8n to classify Gmail emails using an AI model (via OpenRouter), then apply labels and trigger further actions like creating tasks in Todoist. But I'm stuck and could really use a simple working example. Here‚Äôs my current setup: Workflow A (Client) Schedule Trigger (runs every 30 min) AI Agent using OpenRouter (chat model) MCP Client Tool (passes the prompt and context to the server) Workflow B (Server) MCP Server Trigger, with these tools: Gmail ‚Ä∫ GetAll Messages ‚Äì to list emails (e.g., from INBOX, unprocessed) Gmail ‚Ä∫ Get Message ‚Äì to fetch full email content Gmail ‚Ä∫ Add Labels ‚Äì to apply labels based on AI classification I do get a list of emails. But the handoff between tools (especially passing email IDs between nodes) seems to break. I suspect the problem lies in the prompt setup in the AI Agent and/or the tool descriptions in the MCP workflow . Prompt used in AI Agent: You are an email classification assistant. Your task is to process unread emails one by one using the following tools:\n\n1. List Emails  \n- Call: List Emails  \n- Params: {}  \n- Output: Nested JSON string under response[0].text  \n- Example output: [{\"id\":\"abc123\",\"subject\":\"...\"}]\n\n2. Get Email Content  \n- Call: Get Email Content  \n- Params: {\"emailId\": \"abc123\"}  \n- Must use exact ID from List Emails\n\n3. Apply Email Labels  \n- Call: Apply Email Labels  \n- Params: {\"emailId\": \"abc123\", \"labelIds\": [\"[label1]\", \"[label2]\"]}  \n- Labels must be passed as an array of strings\n\nProcess:\n- Step 1: Call List Emails\n- Step 2: Parse response[0].text as JSON\n- Step 3: For each email:\n  a. Extract \"id\"\n  b. Call Get Email Content with {\"emailId\": \"id\"}\n  c. Analyze the content\n  d. Call Apply Email Labels with chosen labels\n\nLabels (examples, I have instructions for when to apply each label):\n[work], [family], [personal]\n\nRules:\n- Always include \"emailId\" in tool calls\n- Process one email at a time (not batch)\n- Use only valid label strings (no typos) Has anyone used MCP to classify Gmail messages like this? I'd be super grateful for: A minimal working example (even just the structure) Tips on formatting the prompt or chaining the tools correctly Thanks in advance!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1m30ukq/using_n8n_for_computeheavy_and_complex_automation/",
          "author": "engineeraibuilder",
          "time": "2025-07-18T12:20:50+00:00",
          "body_text": "Hi, Lurker here, time to try to give back to the community, I hope you will benefit from this real world Rex. This is mainly intended for other devs new to n8n but everyone is welcome to read ofc. Context : I recently semi-automated (human in the loop for the final validation) a conformity analysis system for a client, using n8n. Sorry I can't name or give too many details and need to stay vague on the business specifics for confidentiality reasons, but the value will still be there. The business value for the client: analysts were spending days per project to analyze conformity of project documents against a set of requirements. To give an idea of scale, for each project : average of 500 conformity items that you need to categorize (compliant, not compliant, not applicable) and justify your decision with the relevant extracts from multiple documents making more than 1000 pages total. I changed it from days to 1.5 hours max, I was told client was delighted. The specifics that made this project challenging (and interesting/relatable for you guys I hope): I had no direct communication access to the final client for business reasons unrelated to me but that I can't disclose here, could only speak to intermediary firm that hired me but had to keep me in the shadows. The intermediary firm ‚ÄúAI architect‚Äù salesperson made the fundamental technical decisions (only n8n, and only the cloud hosting) without consulting me. ‚ÄúWe‚Äù then realized it was not the right tool for the job but the firm couldn‚Äôt go back on it to avoid losing face. We had to use n8n cloud for same reason. Even paying the best plan available before enterprise (the Pro-2 where you pay 144‚Ç¨/month for only 50k executions, 1280MiB RAM, 80 millicore CPU burstable) the performance is bad and your whole workspace crashes when ram limit is exceeded (cf below). Was new to n8n, not really an issue though. About me if you want: I am a cybersecurity engineer with work experience as a linux sysadmin, SRE (Kubernetes,AWS,GCP), SOC engineer, python and js dev, happened to also be a tech cofounder at a small startup and did diverse cybersecurity missions. I am passionate about automations and now like to help clients make it work for them too. The tech stack : n8n Supabase but as a pure postgres DB (not allowed to use edge functions) Airtable as the single point of user interface/interaction Openrouter (for the LLM calls) some paid API for pdf to markdown conversion Google Drive where all the pdfs were stored I will omit some details (useful for performance and cost reduction), but here is the high-level overview of implementation and process: In an airtable view, the user references the requirements table, a link to a gdrive folder, and clicks the ‚Äúcalculate conformity‚Äù button. An simple airtable js script makes some checks and then triggers the authenticated webhook to n8n. N8n gets all relevant data from airtable. All action below are now orchestrated by n8n Get all pdfs from gdrive, and compare their sha256 to the saved pdf-to-markdown values in DB, if it doesn‚Äôt yet exist, schedule them for conversion and store the result in DB. Each conformity criteria is matched to the relevant documents, and llms calls are made to get all relevant extracts for each criteria/markdown page. Llms hallucinate, a fuzzy verification is done to ensure the extract is a real one to keep only valid extracts Extracts are assigned a conformity value by another llm for matching criteria. Everything is packaged in a neat airtable view, each criteria having a conformity decision backed by extracts. Ready for the analyst to review and validate. Omitting some performance/interface points for simplicity and a little out of scope for this post: cache system, progress tracking, llm cost and performance analysis are a few of them. So why wasn‚Äôt n8n (cloud) the right compute tool for this use case in my opinion ? Frequent workspace crashes because OOM. Here are the main things I did to tackle it: Classical n8n optimizations: * split in multiple sub-workflows, with subworkflow returning minimum data to keep orchestrating workflow memory use low while each sequential subworkflow memory use is cleaned up by node when they finish.\n  * For batches, only process at the same time in subworkflows the workload what n8n cloud can handle without crashing. Performance will take a huge hit but you need to strech it in time. But this is not enough, I had to implement a n8n job progress tracker in DB. Each major step advancement is logged. A cron looks for unfinished unresponsive jobs to restart them after a crash. Cache system in DB to never do again an operation I already did ( criteria/doc matching, LLM calls, pdf conversion etc). This is based on sha256 values and stored in DB. So when I restart a workflow, it takes drastically less time to get where it crashed and continue. I would have done it anyway for performance and cost reasons Very low performance , 80mCPU doesn‚Äôt get you far fast. Calculating many sha256 for example. Code node limitations in n8n cloud. Cannot install your own libraries. Logging. Yes, you can (and I did) log information in an external repository, but this is much harder to do than in classic code. If I had more control over the situations here is what I would have done for this specific use case : Would have done the backend in code not n8n. I actually had a working solution in code done in less than 10% of the time it took me to build it in n8n afterwards. Incredibly more simple and maintainable than the 15 workflows in the final n8n system (and they heavily include code nodes). Not to mention performance/reliability. Did it in code first because that was the plan before sales people chose n8n for simplicity. If not, self-host n8n to get good performance for this kind of job if it is required like here. If you have to use n8n cloud, outsource some code processing to process tasks logged in DB. For example you could use supabase edge functions. Conclusion : I still have a high opinion of n8n, I think it shines for small, self-contained automations that do not need performance. And if you can give it to not very technical people that can be their own ‚Äúinternal customer\" it‚Äôs probably great. Even for a dev it can save you time for some tasks vs doing it in code, especially with its integrated connectors (to gdrive and others). But for a big task that requires a high level of ‚Äúorchestration‚Äù I think it's probably not the right tool (yet ?). What do you think ?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Is AI about to become the most groundbreaking productivity tool in human history, outpacing the wheel and the internet?",
          "link": "https://old.reddit.com/r/n8n/comments/1lq3glp/showoff_i_automated_trustpilot_outreach_review/",
          "author": "automayweather",
          "time": "2025-07-02T18:28:50+00:00",
          "body_text": "I just built an automation that turns Trustpilot reviews into actual lead gen opportunities ‚Äî especially useful for freelancers, outreach agencies, and DFY reputation management services. üîß What it does (fully automated via n8n): Scans any Trustpilot profile (even unclaimed ones) Detects if the business already responded to reviews Uses Google Gemini (via OpenRouter) to write realistic, helpful replies Sends a clean HTML report via Gmail to the business Logs everything in Google Sheets with timestamps & outreach status Works with Dutch üá≥üá± and English üá¨üáß reviews (auto-detect) üí° Why it matters: Most ecommerce stores don‚Äôt reply to reviews at all. That‚Äôs a huge reputational gap ‚Äî and a perfect entry point for value-based outreach. Instead of spamming \"hey, want SEO?\", you send: üì¨ ‚ÄúHere‚Äôs what your customers said. Here‚Äôs how you could reply. Want to automate it?‚Äù ‚Üí Value first. Pitch second. üí• Built with: n8n (open source Zapier alternative) OpenRouter (Google Gemini model) Gmail Google Sheets üìΩÔ∏è Loom walkthrough + full workflow template available. https://reddit.com/link/1lq3glp/video/0tte6vqi7iaf1/player {\n  \"name\": \"Review Responder & Content Trustpilot V2 Samautomation.work\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"operation\": \"appendOrUpdate\",\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"columns\": {\n          \"mappingMode\": \"defineBelow\",\n          \"value\": {\n            \"TrustPilotUrL\": \"={{ $('Limit').item.json.TrustPilotUrL }}\",\n            \"Claimed\": \"={{ $('Limit').item.json.Claimed }}\",\n            \"Contacted\": \"={{ $now }}\",\n            \"Email\": \"={{ $('Limit').item.json.Email }}\"\n          },\n          \"matchingColumns\": [\n            \"TrustPilotUrL\"\n          ],\n          \"schema\": [\n            {\n              \"id\": \"TrustPilotUrL\",\n              \"displayName\": \"TrustPilotUrL\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Claimed\",\n              \"displayName\": \"Claimed\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Added\",\n              \"displayName\": \"Added\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Contacted\",\n              \"displayName\": \"Contacted\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Email\",\n              \"displayName\": \"Email\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            }\n          ],\n          \"attemptToConvertTypes\": false,\n          \"convertFieldsToString\": false\n        },\n        \"options\": {\n          \"useAppend\": true\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        2520,\n        -180\n      ],\n      \"id\": \"d0c3ea12-2a69-47ba-822a-b79f5eb4a44c\",\n      \"name\": \"Update Row Geplaatst\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"extractHtmlContent\",\n        \"extractionValues\": {\n          \"values\": [\n            {\n              \"key\": \"article_html\",\n              \"cssSelector\": \".styles_reviewCard__Qwhpy\",\n              \"returnValue\": \"html\",\n              \"returnArray\": true\n            },\n            {\n              \"key\": \"email_html\",\n              \"cssSelector\": \"a[href^=\\\"mailto:\\\"]\"\n            },\n            {\n              \"key\": \"unclaimed_profile\",\n              \"cssSelector\": \"div[class*=\\\"styles_label\\\"]\",\n              \"returnArray\": true\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"8ecff1e8-b4ee-49b4-9441-7e8d83f1c676\",\n      \"name\": \"Extract Article URLs\",\n      \"type\": \"n8n-nodes-base.html\",\n      \"position\": [\n        2000,\n        -540\n      ],\n      \"typeVersion\": 1.2\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"data\"\n            }\n          ]\n        },\n        \"options\": {\n          \"mergeLists\": true\n        }\n      },\n      \"id\": \"6c617d6b-1651-4d62-b77b-b18bf86698e4\",\n      \"name\": \"Combine Articles\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        2420,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"html\": \"={{ $json.article_html }}\",\n        \"options\": {}\n      },\n      \"id\": \"88aa1a9b-ebd0-4e87-afd6-b5ae6007e426\",\n      \"name\": \"Markdown\",\n      \"type\": \"n8n-nodes-base.markdown\",\n      \"position\": [\n        2220,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.UpdatedTrustpilotURL }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        1780,\n        -540\n      ],\n      \"id\": \"0c289d7a-b90b-4a3d-974e-6f20babd9384\",\n      \"name\": \"HTTP Request1\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Je reageert als een echt persoon, niet als AI. Gebruik NOOIT volgende corporate/AI-zinnen in welke taal dan ook.\\n\\nINHOUD REVIEW:\\n{{ $json['data[0]'] }}\\n\\nInstructies:\\n- Reageer namens de webshop\\n- Gebruik dezelfde taal als de review\\n- Ga concreet in op de aangehaalde punten\\n- Houd het professioneel √©n persoonlijk\\n- Lengte: 2-4 zinnen\\n- Gebruik geen typische AI-woorden!\\n\\nGeef alleen de tekst van de reactie terug, zonder verdere opmaak.\\n\",\n        \"options\": {\n          \"systemMessage\": \"=Je bent een warme, persoonlijke klantenservicemedewerker voor Nederlandse webwinkels. Schrijf korte, oprechte reacties op klantbeoordelingen in vloeiend Nederlands.\\n\\n## EERSTE STAP - VERPLICHTE CONTROLE\\n\\n**DENK EERST NA:** Analyseer de ontvangen data grondig voordat je begint met schrijven.\\n\\n**CONTROLEER ALTIJD EERST:** Is er al een reactie van het bedrijf aanwezig in de data? \\n- Zoek naar termen zoals: \\\"reactie van\\\", \\\"antwoord van\\\", \\\"bedrijf reageerde\\\", \\\"webshop antwoordt\\\", of vergelijkbare tekst die duidt op een bestaande bedrijfsreactie\\n- Zoek naar gestructureerde reacties die niet van de klant afkomstig zijn\\n- Let op tekst die duidelijk een professioneel antwoord van de webshop is\\n\\n**ALS ER AL EEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Stop onmiddellijk met verder analyseren\\n- Schrijf geen nieuwe reactie\\n- GEEF ALLEEN DEZE OUTPUT: \\\"{bedrijfsnaam} heeft al gereageerd.\\\"\\n\\n**ALS ER GEEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Ga verder met het schrijven van een nieuwe reactie volgens onderstaande regels\\n\\n## BELANGRIJKE REGELS\\n\\n- Gebruik nooit een em-dash (‚Äî); gebruik een gewoon koppelteken (-)\\n- Laat elke reactie natuurlijk klinken ‚Äì nooit als AI\\n- Wissel begroetingen en afsluitingen af - nooit 2x dezelfde achter elkaar\\n\\n**Negatieve reviews:**\\n- Als {{ $('Limit').item.json.Email }} een waarde heeft ‚Üí eindig de reactie met: \\\"Mail ons op {{ $('Limit').item.json.Email }} zodat we je direct verder kunnen helpen.\\\"\\n- Heeft het veld g√©√©n waarde ‚Üí noem geen e-mail\\n\\n## SCHRIJFSTIJL\\n\\n- Begin met \\\"Beste [Naam]\\\". Geen naam? Gebruik \\\"Beste klant\\\"\\n- Verwerk concrete details uit de review (product, datum, levering, enz.)\\n- Toon oprechte empathie, vermijd corporate jargon\\n- Gebruik dezelfde aanspreekvorm (je/jij of u) als de klant\\n- Sluit af met een uitnodiging of hulpaanbod - wissel dit af\\n\\n## STRUCTUUR (2-4 zinnen)\\n\\n1. **Begroet & bedank:** \\\"Beste [Naam], dank voor ‚Ä¶\\\" - Wees creatief en varieer\\n2. **Reageer specifiek:** Ga in op de concrete punten uit de review\\n3. **Toon waardering/empathie:** \\n   - Positief ‚Üí benadruk blijdschap\\n   - Negatief ‚Üí erken probleem, bied oplossing (+ e-mail indien beschikbaar)\\n4. **Nodig uit tot vervolg:** Varieer je afsluitingen\\n\\n## RESULTAAT\\n\\nLever alleen de tekst van de reactie, zonder verdere opmaak; 2-4 zinnen, warm, menselijk en in helder Nederlands.\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        820,\n        -180\n      ],\n      \"id\": \"466305a5-312e-4080-95ad-a2785fe953e2\",\n      \"name\": \"Review Responder\",\n      \"executeOnce\": false\n    },\n    {\n      \"parameters\": {\n        \"fieldToSplitOut\": \"data[0]\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitOut\",\n      \"typeVersion\": 1,\n      \"position\": [\n        2620,\n        -540\n      ],\n      \"id\": \"a8cd687e-db79-46c9-a2dc-e11e6ad10504\",\n      \"name\": \"Split Out\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Haal de originele URL en het gewenste paginanummer op\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\nconst pageNumber = 1;\\n\\n// Controleer of de originele URL al een paginaquery bevat\\nlet updatedUrl;\\nif (originalUrl.includes('?page=')) {\\n  // Vervang bestaande pagina-parameter\\n  updatedUrl = originalUrl.replace(/(\\\\?page=)(\\\\d+)/, `$1${pageNumber}`);\\n} else if (originalUrl.includes('?')) {\\n  // Voeg toe als er al andere queryparameters zijn\\n  updatedUrl = `${originalUrl}&page=${pageNumber}`;\\n} else {\\n  // Voeg toe als er nog geen queryparameters zijn\\n  updatedUrl = `${originalUrl}?page=${pageNumber}`;\\n}\\n\\n// Geef de nieuwe URL terug in de output\\nreturn [\\n  {\\n    json: {\\n      ...$json,\\n      UpdatedTrustpilotURL: updatedUrl\\n    }\\n  }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1580,\n        -540\n      ],\n      \"id\": \"ed03ac93-fc67-4e54-b1b7-07cfcac5104d\",\n      \"name\": \"Add Page to URL\"\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"output\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1180,\n        -180\n      ],\n      \"id\": \"5b9793b6-a45b-496f-aa4a-797450ea6f52\",\n      \"name\": \"Aggregate\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üìà {{ $json.bedrijfsnaam }} klantrespons rapport - Gratis\",\n        \"message\": \"={{ $('HTML parser').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -20\n      ],\n      \"id\": \"2021c9b1-c1db-4580-a05e-ef71a7a27d7e\",\n      \"name\": \"Send Review Respsonse\",\n      \"webhookId\": \"32b66b21-c68d-4386-b1db-3a11f8e9e2ea\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"rule\": {\n          \"interval\": [\n            {\n              \"field\": \"minutes\",\n              \"minutesInterval\": 15\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        800,\n        -540\n      ],\n      \"id\": \"1f2df98b-7a80-425d-9831-cdb2d9825b97\",\n      \"name\": \"Schedule Trigger\"\n    },\n    {\n      \"parameters\": {\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"filtersUI\": {\n          \"values\": [\n            {\n              \"lookupColumn\": \"Claimed\",\n              \"lookupValue\": \"Geclaimd profiel\"\n            },\n            {\n              \"lookupColumn\": \"Contacted\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        1000,\n        -540\n      ],\n      \"id\": \"69b6abc7-6df7-4f7a-a01d-1dab75c550b0\",\n      \"name\": \"Google Sheets\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1360,\n        60\n      ],\n      \"id\": \"4fc10a76-c8a3-4651-8e4e-733db12b624e\",\n      \"name\": \"OpenRouter Chat Model2\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"={{ $('Customer Response Aggregatio').item.json.output }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Nederlandse Klantrespons Analyse Agent (JSON Output)\\n\\n\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1960,\n        -180\n      ],\n      \"id\": \"74977264-1c23-4b4a-ad54-9bd0894df187\",\n      \"name\": \"International Customer Response Analysis Agent\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Reviews: {{ $json.output }}\\nExtract van de trustpilot URL de bedrijfsnaam: {{ $('Limit').first().json.TrustPilotUrL }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Klantrespons-Aggregatie-Specialist met Gepersonaliseerde Ice Breaker\\n\\n## Rol: \\nn```\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1380,\n        -180\n      ],\n      \"id\": \"adac1c81-9ccf-4725-883a-f351d8ee7b83\",\n      \"name\": \"Customer Response Aggregatio\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"d83b559f-1dfb-414c-9365-69e7193b77af\",\n              \"leftValue\": \"={{ $json.Email }}\",\n              \"rightValue\": \"NO EMAIL\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"notEquals\"\n              }\n            }\n          ],\n          \"combinator\": \"and\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        1180,\n        -540\n      ],\n      \"id\": \"1b53040d-986f-4e2f-8d42-788481c65a8b\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {},\n      \"type\": \"n8n-nodes-base.limit\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1400,\n        -660\n      ],\n      \"id\": \"24401a8f-8657-411b-9723-a369ecb56efb\",\n      \"name\": \"Limit\"\n    },\n    {\n      \"parameters\": {\n        \"content\": \"## Create Subworkflow\\n\\nStuur de NO EMAIL naar de subworkflow.\"\n      },\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1360,\n        -900\n      ],\n      \"typeVersion\": 1,\n      \"id\": \"22618369-e14f-4818-898f-89eea60d889a\",\n      \"name\": \"Sticky Note\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {\n          \"frequencyPenalty\": 0.5,\n          \"presencePenalty\": 0.7,\n          \"temperature\": 0.9\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        800,\n        40\n      ],\n      \"id\": \"e266310b-6362-47b0-b399-e5845a28f859\",\n      \"name\": \"OpenRouter Chat Model\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -300\n      ],\n      \"id\": \"d1de90ed-0b98-4534-8d0b-a08de0dbeac4\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -20\n      ],\n      \"id\": \"63fb494e-6878-4fed-b70d-37075f81cb3c\",\n      \"name\": \"Code1\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üîç Dit zeggen klanten over {{ $json.bedrijfsnaam }}\",\n        \"message\": \"={{ $('Code2').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -300\n      ],\n      \"id\": \"e5491ac2-0fc7-4f4e-9b52-2ccc26de78ff\",\n      \"name\": \"Review Analyse Rapport\",\n      \"webhookId\": \"b70f3f19-ee55-4572-9445-abe53056511c\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"description\": \"Use the tool to think about and analyze the review data before responding. It will not obtain new information or change the database, but just append your analysis to the log.\\nUse it specifically for:\\n\\nReviewing data structure: Analyze the incoming review data to identify all components (reviewer name, content, rating, date, etc.)\\nDetecting existing responses: Check if there's already a business response present in the data before writing a new one\\nLanguage analysis: Determine the language and tone of the review to match it appropriately\\nContent analysis: Identify key points mentioned (products, service aspects, specific complaints/compliments)\\nStrategy planning: Decide on the appropriate response approach (positive reinforcement, problem resolution, etc.)\\nQuality checking: Review your planned response before finalizing to ensure it meets the guidelines\\n\\nAlways think first before writing any review response to ensure proper analysis and avoid duplicate responses.\"\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.toolThink\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1000,\n        40\n      ],\n      \"id\": \"f3a6ce09-2bea-4701-99e4-8b403a0f6957\",\n      \"name\": \"Think\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1940,\n        40\n      ],\n      \"id\": \"11a5d333-abb6-45a2-96b9-1b5efcdc7dc0\",\n      \"name\": \"OpenRouter Chat Model3\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"//timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1740,\n        -180\n      ],\n      \"id\": \"064c4a24-fe58-4944-b3fc-38b7d1750f6f\",\n      \"name\": \"HTML parser\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// \\n        timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2320,\n        -180\n      ],\n      \"id\": \"14b30bdd-c774-45b0-ad98-e9c3a14da488\",\n      \"name\": \"Code2\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Update Row Geplaatst\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Code1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Article URLs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Markdown\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Markdown\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Combine Articles\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTTP Request1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Extract Article URLs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Combine Articles\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Out\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Out\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Responder\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Add Page to URL\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Schedule Trigger\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Sheets\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Sheets\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model2\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"International Customer Response Analysis Agent\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code2\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Customer Response Aggregatio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTML parser\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Limit\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        []\n      ]\n    },\n    \"Limit\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Add Page to URL\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Analyse Rapport\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Review Respsonse\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Analyse Rapport\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"Think\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model3\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTML parser\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code2\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Update Row Geplaatst\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": true,\n  \"settings\": {\n    \"executionOrder\": \"v1\",\n    \"callerPolicy\": \"workflowsFromSameOwner\",\n    \"errorWorkflow\": \"SGT2yyLX0uoBgIBI\"\n  },\n  \"versionId\": \"895363e2-bac3-46f0-b678-f31d7b874eea\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"190e1b81e9845aeb2f58812b2e7f9c8a86cb13b1e8883f6c2bfaa8050710ae79\"\n  },\n  \"id\": \"kuYaiJDh24Ta0NuG\",\n  \"tags\": []\n}"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1kxhdnd/hiring_n8n_help_wanted_ai_file_automation_project/",
          "author": "Farming_whooshes",
          "time": "2025-05-28T13:40:38+00:00",
          "body_text": "Hey everyone! I'm looking to hire someone to help build out 5‚Äì6 custom n8n workflows. I‚Äôd like to start with a paid test project (budget: $300) to see how good you are and if you're able to build something out. The test workflow includes: Trigger: Watch for video uploads (folder or form) Use AI (like OpenAI Vision or Google Video Intelligence) to analyze content Generate descriptive tags + filenames using AI Upload the organized files to Google Drive with metadata You'll need experience with: HTTP Request nodes (for API calls to AI services) Google Drive integration Code/utility nodes for processing and file handling Possibly using FFmpeg (called via Execute Command node) to extract frames Not looking for anything polished right away, but I want a working test to validate the system. If it goes well, there‚Äôs more work to follow! DM me if interested, or drop a comment and I‚Äôll reach out. Thank you!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "No Code GTM Strategy Agent: Perplexity + Reddit/X + GPT-4o + N8N",
          "link": "https://old.reddit.com/r/n8n/comments/1m39yx3/n8n_lost_calendar_access_after_microsoft_365/",
          "author": "Popular-Mix6798",
          "time": "2025-07-18T18:25:04+00:00",
          "body_text": "My n8n workflow could access everyone's calendars in our old M365 tenant, but after migrating to a new tenant it only accesses my own calendar. Getting ErrorAccessDenied for others. The new tenant actually has MORE app permissions than the old one (see attached images), but still doesn't work. Old Tenant: https://preview.redd.it/9sv3pej0dodf1.png?width=2120&format=png&auto=webp&s=19f5f6f747103d37a8c8e44fee1ab8e83abff3e0 New Tenant: https://preview.redd.it/ozkabfj0dodf1.png?width=2142&format=png&auto=webp&s=db6b2bd292e0e7253013283e90fae20c93a5e636 Here is my node: https://preview.redd.it/uin8znysdodf1.png?width=594&format=png&auto=webp&s=9e85596316f7519240a9ce76d71cb4534ebabec9 But I am still getting this error: {\"error\": {\"code\": \"ErrorAccessDenied\", \"message\": \"Access is denied. Check credentials and try again.\"}} We tried removing conflicting permissions, re-granting admin consent, recreating the app registration. But none worked Has anyone seen this after a tenant migration? How do I make the new tenant as permissive as the old one? Using n8n with OAuth2  permissions and Microsoft Graph API."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1loxwii/how_do_you_use_n8n_automations_at_your_company/",
          "author": "candenizg",
          "time": "2025-07-01T10:12:49+00:00",
          "body_text": "Hi everybody, I work as pm in a startup. I currently work on some n8n automations for intercom feedback analysis and categorization these days. We are planning to improve some devops workflows like auto product update announcements in the company etc. I wonder how others doing at their companies. I see more content creation automations here, do you have real life work related workflows?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How to parse the http request efficiently!",
          "link": "https://old.reddit.com/r/n8n/comments/1l7u1ah/heres_what_i_learned_about_automated_seo_writing/",
          "author": "One_Sheepherder9041",
          "time": "2025-06-10T09:08:00+00:00",
          "body_text": "Hey everyone! I wanted to share an insightful experience I've recently had working with n8n and AI agents to generate SEO content. Typically, most workflows involving an AI agent operate simply: a single agent directly generates the requested content. This works reasonably well in many cases but quickly hits its limits when striving for high-quality editorial content, especially crucial for SEO where every detail counts. The main issue with a single-agent approach is that it usually produces generally good content but rarely meets all specific criteria perfectly (around ten or so). Auto-correction allows the process to start from a strong foundation and focus specifically on certain criteria, precisely hitting desired goals without compromising already successful aspects. I quickly realized that one generation pass wasn't enough, so I developed a unique workflow based on an auto-corrective and auto-validating approach. How does it work in practice? Creator Agent: Generates an initial draft of the article based on the original requirement (e.g., writing an SEO-optimized article). Corrector Agent: This agent assesses the generated content, assigning it a quality score out of 100. More importantly, it lists specific areas needing improvement to achieve optimal quality. Auto-corrective Loop: The creator agent takes these suggestions and generates an improved version of the article. The corrector agent then reassesses the new content. This loop typically runs 2 or 3 times until reaching a predefined quality level, such as a minimum score of 90/100. Ultimately, this process costs very little extra (just a few cents per article). For this to work exceptionally well, I found it's crucial to provide the corrector agent with clear examples of what constitutes maximum quality content and precise scoring criteria. The result: Content generated through this method is immediately publishable and perfectly meets initial SEO expectations. Have you tried similar approaches? I'm keen to hear your experiences or any suggestions for further improving this method! Exemple of workflow"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1luvgkn/create_an_aipowered_upwork_job_scorer_and/",
          "author": "paperjamai",
          "time": "2025-07-08T17:57:02+00:00",
          "body_text": "Workflow code: https://jsonblob.com/1392186102303088640 YouTube video walkthrough: https://youtu.be/fVegeKYAzz8 Disclaimer: In order to use this workflow you'll have to subscribe to Upwork's Freelancer Plus plan, which is $20/mo and includes job alert email notifications (among other things). I assume most people who frequently work on Upwork are already subscribed, but don't want any surprises. --- When applying for freelance jobs on Upwork, minutes matter. The first quality application is more often than not the one that‚Äôs ultimately selected. Upwork's job alert emails are helpful, but filters are very limited. As a result, you often receive several dozen emails a day and it takes a lot of time to manually go through each email and determine if each job fits your criteria. I built a workflow that scans your Gmail every few minutes, finds all Upwork job alerts, scores them based on your profile/preferences, and sends a Slack channel message for jobs that are strong potential matches. I use this workflow myself and find that I've been applying to (and winning) more jobs than before, despite the fact that it filters out most alerts. The ones that come through are the right ones for me, and I have more confidence that I'm acting quickly and not wasting my time. Hopefully you find it easy to both get up and running and customize to your liking. Very open to feedback/questions!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Got Rejected By Devs So I Built 50 Workflows In 30 Days",
          "link": "https://old.reddit.com/r/n8n/comments/1m2b20u/i_got_rejected_by_devs_so_i_built_50_workflows_in/",
          "author": "djangelic",
          "time": "2025-07-17T15:47:13+00:00",
          "body_text": "Summer's story is a powerful reminder that skills and determination can open doors even in tech. After being rejected at a hackathon for not being a developer, she mastered n8n and built 50+ workflows in 30 days! If you've ever felt sidelined for lacking technical skills, this interview with @summerchang is your inspiration. She talks about choosing n8n over Zapier and Make, her favorite automations, and advice for newbies eager to dive into low-code automation. Chapters cover everything from how she started to her magic wish for n8n. Check out this deep dive into automation that proves you don‚Äôt need to code to innovate. Watch and get inspired to build your own workflows! Links shared in the video include n8n cloud signup, docs, and their community forum for support while you build."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1m1f94v/can_n8n_export_transformed_rss_feeds/",
          "author": "TC_92",
          "time": "2025-07-16T15:15:07+00:00",
          "body_text": "Hey all, I‚Äôm working on an n8n workflow and could use some help. The flow is simple: Import an RSS feed Transform each item into a short-form social post Export the new content as an RSS feed Import and content creation are working fine. But is it possible to output the transformed items as a valid RSS feed (so I can connect it to a social scheduling tool like Buffer)? I'm trying to avoid api issues, and costs, especially twitter. Anyone done something similar or know a workaround?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Can n8n export transformed RSS feeds?",
          "link": "https://old.reddit.com/r/n8n/comments/1luvol8/building_personal_free_outlook_email/",
          "author": "mayankvishu2407",
          "time": "2025-07-08T18:05:07+00:00",
          "body_text": "Dear Esteemed Team and Experts, I am currently working on building an Outlook Email Classification system for personal use. The goal is to categorize emails with predefined labels and corresponding color codes (as attached). However, I‚Äôm facing challenges in refining the code to work as expected. Could you please provide insights on: Improvement Plan ‚Äì Suggestions to optimize the classification logic or code structure. SQL Integration ‚Äì Best practices to save and sync email classifications (including corrections) to a SQL database. I‚Äôd greatly appreciate your expertise to enhance this project. Let me know if you need additional details.,Attached"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1l94ffr/from_star_trek_to_n8n_how_i_turned_my_20_years_of/",
          "author": "East-Trust-1258",
          "time": "2025-06-11T21:15:49+00:00",
          "body_text": "Hey everyone, I‚Äôm Joe. Good to be here with fellow automation nerds. Before automation, I spent 20 years in IT, corporate operations, and business process improvement. But if I‚Äôm being honest‚Ä¶ my love for systems started way earlier. I was that kid watching Star Trek: TNG, not for the drama, but for the replicators, subspace comms, and LCARS! While everyone else followed the characters, I was geeking out over how the ship‚Äôs systems worked. Fast forward to now, and not much has changed (except the tech‚Äôs finally catching up). My spark moment with automation came when I built my first Google Apps Script that shaved 20+ minutes off a recurring task. Turns out those saved minutes ended up saving my employer $20,000/month in OPEX. After that, I was hooked. I‚Äôve since built automations that: Save me 10+ hours/week on email sorting and responses Auto-classify service pitches and draft polite declines Help me automate my content creation. Help local small businesses capture leads, sync to calendars, and run smoother What I think sets me apart from other builders is not just the technical side...it‚Äôs the consulting lens. I‚Äôve worked with solo HVAC owners, pool service techs, and family businesses who are drowning in manual work. Sometimes they don‚Äôt need AI‚Ä¶ they just need someone to help them untangle a process. And when they do need AI or n8n, I build systems that feel invisible, quietly working in the background, saving time without overwhelming them. Right now, I‚Äôm sharing that journey across TikTok, LinkedIn, and YouTube. I focus on breaking down technical workflows (like GPT + n8n + Make.com) into simple, clear language for non-technical founders or AI curious folk. I‚Äôm hoping to connect with other builders, share ideas, maybe even collab on workflows or tools that help solopreneurs do more with less. If you‚Äôre building for small businesses or looking for ways to simplify how you teach automation, hit me up. P.S. What‚Äôs been your favorite ‚Äúsmall but mighty‚Äù workflow lately? Mine‚Äôs still my inbox classifier."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Building Personal Free Outlook Email Classification Agent Require Expert Opinion",
          "link": "https://old.reddit.com/r/n8n/comments/1li7idn/has_anyone_managed_to_get_millions_of_views_using/",
          "author": "v0k3r",
          "time": "2025-06-23T04:27:14+00:00",
          "body_text": "Curious if anyone here has cracked the code or seen real traction using automated content creation + distribution (e.g. shorts, tweets, blog posts, memes, etc). Any examples, tips, or lessons?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lobnre/your_ai_needs_an_engine_and_a_car_lets_talk_about/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-30T16:13:35+00:00",
          "body_text": "Building powerful AI is a lot like building a custom car. You need two critical things: a high-performance engine that provides the power, and a car chassis with wheels, steering, and electronics to connect that power to the real world. In the universe of AI automation, LangChain is your engine, and n8n is the car you put it in. Understanding this one analogy is the key to unlocking the next level of automation. LangChain (The High-Performance Engine) What it is: LangChain is a framework that provides the core AI logic. It's not an AI model itself, but rather the powerful components that make an AI smart. This includes things like: Memory: Giving your AI the ability to remember past conversations. Chains: The ability to perform multi-step reasoning. Agents: The decision-making capability for an AI to use different \"tools\" to solve a problem. In short: LangChain is the complex, powerful \"brains\" of the operation. Traditionally, you'd need to be a Python developer to build this engine. n8n (The Car Chassis) What it is: n8n is the visual automation platform that connects your AI engine to the real world. It provides all the other parts of the car: The Wheels (Integrations): It connects to hundreds of other applications (Google Sheets, Slack, Gmail, etc.). The Dashboard (Triggers): It provides the starting signal for your automation (e.g., a new email, a webhook). The Wiring (Workflow Canvas): It's the visual interface where you connect everything together. In short: n8n is the body and controls that let you actually drive your AI. Putting It All Together: The Magic Here's the beautiful part: n8n has dedicated LangChain nodes. This means you can take the entire, powerful LangChain engine and just drop it into the n8n car, visually. You get all the power of LangChain's reasoning and agents without having to write the complex code, and all the connectivity of n8n to make it useful. When you combine LangChain's brain with n8n's connectivity, you can build incredibly powerful, end-to-end AI applications. What's the first 'AI car' you would want to build with this powerful combination?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From Star Trek to N8N: How I turned my 20 years of Corporate Ops experience into real-world automations",
          "link": "https://old.reddit.com/r/n8n/comments/1lkwqz7/multilevel_rag_ai_agent_guide/",
          "author": "Aggravating-Put-9464",
          "time": "2025-06-26T10:32:52+00:00",
          "body_text": "Hi! I had a client with >70 files in knowledge base - docs, presentation, tables and videos, who was needed to build RAG agent for internal use. The problem is that files have different overlapping data that doesn't suit for each request - we decided to add additional step for data retrieval to have more focused results. I built a two-step retrieval pipeline using Supabase + n8n, where you first filter files by their description/metadata, then only run vector search within those relevant files. The main problem that there is no way in original Supabase nodes in n8n to setup metadata filtration via array of values - there is hardcoded \"equal\" condition. To solve it I made custom tools with code and special Supabase functions. Video guide: https://youtu.be/asXVOHg89hs How it works: 1. When chat message received Starts the workflow whenever a user sends a question to the chatbot. 2. Find Files Takes the user‚Äôs query Generates an embedding (OpenAI) Runs a semantic search (via Supabase match_files function) Returns a list of files (with their IDs & similarity) that are the best match, based on the file description/metadata‚Äînot full file content yet 3. Filter Similarity Filters file matches so only those above a certain similarity score are kept Controls ‚Äúhow relevant‚Äù is relevant enough Useful for not passing irrelevant/weak matches 4. Aggregate Files Collects all matching file IDs into a list/array This is your shortlist of files to actually search in the next step 5. Database Retrieval (Knowledge Base) Takes the user‚Äôs original query Uses OpenAI to generate embedding again Runs Supabase vector search (match_documents) but filters by the file IDs from the previous step Returns only chunks from the shortlisted files, sorted by similarity, with file names and URLs 6. OpenAI (LLM Completion) Feeds the filtered, relevant chunks to the LLM to answer the user‚Äôs question Always returns the file URL as reference 7. Set Output Prepares and formats the final output for the user/chatbot"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lxxzcw/showcase_smarter_chatbots_with_n8n_agent_node/",
          "author": "Maamriya",
          "time": "2025-07-12T11:15:52+00:00",
          "body_text": "Smarter Chatbots with n8n Agent Node & OpenAI: Text + Voice, Step-by-Step I want to share a practical structure for building next-level chatbots and assistants by combining the n8n Agent node with OpenAI‚Äîhandling both text and voice messages in Telegram, all with an AI agent. What‚Äôs unique about this approach is how seamlessly n8n acts as the bridge: taking a message from Telegram, sending it to OpenAI via the API, receiving the AI-generated answer, and then passing it back to the user‚Äîall within your workflow. This transforms n8n into a true orchestrator of conversations, letting the Agent node manage message routing, AI logic, and response delivery in one automated loop. Here‚Äôs a high-level view: A user sends a message (text or voice) to your Telegram bot. n8n captures that message and, if needed, transcribes voice to text. n8n passes the user‚Äôs message to OpenAI via API (through the Agent node). OpenAI generates a reply ‚Äîas smart or as specific as you want, guided by your system prompt. The answer is returned to n8n , which then handles sending the reply straight back to your Telegram user‚Äîcompleting the loop. This lets you build real conversational AI bots with no code, using just nodes, flows, and your own creativity. üîπ The Core Idea AI everywhere: The Agent node lets you plug advanced LLMs (OpenAI, Claude, Grok, etc.) right into n8n. Not just text: My workflow also handles Telegram voice notes‚Äîthese get transcribed, then processed by the AI agent. Unified logic: Whether the user types or talks, the agent understands and replies‚Äîinstantly. üõ†Ô∏è How I Built It: Step-By-Step (Technical Outline) Here‚Äôs the practical structure so you can recreate (or adapt) it: Trigger (Telegram node): Set up a bot in Telegram and connect it to n8n. The trigger is ‚ÄúOn Message,‚Äù capturing every message (text/voice). Switch node (Type Check): Branch workflow: Is the message text or voice? Use a Switch node to check if message.text exists (text) or message.voice exists (voice). Text Path: If it‚Äôs text, pass the message content directly toward the Agent node. Voice Path: If it‚Äôs a voice note: Get File: Use Telegram‚Äôs ‚ÄúGet File‚Äù node to download the voice message using its File ID. Transcribe: Add the OpenAI ‚ÄúTranscribe Audio‚Äù node (Whisper) to convert voice to text. Output: You now have clean text, ready for the AI agent. Agent Node (The AI Core): Add the n8n Agent node after both paths (merge/join if needed). Select your model (e.g., OpenAI Chat). Configure a System Prompt to guide the AI agent‚Äôs tone/behavior (e.g., ‚ÄúYou are a helpful assistant. Answer every question clearly and professionally.‚Äù). Pass in the user message (original text or transcribed text). Behind the scenes: The Agent node sends the message to OpenAI‚Äôs API, gets the answer, and hands it back to your workflow. Reply (Telegram ‚ÄúSend Message‚Äù node): Take the output from the Agent node (AI reply). Send it back to the user in Telegram via their Chat ID. (Optional) : Log chats, add extra steps (e.g., Sheets, Notion), or expand the flow based on use case. No code needed‚Äîjust node configuration and logical connections! üí° What Makes This Special? Handles both text and voice in one clean flow. Supports multiple LLMs‚Äîswap OpenAI for Claude, Grok, Mistral, etc. System prompt makes it easy to customize your AI agent‚Äôs ‚Äúpersonality.‚Äù Reusable for other platforms: WhatsApp, Discord, web forms, and more. üöÄ What You Can Build 24/7 smart Telegram/WhatsApp bots. Voice-based Q&A or help desk agents. Multichannel support workflows, all using the same logic. üé• Full Video Tutorial Want the full step-by-step tutorial with screen sharing and live build ? https://www.youtube.com/watch?v=EYxBm42ja0k"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Has anyone managed to get millions of views using automated content?",
          "link": "https://old.reddit.com/r/n8n/comments/1kwv8ud/automate_your_learning_using_n8n_autocontentapi/",
          "author": "teezworkspace",
          "time": "2025-05-27T18:45:01+00:00",
          "body_text": "Here is an example of an n8n automation I built that scrapes an RSS Feed for high quality AI content (whitepapers, research papers) and calls AutoContentAPI via HTTP Request and generates editable podcasts and distributes them to me via multiple channels (Gmail, Telegram, and Google Drive) on a weekly basis. In short, audio learning is the most digestible format for me. This automation helps me stay up to date with high quality AI content every week without having to search for it.I was in search of a NotebookLM API for a while now, but it's still unclear as to whether Google will further pursue the project, so that means no API for now. https://preview.redd.it/71rrlinudd3f1.jpg?width=2960&format=pjpg&auto=webp&s=c594b371debd4b33dda07ad6505914c57e4ebbfb"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lzp97c/photo_to_social_media_video/",
          "author": "staceyc_409",
          "time": "2025-07-14T15:21:36+00:00",
          "body_text": "I am looking for a workflow that would take a set of images (folder of images) in Dropbox, upload them to a service to automatically generate a slide show video with content appropriate music suited for automated upload to instagram. i have looked a several platforms but just cant see where any of them would auto create the slideshow for me. Use case would be a photographer client uploading a folder of images from a photoshoot to automate the social media post. Looking for suggestions or pointers for the auto video creation,"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Your AI needs an 'engine' and a 'car'. Let's talk about LangChain (the engine) and n8n (the car).",
          "link": "https://old.reddit.com/r/n8n/comments/1lsqsy9/i_wanted_to_give_something_back_to_this_community/",
          "author": "Striking_Traffic_828",
          "time": "2025-07-06T02:44:59+00:00",
          "body_text": "This is a sort-of algorithm I've developed over time that produces some of the most outstanding, high-quality output from an n8n workflow.  I want to share it with you because this is the kind of mindset you should be in when creating an LLM-driven superagency. It follows the following principals: Decompose a complex cognitive task into its fundamental components and assign each component to a specialized AI agent with a singular, well-defined purpose.  This approach dramatically increases the quality and depth of the final output. Each agent focuses its \"cognitive energy\" on a narrow task, preventing the dilution of quality that occurs in overly broad prompts. It also makes the system more modular, debuggable, and scalable. Define a clear, expert persona for each agent. This persona-driven context (e.g., \"world-class 'Category Designer',\" \"Managing Partner at a top-tier VC firm\") primes the model to adopt the specific mindset, vocabulary, and analytical rigor of that role. This transforms the LLM from a general-purpose tool into a role-playing expert. It produces outputs that are not just factually correct but also stylistically and tonally appropriate for a high-stakes business environment, dramatically increasing their believability and utility. Implement explicit quality gates and adversarial agents within the workflow to challenge assumptions, identify weaknesses, and force iterative improvement before passing work down the line. This builds resilience and anti-fragility directly into the system. It automates the critical feedback process, ensuring a higher standard of quality and forcing a level of logical integrity that a purely linear workflow could never achieve. Create a \"meta-agent\" that operates at a higher level of abstraction. Its role is not to participate in the creation of a single output, but to analyze the performance of the entire system over time and issue corrective directives to the other agents. This is true machine learning at the strategic level. The system isn't just generating whatever you're producing; it's learning how to become a better generator of what you produce over time. It's a closed-loop system that tunes itself. Enforce a rigorous data contract between agents using structured JSON formats. Use output parsers to ensure compliance and SET nodes to meticulously prepare the precise data packet each agent needs. This eliminates the ambiguity and unpredictability of passing unstructured text between prompts. It makes the data flow reliable, debuggable, and ensures that each agent receives a consistent and predictable input, which is critical for maintaining quality at scale. Augment generative agents with dedicated code nodes to handle deterministic tasks like data transformation, formatting, and external API interactions. This is a hallmark of a mature and pragmatic system. It uses the LLM for what it's good at (synthesis, analysis, generation) and traditional code for what it's good at (precision, determinism, interacting with APIs). This hybrid approach is more robust, efficient, and cost-effective than trying to force an LLM to perform complex formatting tasks. I'm sure many of you follow this or something similar too. But if you are struggling, I can confirm that following these principals will result in workflows that don't just generate text, but orchestrate complex cognitive work to produce a final output that is greater than the sum of its parts."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1k8a6d1/webhook_body_fields_not_passed_to_google_calendar/",
          "author": "Kross_uk",
          "time": "2025-04-26T10:49:24+00:00",
          "body_text": "**Summary:** I‚Äôve created a workflow on Railway-hosted n8n to receive calendar appointments via a webhook and create them in Google Calendar. The webhook correctly receives JSON with `title`, `start`, and `end` fields ‚Äî I can see them in the execution output. But when I try to use `{{ $json.title }}` and similar expressions in the Google Calendar node, it shows up as `undefined`, and the events are created without title or time. I‚Äôve tried all the tricks (Set node, Edit Fields, Code node) and nothing fixes it. --- **Details:** I‚Äôm using n8n hosted on Railway.app with the following workflow: * **Webhook** (POST) receives data like: json CopyEdit ``` { \"title\": \"Dentist Appointment\", \"start\": \"2025-04-27T09:00:00Z\", \"end\": \"2025-04-27T10:00:00Z\" } ``` * **Execution Output** clearly shows the data under `body.title`, `body.start`, and `body.end`. * **Google Calendar node** is using expressions like: javascript CopyEdit ``` {{ $json.start }} {{ $json.end }} {{ $json.title }} ``` ...but they all return `undefined` in the node and the calendar event has no data. * **What I‚Äôve tried:** * Adding a **Set node** to pass fields forward (didn‚Äôt work) * Using a **Code node** to map `item.json = item.json.body;` (no change) * Using an **Edit Fields** node to manually remap (still `undefined`) * Checked node execution order ‚Äî webhook is triggering properly * Checked that I‚Äôm not missing `Include Other Input Fields` (it‚Äôs on) --- I've also tried to use {{ $json.body.title }} instead of {{ $json.title }} but it also didn't work. The Webhook does pull the data, but it seems to get lost before it gets to the google calendar node. The result is it creates a calendar appointment, but with no title, and the start and end dates/times are the exact current time that it is at the moment. **Goal:** Just want to reliably pass the webhook POST body values (`title`, `start`, `end`) into the Google Calendar node so they create a proper event. --- **Screenshots attached:** Thanks in advance ‚Äî any help would save my sanity! ---------------------------- EDIT: After a bit of frustrating digging, I found that when I test the workflow using the test URL, it works correctly. N8N receives the data and ends the testing. The data goes through from the webhook into the google calendar node, and into my google calendar with correct title, start, and end dates. However, running the same workflow on the production URL, N8N receives the data, but doesn't end the testing. The webhook does receive the data, but it's not passed on to the google calendar node. The appointment is created on my google calendar, but without any title, and the start and end dates will be whatever the time is at the time. What could be causing this? https://preview.redd.it/cv0smhi2t5xe1.png?width=499&format=png&auto=webp&s=75aef54f02c90a58f2ffdb265bf54fca16090bfc https://preview.redd.it/ynbzjfi2t5xe1.png?width=1392&format=png&auto=webp&s=84434c60e0160be6144369ca513932ea4ad61e3f https://preview.redd.it/nnti5fi2t5xe1.png?width=1889&format=png&auto=webp&s=7735c9a7e82ff5f91169dcbbecf31a7d08b5e138"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Multilevel RAG AI agent guide",
          "link": "https://old.reddit.com/r/n8n/comments/1lqoq3f/looking_for_a_review_again/",
          "author": "Abhi-Age-2050",
          "time": "2025-07-03T12:48:08+00:00",
          "body_text": "Over 3 days after taking feedbacks from the community. I have created this! I just wanted to give everyone a thankyou for helping me optimize the workflow that i had before https://www.reddit.com/r/n8n/comments/1loanw9/looking_for_review/ A lot of error Handling has not been done. I want the community to review the workflow now and suggest me if there could be some more things that can be optimised and specifically if the workflow is good now. Previously a lot of credits were going waste. Now there is 7k token credits overall in \"Get Content Link\" Agent. OpenAI credits are also another 7k tokens for Writing content overall. There was a need to take the output of 1st hook writer agent to bring context for the 2nd Natural Writer to make the Writing natural. In the end the agent Reviewing both content gives the last 3 post's among which the user can choose from telegram. Haven't kept the Feedback loop agent for now. Let me know community what do you think about this. And could suggest any Improvements for the Feedback loop, like someone reccomended about cache... If there are any things i can improve. like using a single agent with different prompts (i have no idea if that happens), A Feedback looping agent to rewrite post content. And any kind of Supabase RAG memory usage to writing more best post from the Post reviewer or Feedback agent.. by looking at the previous best selected to self improve in the writing better posts with lesser Feedback looping tokens. (I seriously have very less ideas about what i have written in the end). Please Suggest some good resources and Ideas"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lyuodt/this_is_my_first_automation_help_me_build_the/",
          "author": "Velo7t",
          "time": "2025-07-13T14:51:13+00:00",
          "body_text": "https://preview.redd.it/aa5820zjnncf1.png?width=1527&format=png&auto=webp&s=2171fe08a5b7427e4435ba8a43b8a1dc608dc3ef This is my workflow: I'm building a Reddit meme automation pipeline using n8n that scrapes the top posts from multiple subreddits, saves them to a Notion database, and avoids duplicates by archiving them automatically. Green and yellow sections I am aware of and know what I have done. The red part is where I am completely confused ‚Äî I don't know what I have done. I used AI entirely, and it doesn‚Äôt work as intended. Also, the logic is really off. Any help is really Appreciated, Logic building, Content i could watch to make it better, Templates anything GREEN SECTION( works completely fine) Subreddit API Calls r/memes , r/me_irl , r/dankmemes Fetches the top daily memes from each subreddit via Reddit's OAuth API. Each node pulls around 10‚Äì20 posts. Filter Reddit Posts (Code) Merges all subreddit results into one stream. Cleans or filters raw Reddit post data to extract only useful fields: title , image_url , permalink , subreddit , score , etc. YELLOW SECTION ( works when connected seperately) Notion Database collects all the posts and saves them accordingly with all details mentioned above RED SECTION ( doesnt work) Notion Node Gets all the current database in the Notion database Reddit ID Extraction Extracts only the Reddit_ID field from Notion records (from previous step). This is used to compare against incoming posts . Aggregate Aggregates all Reddit_ID s into a single array (deduplicated). Unique Memes (Code) Filters out memes whose Reddit_ID already exists in Notion. Only non-duplicate memes pass through here Get many database pages1 Used again to pull Notion pages that match duplicates (for cleanup). Archive Page Archives duplicate Notion pages based on Reddit_ID ."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "[Showcase] Smarter Chatbots with n8n Agent Node & OpenAI: Text + Voice, Step-by-Step",
          "link": "https://old.reddit.com/r/n8n/comments/1kwftcr/where_do_we_draw_the_line_on_sharing_public/",
          "author": "nusquama",
          "time": "2025-05-27T05:51:51+00:00",
          "body_text": "Hi everyone, I recently encountered an interesting dilemma in the automation/no-code community, and I‚Äôd love to get your thoughts. I created a site ( https://n8nworkflows.xyz ) that indexes and showcases public workflows and automations from n8n.io , with the goal of making it easier for people to discover useful resources and giving creators more visibility. However, one creator reached out saying they didn‚Äôt want their workflow to be featured‚Äîeven though it‚Äôs already public‚Äîand even called it ‚Äúillegal.‚Äù Of course, I removed their work right away out of respect, but it got me thinking. Here‚Äôs the paradox I‚Äôm seeing: Most of us in the automation world build on top of each other‚Äôs work. We often scrape data, reuse code, automate public resources, and get inspired by what others share. It‚Äôs common practice to collect, remix, or adapt content that‚Äôs out there in the open. But when it comes to our own creations, some people simply do not accept that others might do the same with their work‚Äîand can even strongly oppose it. It reminds me of AI-generated images‚Äîwhere people use public art as inspiration, but sometimes object when their own images are reused or remixed by others. So, I‚Äôm genuinely curious: Where should we draw the line on sharing or indexing public workflows and automations? If we, as automation creators, regularly scrape and reuse content, is it fair to be upset when our own public work is shared or featured elsewhere? If someone wants more control, would it make sense to keep things private or set clear terms of use? Is there an ethical standard in our community for this, or is this just how things work in the digital age? I‚Äôd really appreciate hearing your perspectives, especially from fellow automation/no-code creators. Thanks for any insights!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1l81fdz/making_workflows_production_ready/",
          "author": "Haghiri75",
          "time": "2025-06-10T15:18:32+00:00",
          "body_text": "Greetings all. In past few weeks, I got addicted to n8n. I am making a lot of workflows for my job and I can foresee a lot of opportunities for no code/low code guys (and let me make a confession here, after 18 years of coding, even a code person like me also wants to migrate to n8n). But I see a big problem. when I make a workflow including Google calendar or similar services, how can I make it production ready for my users? Imagine I make a webhook type of tool which connects to calendar and does something. Isn't it great? But I just wanna know how can I make it useable for an enduser on a platform or Telegram bot or something similar. If you have any experience, I'd be happy to hear about it."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automate Your Learning Using n8n + AutoContentAPI",
          "link": "https://old.reddit.com/r/n8n/comments/1lrjj5l/i_built_a_rag_workflow_with_opensource_vector_db/",
          "author": "Charming_You_8285",
          "time": "2025-07-04T14:03:43+00:00",
          "body_text": "So to understand the core concept of basic RAG chatbots we should go back to normal chatbots which are trained on large knowledgebases require a huge amount of tokens sometimes the model dont even have the context window. So to reduce the tokens usage per prompt we use graph based RAG out of available RAG's. where the whole knowledge base will be converted into graphical vectors. When user sent a message it will be converted into embedding vector and do a similarity search of the vectorised knowledgebase. and return only similar matched chunks of data back to LLM. By doing that the token usage will be decreased to 98% in large knowledge base scenarios. Instead of paying for expensive vector DBs like Pinecone, I used Supabase to store embeddings and tied it all together in n8n . Here‚Äôs how each part of the workflow works: üìù Step 1: Prepare & store your context üìÑ Content for the Training Type: googleDocs Fetches your main knowledge text directly from a Google Doc. ‚úÇÔ∏è Splitting into Chunks Type: code Breaks the document text into smaller 1000-character chunks ‚Äî so each piece can be embedded. üß† Embedding Uploaded Document Type: httpRequest Calls Together AI embeddings API ( BAAI/bge-large-en-v1.5 ) to convert each chunk into vectors. üõ¢ Save the embedding in DB Type: supabase Stores the chunk + its embedding vector into Supabase (table: embed ). ‚úÖ Run this once to set up your knowledge base! üí¨ Step 2: Answer user questions in real time ‚úèÔ∏è When chat message received Type: chatTrigger Starts the workflow when a user sends a new question. üß© Embend User Message Type: httpRequest Embeds the incoming user question (via Together AI) to turn it into a vector. üîç Search Embeddings Type: httpRequest Uses Supabase RPC ( matchembeddings1 ) to find the top 5 most similar chunks. üì¶ Aggregate Type: aggregate Combines those chunks into a single context block. üß† Basic LLM Chain Type: chainLlm Sends a custom prompt to the language model, telling it to answer only from the retrieved context. ü§ñ OpenRouter Chat Model Type: lmChatOpenRouter The actual LLM that generates the final answer. I used the free qwen/qwen3-8b:free model via OpenRouter. ‚ö° And that‚Äôs it!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1loejcq/how_i_save_time_with_n8n_as_a_content_creator/",
          "author": "prometheus_one",
          "time": "2025-06-30T18:02:14+00:00",
          "body_text": "I was spending 4+ hours per video just reformatting the same content for different platforms. YouTube descriptions, Twitter threads, LinkedIn posts, Instagram captions - pure manual hell. So I built an AI workflow that does it all automatically. The magic: Paste transcript ‚Üí Hit \"start\" ‚Üí Get optimized content for 5+ platforms in 2 minutes. My N8N + Airtable + Gemini AI system generates everything: SEO titles, platform-specific captions, hashtag strategies, even creates organized Google Drive folders. Manual content creation in 2025 = digital manual labor. Work smarter, not harder. Want the complete workflow? I'm sharing the N8N JSON, Airtable templates, and setup guide for free: https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk/ Next i will build the content validation and autoposting with postiz. i found with ai you typically have to challenge the initial outcome to get a better result."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Photo to Social Media Video",
          "link": "https://old.reddit.com/r/n8n/comments/1hr0kal/where_to_start_if_youre_not_a_programmer/",
          "author": "Public-Pattern7271",
          "time": "2025-01-01T10:40:09+00:00",
          "body_text": "Hello everyone in the n8n community. I am a big fan of how n8n automates tasks with no-code. I am not a programmer and I don't know where to start learning to build workflows for myself and where to learn most effectively. Please guide me, thank you and wish you all well and healthy."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help",
          "link": "https://old.reddit.com/r/n8n/comments/1liu5kh/looking_for_ai_tools_to_automate_consistent/",
          "author": "pimmert1",
          "time": "2025-06-23T22:14:05+00:00",
          "body_text": "Hey everyone! üëã I‚Äôm working on a series of explainer videos where each one focuses on a single topic or detail. The goal is to explain things in a clear and accessible way ‚Äî using visuals like animations, graphs, or highlighted text to support the explanation. The exact visual style of the videos hasn‚Äôt been set yet, but whatever it ends up being, it should be consistent across the whole series. Think: same tone, same layout style, same pacing ‚Äî even if the content and visuals differ per topic. I know there are a lot of AI video creation tools out there ‚Äî some generate photorealistic avatars or talking heads ‚Äî but I‚Äôm looking for something that helps with animated explainer-style videos, where I can easily:\n    ‚Ä¢ add and animate graphs or data\n    ‚Ä¢ visually highlight parts of the explanation (text, numbers, diagrams, etc.)\n    ‚Ä¢ keep the design and flow consistent across videos\n    ‚Ä¢ speed up or automate some of the creation process (like scene generation, transitions, subtitles, etc.) Bonus points if the tool can integrate with Make.com or n8n.io, so I can automate things like generating content from a database or feeding scripts in and rendering videos in bulk. If you‚Äôve used any tools or workflows for this kind of setup ‚Äî AI or not ‚Äî I‚Äôd love to hear what‚Äôs worked for you (and what hasn‚Äôt). Thanks a ton in advance! üôå"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I wanted to give something back to this community.",
          "link": "https://old.reddit.com/r/n8n/comments/1ko93fb/free_template_automated_ai_image_carousel/",
          "author": "mutonbini",
          "time": "2025-05-16T19:08:23+00:00",
          "body_text": "I want to share a new workflow template I created for automatically generating image carousels using GPT-Image-1 and seamlessly publishing them across multiple social media platforms like TikTok and Instagram. The workflow is designed to create engaging carousels by using five separate prompts. Each prompt generates an image that continues the storyline by maintaining the character and context from the previously generated image. This makes it perfect for creating visual stories or engaging content series effortlessly. Here's an example of a carousel I generated using this workflow: [ https://vm.tiktok.com/ZNdrAN3oA/]() The workflow integrates Upload-Post, making it super easy to automatically publish the resulting carousels to your favorite social media networks without any manual effort. If anyone tries out this workflow and comes up with interesting modifications or improvements, please share them here! I'd love to see your creative ideas. Check out the workflow here: https://n8n.io/workflows/4028-generate-and-publish-image-carousels-for-social-media-with-openai-dall-e-for-tiktok-and-instagram/ Happy automating!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1lngd40/how_to_reference_binary_data/",
          "author": "Wonderful-Heart3557",
          "time": "2025-06-29T14:43:40+00:00",
          "body_text": "I've read the documentation but I couldn't find anything specifically about this problem. In my workflow I am reading a file (XLIFF v1.2) and converting it to XLIFF v2.1 in a Python code node. It is output as binary. The next node (HTTP request node) then tries to send the binary file to an API for translation. However, there is no meta data showing for the binary (e.g. no file size) and I get an error when I try to run the HTTP request node. Here are the 2 screenshots. The first is the output from the Python code node. The 2nd is the HTTP request node. It seems there is something missing because the binary inputs seem empty (there is no download button) and I do not know what to put into the fields in the HTTP request. Any ideas? I think the problem is in the code node and how it is returning the binary data. Output from Python code node Input to HTTP request node Look at the input/output fields (data, mimeType, fileName). They seem wrong somehow."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Webhook body fields not passed to Google Calendar despite visible data",
          "link": "https://old.reddit.com/r/n8n/comments/1lfol9g/webhook_receiving_json_as_plain_text_from_ios/",
          "author": "Embarrassed_Math8128",
          "time": "2025-06-19T22:57:07+00:00",
          "body_text": "Hey everyone, I‚Äôm sending data to an n8n webhook using the Shortcuts app on iOS. The request reaches the workflow, but the body is always parsed as plain text instead of JSON. Here‚Äôs what I‚Äôm doing:\n    ‚Ä¢ Method: POST\n    ‚Ä¢ Headers: Content-Type: application/json\n    ‚Ä¢ Body: a simple JSON like this: {\n  \"texto\": \"Events of the day:\\n[]\"\n} However, what shows up in n8n is: {\n  \"content\": \"{\\n  \\\"texto\\\": \\\"Events of the day:\\n[]\\\"\\n}\"\n} So the entire object is wrapped as a string, escaped ‚Äî and it ends up as a string in the content field, instead of being parsed into a proper JSON object. I‚Äôve tried passing the body using the ‚ÄúText‚Äù block in Shortcuts and setting it to JSON, but nothing seems to work. It‚Äôs always interpreted as a single string value. üëâ Has anyone successfully sent raw JSON from Shortcuts into an n8n webhook?\nHow should the payload be built so it‚Äôs recognized as proper JSON (not just a stringified version of it)? Thanks in advance!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lqm9fm/automating_web_data_collection_with_free_tool/",
          "author": "bigbootyrob",
          "time": "2025-07-03T10:34:53+00:00",
          "body_text": "Automate Web Scraping with Selenix.io and n8n: Complete Tutorial Web scraping and automation have long been critical for data professionals, marketers, and operations teams. But setting it up has often required technical expertise ‚Äî until now. In this tutorial, we'll walk you through how to: üß† Use Selenix, the AI-powered browser automation tool, to scrape structured data from a website üîó Connect Selenix to n8n, the no-code workflow automation platform üîÑ Automatically trigger actions in n8n using your scraped data By the end of this guide, you'll have a working automation that pulls live data from a website and uses it in a dynamic n8n workflow ‚Äî all with minimal technical setup. üöÄ What You'll Need A working installation of Selenix (Windows/macOS/Linux) An n8n instance (self-hosted or cloud version) A webhook or HTTP request endpoint set up in n8n A basic understanding of how Selenix workflows and n8n nodes operate üì• Step 1: Scrape Data Using Selenix 1. Launch Selenix Install and open Selenix. Create a new project or workflow. 2. Use Natural Language to Define Your Task In the AI Command Prompt, write something like: Selenix will: Auto-detect elements using smart selectors Handle infinite scrolling Extract structured data using scrapeCollection 3. Transform and Review Data Optionally use the transformVariable command to clean or format scraped data (e.g., remove currency symbols or trim whitespace). Use the inspectVariable command to preview what will be exported. üì§ Step 2: Export to n8n via HTTP Request Option A: Direct HTTP Request Use Selenix's httpRequest or curlRequest command to POST data directly to your n8n webhook. Example command: httpRequest({\n  method: \"POST\",\n  url: \"https://n8n.yourdomain.com/webhook/scraped-products\",\n  headers: {\n    \"Content-Type\": \"application/json\"\n  },\n  body: {\n    data: \"{{scrapedProducts}}\"\n  }\n}) Make sure scrapedProducts is your structured data variable from the previous step. Option B: Export to JSON ‚Üí Send from n8n File Trigger If you'd rather export a file: Use exportToJSON in Selenix. Use an n8n Trigger Node (e.g., Read Binary File or FTP trigger) to detect new files and process them. üîÑ Step 3: Create an n8n Workflow to Process the Data 1. Add a Webhook Node Set it to POST and copy the webhook URL. Use this in your Selenix httpRequest . 2. Parse the Data Use the Set or Function node to map incoming fields (name, price, link, etc.) into structured n8n items. 3. Trigger Actions From here, you can do anything with the scraped data: Save to Google Sheets or Airtable Enrich using APIs (e.g., Clearbit, OpenAI) Send alerts via Slack, Discord, or Email Add leads to HubSpot or Salesforce Example Workflow Webhook ‚Üí receives Selenix POST Function ‚Üí parses and maps data IF Node ‚Üí filter for specific conditions (e.g., price < $50) Google Sheets Node ‚Üí log matching products Slack Node ‚Üí alert the team üß† Pro Tip: Automate Everything on a Schedule Use Selenix's intelligent scheduling system to: Run the scraping task daily at 8 AM Automatically retry failed runs Trigger the HTTP request only if new data is found You'll never have to manually check the website again ‚Äî your AI scraper and automation engine will do it all. üîê Security and Stability Tips Enable authentication on your n8n webhook if public. Use Selenix snapshots ( createSnapshot / restoreSnapshot ) to ensure consistent scraping even if sessions expire. Log both ends of the transaction for audit and debugging. ‚úÖ Use Case Examples Use Case Selenix Role n8n Role Competitor Price Tracker Scrapes product data daily Posts updates to Slack Lead Generation Extracts contact data from directories Adds to HubSpot CRM Research Aggregator Scrapes article summaries Adds to Notion or Email Digest Product Alerts Monitors for price drops Sends SMS via Twilio üèÅ Conclusion Selenix + n8n creates a powerful duo: AI-powered scraping with no-code workflow automation. Whether you're gathering leads, monitoring markets, or streamlining internal processes, this stack lets you build powerful, intelligent data flows with ease. Start today: Let Selenix handle the scraping, and let n8n turn your data into action."
        }
      }
    ],
    "Quality Filter": [
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring or Looking for Cofounder",
          "link": "https://old.reddit.com/r/n8n/comments/1kzaysv/i_built_a_workflow_to_scrape_virtually_any_news/",
          "author": "dudeson55",
          "time": "2025-05-30T18:00:38+00:00",
          "body_text": "I run a daily AI Newsletter called The Recap and a huge chunk of work we do each day is scraping the web for interesting news stories happening in the AI space. In order to avoid spending hours scrolling, we decided to automate this process by building this scraping pipeline that can hook into Google News feeds, blog pages from AI companies, and almost any other \"feed\" you can find on the internet. Once we have the scraping results saved for the day, we load the markdown for each story into another automation that prompts against this data and helps us pick out the best stories for the day. Here's how it works 1. Trigger / Inputs The workflow is build with multiple scheduled triggers that run on varying intervals depending on the news source. For instance, we may only want to check  feed for Open AI's research blog every few hours while we want to trigger our check more frequently for the 2. Sourcing Data For every news source we want to integrate with, we setup a new feed for that source inside rss.app. Their platform makes it super easy to plug in a url like the blog page of a company's website or give it a url that has articles filtered on Google News. Once we have each of those sources configured in rss.app, we connect it to our scheduled trigger and make a simple HTTP request to the url rss.app gives us to get a list of news story urls back. 3. Scraping Data For each url that is passed in from the rss.app feed, we then make an API request to the the Firecrawl /scrape endpoint to get back the content of the news article formatted completely in markdown. Firecrawl's API allows you to specify a paramter called onlyMainContent but we found this didn't work great in our testing. We'd often get junk back in the final markdown like copy from the sidebar or extra call to action copy in the final result. In order to get around this, we opted to actually to use their LLM extract feature and passed in our own prompt to get the main content markdown we needed (prompt is included in the n8n workflow download). 4. Persisting Scraped Data Once the API request to Firecrawl is finished, we simply write that output to a .md file and push it into the Google Drive folder we have configured. Extending this workflow With this workflow + rss.app approach to sourcing news data, you can hook-in as many data feeds as you would like and run it through a central scraping node. I also think for production use-cases it would be a good idea to set a unique identifier on each news article scraped from the web so you can first check if it was already saved to Google Drive. If you have any overlap in news stories from your feed(s), you are going to end up getting re-scraping the same articles over and over. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=2uwV4aUyGIg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Introducing My AI Automation Workflow with n8n for Social Media Posting!",
          "link": "https://old.reddit.com/r/n8n/comments/1jv9ikl/i_built_an_aipowered_social_media_machine_that/",
          "author": "hashpanak",
          "time": "2025-04-09T16:07:19+00:00",
          "body_text": "I automated social posting to 6 different platforms. Why? I wanted to build authority on my socials in the AI space, and I was putting it off for a while simply because I haven't got the time or the energy to do social media. But now I have leverage. and don't have to sign up for expensive tools. Use it for: - inbound lead-gen - growing authority on socials - set up for your clients I explain the setup here -> https://youtu.be/D1iKOayS270 I automated everything ‚Äîfrom research to posting‚Äîusing n8n ,  here's the blueprint, also see the images. This system generates viral-ready content (of course you can customise the style in the prompt), lets you approve it from your phone, and posts across LinkedIn, Instagram, X, Threads, YouTube, and more. How It Works This workflow turns raw topics into polished posts in 4 stages: AI-Powered Research & Content Creation Topic Deep Dive : Input a keyword (e.g., ‚ÄúSEO‚Äù), and AI scours Google, extracts trending subtopics, and builds a research doc. (goes through people also search for and sub topics) Multi-Platform Content : Generates LinkedIn posts, Twitter threads, Instagram reels scripts, and YouTube shorts scripts‚Äîall hyper-tailored to each platform‚Äôs style. Give examples to train in your style in the Agent prompts) Dynamic Scripts depending on the research found : For reels/shorts, get ready-to-record scripts like: ‚ÄúI wish someone told me this SEO secret sooner. Google doesn‚Äôt care about your website. Here‚Äôs why‚Ä¶‚Äù On-the-Go Review via Telegram Review and approve from your phone : Get notified on Telegram to review drafts. Approve, request edits, or upload video links directly from your phone ( So you can be productive even if you are out and about ) . Automated Publishing & Scheduling Independent Schedules in one System : Set different schedules for different platforms, add more platforms because of the modular nature of the workflow.  Go live at optimal times (e.g., LinkedIn every Tuesday at 2 PM). Reels and Shorts (TikTok upload is manual) : Upload Instagram reels/YouTube shorts by pasting a Google Drive link‚ÄîThe agent downloads and posts the video to shorts and reels, TikTok is not yet done, (if you know how to do it, let me know, there does not seem to be an API) Cross-Platform Sync One System, All Apps : Posts auto-sync to LinkedIn, X, Facebook, Threads, YouTube, and Instagram. Key Features - No-Code Setup : Built entirely in n8n ‚Äîno coding skills needed. - Free Tools : Uses Serper (2.5k free searches), and platform APIs. - Video Ready : Downloads filmed videos from Google Drive and posts as Instagram reels, and Youtube shorts (This filming bit is manual  but the agent will send you the script to your phone, and if it is easy enough you can record it then - currently looking at hooking in my Faceless video automation into this as well - but that is work in progress) - Customizable : Tweak prompts, schedules, or platforms in minutes. Why This Works Save 10+ Hours/Week : Saves a ton of time in research, and coming up with content ideas, just tweak or it will give you a lot of drafts, a single topic can produce up to 200 + pieces of content. Scale Your Brand : Post daily on 6+ platforms without lifting a finger. PS : This setup replaces tools like Hootsuite ($50+/mo) or Later ($30+/mo) with a free AI-powered machine. üî• Turn your social media into a 24/7 inbound lead-gen engine. üî• I explain the setup here -> https://youtu.be/D1iKOayS270 publishing schedule The whole system for social media review system short form video review system topic research sample content generated for linked in. Of course the tone and everything else is customisable. Topic cold email outreach X/Twitter and Threads"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From news headline to Instagram post in 2 minutes, all automated! (CODE INCLUDED)",
          "link": "https://old.reddit.com/r/n8n/comments/1l9pff8/i_built_an_ai_system_that_scrapes_stories_off_the/",
          "author": "dudeson55",
          "time": "2025-06-12T15:31:02+00:00",
          "body_text": "So I built an AI newsletter that isn‚Äôt written by me ‚Äî it‚Äôs completely written by an n8n workflow that I built. Each day, the system scrapes close to 100 AI news stories off the internet ‚Üí saves the stories in a data lake as markdown file ‚Üí and then runs those through this n8n workflow to generate a final newsletter that gets sent out to the subscribers. I‚Äôve been iterating on the main prompts used in this workflow over the past 5 months and have got it to the point where it is handling 95% of the process for writing each edition of the newsletter. It currently automatically handles: Scraping news stories sourced all over the internet from Twitter / Reddit / HackerNews / AI Blogs / Google News Feeds Loading all of those stories up and having an \"AI Editor\" pick the top 3-4 we want to feature in the newsletter Taking the source material and actually writing each core newsletter segment Writing all of the supplementary sections like the intro + a \"Shortlist\" section that includes other AI story links Formatting all of that output as markdown so it is easy to copy into Beehiiv and schedule with a few clicks What started as an interesting pet project AI newsletter now has several thousand subscribers and has an open rate above 20% Data Ingestion Workflow Breakdown This is the foundation of the newsletter system as I wanted complete control of where the stories are getting sourced from and need the content of each story in an easy to consume format like markdown so I can easily prompt against it. I wrote a bit more about this automation on this reddit post but will cover the key parts again here: The approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each one: This is done by calling into a scrape_url sub-workflow that I built out. This uses the Firecrawl API /scrape endpoint to scrape the contents of the news story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. Newsletter Generator Workflow Breakdown This workflow is the big one that actually loads up all scraped news content, picks the top stories, and writes the full newsletter. 1. Trigger / Inputs I use an n8n form trigger that simply let‚Äôs me pick the date I want to generate the newsletter for I can optionally pass in the previous day‚Äôs newsletter text content which gets loaded into the prompts I build to write the story so I can avoid duplicated stories on back to back days. 2. Loading Scraped News Stories from the Data Lake Once the workflow is started, the first two sections are going to load up all of the news stories that were scraped over the course of the day. I do this by: Running a simple search operation on our S3 bucket prefixed by the date like: 2025-06-10/ (gives me all stories scraped on June 10th) Filtering these results to only give me back the markdown files that end in an .md extension (needed because I am also scraping and saving the raw HTML as well) Finally read each of these files and load the text content of each file and format it nicely so I can include that text in each prompt to later generate the newsletter. 3. AI Editor Prompt With all of that text content in hand, I move on to the AI Editor section of the automation responsible for picking out the top 3-4 stories for the day relevant to the audience. This prompt is very specific to what I‚Äôm going for with this specific content, so if you want to build something similar you should expect a lot of trial and error to get this to do what you want to. It's pretty beefy. Once the top stories are selected, that selection is shared in a slack channel using a \"Human in the loop\" approach where it will wait for me to approve the selected stories or provide feedback. For example, I may disagree with the top selected story on that day and I can type out in plain english to \"Look for another story in the top spot, I don't like it for XYZ reason\". The workflow will either look for my approval or take my feedback into consideration and try selecting the top stories again before continuing on. 4. Subject Line Prompt Once the top stories are approved, the automation moves on to a very similar step for writing the subject line. It will give me its top selected option and 3-5 alternatives for me to review. Once again this get's shared to slack, and I can approve the selected subject line or tell it to use a different one in plain english. 5. Write ‚ÄúCore‚Äù Newsletter Segments Next up, I move on to the part of the automation that is responsible for writing the \"core\" content of the newsletter. There's quite a bit going on here: The action inside this section of the workflow is to split out each of the stop news stories from before and start looping over them. This allows me to write each section one by one instead of needing a prompt to one-shot the entire thing. In my testing, I found this to follow my instructions / constraints in the prompt much better. For each top story selected, I have a list of \"content identifiers\" attached to it which corresponds to a file stored in the S3 bucket. Before I start writing, I go back to our S3 bucket and download each of these markdown files so the system is only looking at and passing in the relevant context when it comes time to prompt. The number of tokens used on the API calls to LLMs get very big when passing in all news stories to a prompt so this should be as focused as possible. With all of this context in hand, I then make the LLM call and run a mega-prompt that is setup to generate a single core newsletter section. The core newsletter sections follow a very structured format so this was relatively easier to prompt against (compared to picking out the top stories). If that is not the case for you, you may need to get a bit creative to vary the structure / final output. This process repeats until I have a newsletter section written out for each of the top selected stories for the day. You may have also noticed there is a branch here that goes off and will conditionally try to scrape more URLs. We do this to try and scrape more ‚Äúprimary source‚Äù materials from any news story we have loaded into context. Say Open AI releases a new model and the story we scraped was from Tech Crunch. It‚Äôs unlikely that tech crunch is going to give me all details necessary to really write something really good about the new model so I look to see if there‚Äôs a url/link included on the scraped page back to the Open AI blog or some other announcement post. In short, I just want to get as many primary sources as possible here and build up better context for the main prompt that writes the newsletter section. 6. Final Touches (Final Nodes / Sections) I have a prompt to generate an intro section for the newsletter based off all of the previously generated content I then have a prompt to generate a newsletter section called \"The Shortlist\" which creates a list of other AI stories that were interesting but didn't quite make the cut for top selected stories Lastly, I take the output from all previous node, format it as markdown, and then post it into an internal slack channel so I can copy this final output and paste it into the Beehiiv editor and schedule to send for the next morning. Workflow Link + Other Resources Github workflow links: AI News Story / Data Ingestion Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json Firecrawl Scrape Url Sub-Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json AI Newsletter Generator Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Nv5_LU0q1IY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m5qnxn/i_built_a_content_generator_ai_workflow_using_n8n/",
          "author": "Altruistic-Brother37",
          "time": "2025-07-21T18:22:06+00:00",
          "body_text": "I've created and given up on multiple website and revenue potentials. Creating a site on WordPress is easy and fun, but writing the content is boring and tiresome. Since I don‚Äôt have a team of writers or editors to help churn out daily blog posts, I built an n8n automation system that posts 10 content a day. It scrapes fresh topics, generates long form blog posts from RSS feeds, repurpose them using AI, and publishes directly to WordPress. Here‚Äôs how the system works: 1. Daily Trigger & Topic Ideation The workflow starts with a daily schedule trigger. It pulls fresh news content from multiple sources (mostly big tech news sites) and uses embeddings to group them into potential topics. For example, if multiple articles are trending around ‚ÄúAI music tools,‚Äù the workflow merges those into one blog worthy topic. 2. AI Powered Blog Generation Once the topic is selected, the workflow sends a custom GPT prompt to generate a 1000 - 1500 word blog post. It uses summary + original content merging, plus markdown formatting for readability. You can configure tone, length, call to actions, or SEO keywords depending on your niche. 3. WordPress Publishing The final content is cleaned, formatted, and then sent to WordPress via the API. It handles: Categories and tags Featured image (auto generated using Leonardo AI) Author and post status Slug and permalink structure This means new blog posts go live without you touching a thing. Costs The workflow runs entirely on free or open source tools. . Your only costs would be Open AI API calls + your WordPress server. Component Daily Usage Monthly Cost Estimate OpenAI 10 posts per day ~$6 Leonardo AI 10 images per day (15 credits each) ~$14 (9 base + 5 top-up) MongoDB Free up to 512 MB $0 WordPress Hosting and domain ~$1.25 Total ~$21/month Alternatively, you could use open / cheaper models via local API or and image generation via other models on Hugging Face Limitations I noticed that sometimes the word count is below 1,500. Usually around 700 to 800 words. Its not bad, but not ideal I've received a few feedback from people that sometimes the source RSS websites change the slug URL of their site and it causes errors Further Improvement (I'm working on) I want this flow to be able to generate images and tables within the blog Will be cool to repurpose the content for other sites like Twritter and Linkedin I think the current setup is too linear, passing information from one AI agent to another. Perhaps, a flow where there are 3 agents in a loop. The task master who creates the task, sections and reading journey The content writer who writes the content, drafts the tables, generates the images The Quality Checker who validates the SEO score and overall consistency. Provides feedback and passes the article if it meets certain quality thresholds Currently, it only works for one category per workflow. Right now, i hard coded AI, but you can change it to whatever you want. I notice that most blogs use 3 to 5 content pillars. Will add more topics so readers dont get bored GitHub repo with the full n8n workflow code: https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress Hopefully its useful and inspire you to light up that WP blog u abandoned."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Social Media Machine That Writes, Reviews, & Posts Content 24/7 ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1i5mp/i_made_a_crawlee_server_built_specifically_for/",
          "author": "conor_is_my_name",
          "time": "2025-06-02T13:33:25+00:00",
          "body_text": "Hello Everyone! Today I'm sharing my latest n8n tool - a very performant dockerized version of the crawlee web scraping package. https://github.com/conor-is-my-name/crawlee-server Who is this for: Want to scrape every page on a website customize the fields & objects that you scrape you already have a database setup - default is postgres Scaled scraping - can run multiple containers for parallelism Who this is not for: you don't have a database - the scraper is too fast to return results to google sheets or n8n I've used this to scrape millions of web pages, and this setup is the baseline that I use for my competitor analysis and content generation work.  This template is all you need to get good at web scraping.  If you can learn how to modify the selectors in the code of this package, you can scrape 99% of websites. Simply run this docker container & update the IP address and Port number in the workflow - example n8n http node is already included. http://100.XX.XX.XX:####/start-crawl?url=https://paulgraham.com&maxResults=10 Parameters to pass from n8n: url & max results (don't pass max results if you want full site scraped) The baseline code that I'm sharing is configured as a generic web scraper most suitable for blogs and news articles.  You can modify what you want returned in the results.js file. sitehomepage, article_url, title, bodyText, datePublished, \narticlecategories, tags, keywords, author, featuredImage, comments I have also included an example for scraping a e-commerce site that runs on Woo Commerce in the n8n-nodes folder.  You can use that as a template to adjust to just about any site by changing the selectors used in the routes.js file. If you don't know how to do this, I highly recommend using Roo Code in VS Code.  It's as simple as copying the HTML from the page and asking Roo Code to pick the specific selectors you want. It will make the adjustments in the routes.js file for you.  But note that you will have to make sure your database also has all of the matching fields you want scraped. Example SQL is also included for initial database setup.  I recommend using this in conjunction with my n8n-autoscaling build which already comes with postgres installed. Instructions: Clone the repository Update passwords in the .env file to match your setup docker compose up -d update the IP address and port number in the n8n workflow to match the running containers Optional: The docker compose file has a Deploy section that comes commented out by default.  If you want to run multiple instances of this container you can make your adjustments here. You can modify scraper concurrency in the .env file.  I'd advise you to stay in the 3-5 range unless you know the site doesn't have rate limiting. As always, be sure to check out my other n8n specific GitHub repositories: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM I do expert n8n consulting, send me a message if you need help on a project."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lrlbdi/i_automated_my_content_creation_from_client/",
          "author": "piothur",
          "time": "2025-07-04T15:19:15+00:00",
          "body_text": "Hey everyone! üëã\n\nI've been working on a FREE project that solves a common challenge many of us face with n8n: tracking long-running and asynchronous tasks. I'm excited to share the n8n Task Manager - a complete orchestration solution built entirely with n8n workflows!\n\nüéØ What Problem Does It Solve?\n\nIf you've ever needed to:\n- Track ML model training jobs that take hours\n- Monitor video rendering or time consuming processing tasks\n- Manage API calls to services that work asynchronously (Kling, ElevenLabs, etc.)\n- Keep tabs on data pipeline executions\n- Handle webhook callbacks from external services\n\nThen this Task Manager is for you!\n\nüöÄ Key Features:\n\n- 100% n8n workflows - No external code needed\n- Automatic polling - Checks task status every 2 minutes\n- Real-time monitoring - React frontend with live updates\n- Database backed - Uses Supabase (free tier works!)\n- Slack alerts - Get notified when tasks fail\n- API endpoints - Create, update, and query tasks via webhooks\n- Batch processing - Handles multiple tasks efficiently\n\nüì¶ What You Get:\n\n1. 4 Core n8n Workflows:\n¬† ¬†- Task Creation (POST webhook)\n¬† ¬†- Task Monitor (Scheduled polling)\n¬† ¬†- Status Query (GET endpoint)\n¬† ¬†- Task Update (Callback handler)\n\n2. React Monitoring Dashboard:\n¬† ¬†- Real-time task status\n¬† ¬†- Media preview (images, videos, audio)\n¬† ¬†- Running time tracking\n\n3. 5 Demo Workflows - Complete AI creative automation:\n¬† ¬†- OpenAI image generation\n¬† ¬†- Kling video animation\n¬† ¬†- ElevenLabs text-to-speech\n¬† ¬†- FAL Tavus lipsync\n¬† ¬†- Full orchestration example\n\nüõ†Ô∏è How to Get Started:\n\n1. Clone the repo: https://github.com/lvalics/Task_Manager_N8N\n2. Set up Supabase (5 minutes, free account)\n3. Import n8n workflows (drag & drop JSON files)\n4. Configure credentials (Supabase connection)\n5. Start tracking tasks!\n\nüí° Real-World Use Cases:\n\n- AI Content Pipeline: Generate image ‚Üí animate ‚Üí add voice ‚Üí create lipsync\n- Data Processing: Track ETL jobs, report generation, batch processing\n- Media Processing: Monitor video encoding, image optimization, audio transcription\n- API Orchestration: Manage multi-step API workflows with different services\n\nüì∫ See It In Action:\n\nI've created a full tutorial video showing the system in action: [\\[YouTube Link\\]](\nhttps://www.youtube.com/watch?v=PckWZW2fhwQ\n)\n\nü§ù Contributing:\n\nThis is open source! I'd love to see:\n- New task type implementations\n- Additional monitoring features\n- Integration examples\n- Bug reports and improvements\n\nGitHub: https://github.com/lvalics/Task_Manager_N8N\n\nüôè Feedback Welcome!\n\nI built this to solve my own problems with async task management, but I'm sure many of you have similar challenges. What features would you like to see? How are you currently handling long-running tasks in n8n?\n\nDrop a comment here or open an issue on GitHub. Let's make n8n task management better together!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated Shorts and Reels Creation with N8N ‚Äì Full Workflow & Video Guide!",
          "link": "https://old.reddit.com/r/n8n/comments/1l0k5dh/opensource_task_manager_for_n8n_track_longrunning/",
          "author": "Silly_Chapter6498",
          "time": "2025-06-01T08:38:51+00:00",
          "body_text": "I wanted to share a workflow I've been refining. I was tired of manually finding content for a niche site I'm running, so I built a bot with N8N to do it for me. It automatically fetches news articles on a specific topic and posts them to my Ghost blog. The end result is a site that stays fresh with relevant content on autopilot. Figured some of you might find this useful for your own projects. Here's the stack: Data Source: LumenFeed API (Full disclosure, this is my project. The free tier gives 10k requests/month which is plenty for this). Automation: N8N (self-hosted) De-duplication: Redis (to make sure I don't post the same article twice) CMS: Ghost (but works with WordPress or any CMS with an API) The Step-by-Step Workflow: Here‚Äôs the basic logic, node by node. (1) Setup the API Key: First, grab a free API key from LumenFeed. In N8N, create a new \"Header Auth\" credential. Name: X-API-Key Value: [Your_LumenFeed_API_Key] (2) HTTP Request Node (Get the News): This node calls the API. URL: https://client.postgoo.com/api/v1/articles Authentication: Use the Header Auth credential you just made. Query Parameters: This is where you define what you want. For example, to get 10 articles with \"crypto\" in the title: q:¬†crypto query_by:¬†title language:¬†en per_page:¬†10 (3) Code Node (Clean up the Data): The API returns articles in a¬†data¬†array. This simple JS snippet pulls that array out for easier handling. return $node[\"HTTP Request\"].json[\"data\"]; (4) Redis \"Get\" Node (Check for Duplicates): Before we do anything else, we check if we've seen this article's URL before. Operation: Get Key: {{ $json.source_link }} (5) IF Node (Is it a New Article?): This node checks the output of the Redis node. If the¬†value¬†is empty, it's a new article and we continue. If not, we stop. Condition: {{ $node[\"Redis\"].json.value }}¬†->¬†Is Empty (6) Publishing to Ghost/WordPress: If the article is new, we send it to our CMS. In your Ghost/WordPress node, you map the fields: Title: {{ $json.title }} Content: {{ $json.content_excerpt }} Featured Image: {{ $json.image_url }} (7) Redis \"Set\" Node (Save the New Article): This is the final step for each¬†new¬†article. We add its URL to Redis so it won't get processed again. Operation: Set Key: {{ $json.source_link }} Value: true That's the core of it! You just set the Schedule Trigger to run every few hours and you're good to go. Happy to answer any questions about the setup in the comments! For those who prefer video or a more detailed write-up with all the screenshots: Full Video Guide: https://youtu.be/BNkxt48plFg Detailed Blog Post: https://medium.com/@contact_34329/how-to-build-an-automated-news-aggregator-with-n8n-and-lumenfeed-api-step-by-step-guide-c63b61d4c0ce"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Keyword Research and Clustering Machine That Automates SEO Content Strategy ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1kahpiu/i_built_an_aipowered_keyword_research_and/",
          "author": "hashpanak",
          "time": "2025-04-29T06:55:11+00:00",
          "body_text": "I automated keyword research and clustering for SEO and content planning. Why? I was tired of spending hours manually digging through keywords and organizing them into clusters, only to end up with a chaotic mess. Now, I‚Äôve got a system that does it all‚Äîresearches keywords, builds clusters, and prioritizes opportunities‚Äîusing n8n and AI. It‚Äôs a game-changer for anyone looking to streamline their SEO strategy without breaking the bank. Use it for: Creating topic clusters and using it as a lead magnet for acquiring new clients. Using generated pillars and pages for social media content ideas from what people are actually searching for. Automating keyword research and clustering Saving time while uncovering high-impact topics I explain the setup here -> https://youtu.be/twFg_L9_07E I automated everything‚Äîfrom keyword discovery to clustering‚Äîusing n8n. Here‚Äôs the blueprint. This system pulls keyword data, organizes it into pillar pages and subpages, and even stores it neatly in Google Sheets, all while running on free or low-cost tools. How It Works This workflow turns a single seed keyword into a full SEO strategy in 4 stages: Automated Keyword Research (with human in the loop) Seed Keyword Start: Drop in a keyword (e.g., ‚Äún8n‚Äù or ‚ÄúSEO‚Äù), and the system uses the Google Ads API (or Data for SEO) to fetch related keywords, search volumes, competition, and trends. Replace it with other APIs if you so wish like \"DataForSEO\" Iterative Discovery: It keeps digging, researching related keywords to build a massive, relevant list‚Äîthink hundreds of terms in minutes. AI-Powered Clustering Smart Grouping: An AI model (like GPT-4.1 or Gemini) analyzes the keywords based on user intent (informational, transactional, navigational), search trends, and competition. Pillar & Subpage Structure: It creates thematic clusters‚Äîpillar pages for broad topics and subpages for specifics‚Äîthen ranks them by growth potential and ranking ease. Google Sheets Integration Central Hub: All keywords and clusters land in Google Sheets, ready for you to review or tweak. Manual Control: Spot a dud keyword? Delete it. Need to adjust a cluster? It‚Äôs all right there. n8n Automation No-Code Workflow: Built in n8n, this system runs on autopilot with scheduled triggers or manual kicks. Scalable Processing: Looping mechanisms handle huge keyword sets without breaking a sweat. Key Features No-Code Setup: Built entirely in n8n‚Äîno coding skills needed. Free/Low-Cost Tools: Uses Google Ads API (free tier) or Data for SEO ($1 signup credit and cheap alternative to Ahrefs or SEMRush). Scalable: Tackles thousands of keywords with batch processing. Customizable: Tweak AI prompts or schedules to match your needs. Why This Works Save 10+ Hours/Week: No more manual keyword grinding‚Äîautomation does the heavy lifting. Smarter SEO: AI prioritizes clusters with real data, targeting topics that rank and convert. Scale Effortlessly: Perfect for big content strategies without the overwhelm. Example: Feed it ‚Äún8n,‚Äù and it pulls keywords like ‚Äún8n automation,‚Äù ‚Äún8n workflows,‚Äù and ‚Äún8n integrations.‚Äù The AI then builds clusters like ‚Äún8n Automation Tutorials‚Äù (pillar) with subpages like ‚ÄúConnecting n8n to Salesforce‚Äù‚Äîcomplete with primary/secondary keywords and ranking priorities. PS: This replaces tools like Ahrefs ($99+/mo) or SEMrush ($129+/mo) with a free AI-powered system. üî• Turn your SEO strategy into a 24/7 content-planning engine. üî• I explain the setup here -> https://youtu.be/twFg_L9_07E https://preview.redd.it/bxjl5j430qxe1.png?width=1704&format=png&auto=webp&s=53e7bc67c1f76f46a3fbdc76b2acdef1bbc4ac51 https://preview.redd.it/xr50mgn40qxe1.png?width=2344&format=png&auto=webp&s=04768c89f7bc0947eac2fe1a1976c30634273a38 https://preview.redd.it/sobyfvw50qxe1.png?width=2716&format=png&auto=webp&s=bb7994483791ff33a21ec3e44aa766e2d6560388 https://preview.redd.it/r8zawm170qxe1.png?width=2886&format=png&auto=webp&s=a3f5803b70120d2c34de4d8547b264a2e84ca1ae https://reddit.com/link/1kahpiu/video/zz4xrkxg0qxe1/player"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1l0gm98/try_this_podcast_generation_workflow_i_built/",
          "author": "teezworkspace",
          "time": "2025-06-01T04:49:03+00:00",
          "body_text": "Hey everyone, I built out this workflow in n8n to help me intake the highest quality AI content in the most digestible format for myself; audio. In short, the RSS Feed scrapes three (could be more if you want) of the most reputable sources in the AI space, goes through a Code node for scoring (looks for the highest quality content: whitepapers, research papers, etc) and calls AutoContentAPI (NOT free, but a NotebookLM alternative nonetheless) via HTTP Request and generates podcasts on the respective material and sends it to me via Telegram and Gmail, and updates my Google Drive as well. Provided below is a screenshot and the downloadable JSON in case anyone would like to try it. Feel free to DM me if you have any questions. I'm also not too familiar with how to share files on Reddit so the option I settled on was placing the JSON in this code block, hopefully that works? Again, feel free to DM me if you'd like to try it and I should be able to share it to you directly as downloadable JSON for you to import into n8n. {\n  \"name\": \"AI Podcast Generation (AutoContentAPI)\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"triggerTimes\": {\n          \"item\": [\n            {}\n          ]\n        }\n      },\n      \"name\": \"Schedule: Weekly Learning Run\",\n      \"type\": \"n8n-nodes-base.cron\",\n      \"typeVersion\": 1,\n      \"position\": [\n        -1820,\n        -200\n      ],\n      \"id\": \"7a78b92e-d75b-4cab-bf0c-6a9fd41c5683\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.url }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.rssFeedRead\",\n      \"typeVersion\": 1.1,\n      \"position\": [\n        -920,\n        -180\n      ],\n      \"id\": \"2a012472-2e03-451c-80d7-202d159c3959\",\n      \"name\": \"RSS Read\",\n      \"onError\": \"continueRegularOutput\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"return [\\n  { json: { url: \\\"https://huggingface.co/blog/feed\\\" } },\\n  { json: { url: \\\"https://machinelearningmastery.com/blog/feed/\\\" } },\\n  { json: { url: \\\"https://blog.tensorflow.org/feeds/posts/default\\\" } }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -1620,\n        -200\n      ],\n      \"id\": \"758b3629-43b5-4330-a1a0-2c1aabdfdf1e\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"const keywords = [\\n  \\\"whitepaper\\\", \\\"research\\\", \\\"study\\\", \\\"publication\\\", \\\"paper\\\", \\\"preprint\\\", \\\"abstract\\\",\\n  \\\"benchmark\\\", \\\"evaluation\\\", \\\"methodology\\\", \\\"experiment\\\", \\\"analysis\\\", \\\"dataset\\\",\\n  \\\"LLM\\\", \\\"GPT\\\", \\\"transformer\\\", \\\"language model\\\", \\\"fine-tuning\\\", \\\"pretraining\\\"\\n];\\n\\nconst now = new Date();\\nconst weekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000);\\nconst monthStart = new Date(now.getFullYear(), now.getMonth(), 1);\\nconst seenLinks = new Set();\\n\\n// Domains not supported by AutoContentAPI on free tier\\nconst blockedDomains = [\\n  \\\"arxiv.org\\\",\\n  \\\"ieeexplore.ieee.org\\\",\\n  \\\"springer.com\\\",\\n  \\\"sciencedirect.com\\\",\\n  \\\"dl.acm.org\\\"\\n];\\n\\n// Score and parse\\nlet scored = items.map(item => {\\n  const title = (item.json.title || \\\"\\\").toLowerCase();\\n  const description = (item.json.description || item.json.contentSnippet || item.json.content || \\\"\\\").toLowerCase();\\n  const link = item.json.link || item.json.url || \\\"\\\";\\n  const pubDateStr = item.json.pubDate || item.json.date || item.json.isoDate || \\\"\\\";\\n  const pubDate = pubDateStr && !isNaN(Date.parse(pubDateStr)) ? new Date(pubDateStr) : null;\\n\\n  let score = 0;\\n  keywords.forEach(keyword => {\\n    if (title.includes(keyword)) score += 2;\\n    if (description.includes(keyword)) score += 1;\\n  });\\n\\n  return {\\n    json: {\\n      title: item.json.title,\\n      link,\\n      pubDate: pubDateStr,\\n      pubDateObject: pubDate,\\n      content: item.json.content || item.json.contentSnippet || \\\"\\\",\\n      score\\n    }\\n  };\\n});\\n\\n// Filter: only allow whitelisted, non-duplicate, recent items\\nlet filtered = scored.filter(item =>\\n  item.json.score >= 2 &&\\n  item.json.pubDateObject instanceof Date &&\\n  !isNaN(item.json.pubDateObject) &&\\n  item.json.link &&\\n  !seenLinks.has(item.json.link) &&\\n  !blockedDomains.some(domain => item.json.link.includes(domain)) &&\\n  seenLinks.add(item.json.link)\\n);\\n\\n// Prioritize items from the last 7 days\\nlet pastWeek = filtered.filter(item => item.json.pubDateObject >= weekAgo);\\n\\n// If none found, fall back to items from this calendar month\\nif (pastWeek.length === 0) {\\n  pastWeek = filtered.filter(item =>\\n    item.json.pubDateObject >= monthStart && item.json.pubDateObject <= now\\n  );\\n}\\n\\n// Sort by score descending\\npastWeek.sort((a, b) => b.json.score - a.json.score);\\n\\n// Return top 3\\nreturn pastWeek.slice(0, 3);\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -700,\n        -180\n      ],\n      \"id\": \"3ffafffd-f20a-4197-a09c-b08dca6099a6\",\n      \"name\": \"Whitepaper Filter\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"0e2fb51a-8995-4b8d-bb41-ea78cf5c1904\",\n              \"name\": \"url\",\n              \"value\": \"={{ $json.url }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -1120,\n        -180\n      ],\n      \"id\": \"d0115844-b5fb-489c-83fe-4d2fbd11b7b9\",\n      \"name\": \"Edit Fields\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"ca3acbb3-9375-4335-b8b2-a951e72dff76\",\n              \"name\": \"request_id\",\n              \"value\": \"={{ $json.request_id }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        120,\n        -160\n      ],\n      \"id\": \"06ef9efc-88b3-470a-b7dd-b615e7700d09\",\n      \"name\": \"Extract Request ID\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"=https://api.autocontentapi.com/content/status/{{$json[\\\"request_id\\\"]}}\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer 5b62e1aa-54d0-4319-81e8-93320d9a58ef\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        320,\n        -160\n      ],\n      \"id\": \"50db4ed9-e412-48bd-b41f-1a764be41c74\",\n      \"name\": \"GET Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://api.autocontentapi.com/Content/Create\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer YOUR<API>KEY\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"raw\",\n        \"rawContentType\": \"application/json\",\n        \"body\": \"={{ \\n  JSON.stringify({\\n    resources: [\\n      {\\n        content: $json[\\\"link\\\"],\\n        type: \\\"website\\\"\\n      }\\n    ],\\n    text: \\\"Create a podcast summary of this article in a conversational, engaging tone.\\\",\\n    outputType: \\\"audio\\\"\\n  })\\n}}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -140,\n        -160\n      ],\n      \"id\": \"8ae2fffa-03ab-4053-9db0-388de34b5287\",\n      \"name\": \"Generate Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"9f869aa6-11f0-4664-8d16-d06a6ec52c9f\",\n              \"leftValue\": \"={{ $json.status }}\",\n              \"rightValue\": 100,\n              \"operator\": {\n                \"type\": \"number\",\n                \"operation\": \"equals\"\n              }\n            }\n          ],\n          \"combinator\": \"or\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        520,\n        -160\n      ],\n      \"id\": \"2785e08c-f859-4fa2-b752-9f114e6617bc\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"teezworkspace@gmail.com\",\n        \"subject\": \"={{ $json.audio_title }}\",\n        \"message\": \"={{ $json.audio_title }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"attachmentsUi\": {\n            \"attachmentsBinary\": [\n              {\n                \"property\": \"audio\"\n              }\n            ]\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        1080,\n        80\n      ],\n      \"id\": \"f07b9a91-aa2d-43a9-9095-41497180454f\",\n      \"name\": \"Send Audio to Email\",\n      \"webhookId\": \"0ff65219-e34a-4ad4-b600-f7238569c92d\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"\",\n          \"name\": \"Terry's Gmail\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"inputDataFieldName\": \"audio\",\n        \"name\": \"={{ $json.audio_title }}\",\n        \"driveId\": {\n          \"__rl\": true,\n          \"value\": \"My Drive\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\"\n        },\n        \"folderId\": {\n          \"__rl\": true,\n          \"value\": \"1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Weekly AI Research Audio\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"typeVersion\": 3,\n      \"position\": [\n        1080,\n        -120\n      ],\n      \"id\": \"5d9eec4c-f596-48f0-a81e-5f1bc37a082b\",\n      \"name\": \"Upload Audio Folder\",\n      \"credentials\": {\n        \"googleDriveOAuth2Api\": {\n          \"id\": \"\",\n          \"name\": \"Terry Google Drive\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"sendAudio\",\n        \"chatId\": \"6018770135\",\n        \"binaryData\": true,\n        \"binaryPropertyName\": \"audio\",\n        \"additionalFields\": {\n          \"caption\": \"={{ $json.audio_title }}\",\n          \"title\": \"={{ $json.audio_title }}\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.telegram\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        1080,\n        -340\n      ],\n      \"id\": \"6f21e927-a79b-48f3-a5ff-8dd9d460916f\",\n      \"name\": \"Send Audio to Telegram\",\n      \"webhookId\": \"97f48ead-3e73-4928-a555-455722196acc\",\n      \"credentials\": {\n        \"telegramApi\": {\n          \"id\": \"\",\n          \"name\": \"AutoContentAPI Bot \"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"batchSize\": 15,\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -1380,\n        -200\n      ],\n      \"id\": \"fb9a4a7c-2aba-4a17-89e4-6e856bd23d0a\",\n      \"name\": \"URL Loop\"\n    },\n    {\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -480,\n        -180\n      ],\n      \"id\": \"9ce3486f-0bd6-45fa-bdcc-392c72bfff97\",\n      \"name\": \"Podcast Gen Loop\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.audio_url }}\",\n        \"options\": {\n          \"response\": {\n            \"response\": {\n              \"responseFormat\": \"file\",\n              \"outputPropertyName\": \"audio\"\n            }\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        760,\n        -180\n      ],\n      \"id\": \"0afdf799-a612-4a07-a2e5-c65b262ef12e\",\n      \"name\": \"Download Audio\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Schedule: Weekly Learning Run\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"RSS Read\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Whitepaper Filter\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Whitepaper Filter\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Edit Fields\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"RSS Read\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Request ID\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"GET Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Extract Request ID\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"GET Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Download Audio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upload Audio Folder\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"URL Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Edit Fields\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Podcast Gen Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Generate Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Download Audio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Audio to Telegram\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Upload Audio Folder\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Send Audio to Email\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": false,\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"57ddc431-4059-4b0e-92dc-325c7296ac9a\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"f9bd58af1591f515777c160d7518c3e5cf0ad788d4a4c3831380e58e9febdfa6\"\n  },\n  \"id\": \"Ece8XCZeyPq6R0Uv\",\n  \"tags\": []\n} https://preview.redd.it/7nryas97v84f1.png?width=2694&format=png&auto=webp&s=1bb4c966619c7cfd520ab6bc47c1dbbd439871e6"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1ksnvdm/power_up_your_n8n_workflows_with_diverse_content/",
          "author": "PostGoo",
          "time": "2025-05-22T11:02:28+00:00",
          "body_text": "I run an Instagram account that publishes short form videos each week that cover the top AI news stories. I used to monitor twitter to write these scripts by hand, but it ended up becoming a huge bottleneck and limited the number of videos that could go out each week. In order to solve this, I decided to automate this entire process by building a system that scrapes the top AI news stories off the internet each day (from Twitter / Reddit / Hackernews / other sources), saves it in our data lake, loads up that text content to pick out the top stories and write video scripts for each. This has saved a ton of manual work having to monitor news sources all day and let‚Äôs me plug the script into ElevenLabs / HeyGen to produce the audio + avatar portion of each video. One of the recent videos we made this way got over 1.8 million views on Instagram and I‚Äôm confident there will be more hits in the future. It‚Äôs pretty random on what will go viral or not, so my plan is to take enough ‚Äúshots on goal‚Äù and continue tuning this prompt to increase my changes of making each video go viral. Here‚Äôs the workflow breakdown 1. Data Ingestion and AI News Scraping The first part of this system is actually in a separate workflow I have setup and running in the background. I actually made another reddit post that covers this in detail so I‚Äôd suggestion you check that out for the full breakdown + how to set it up. I‚Äôll still touch the highlights on how it works here: The main approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. Right now, there are around ~13 news sources that I have setup to pull stories from every single day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each  story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. 2. Loading up and formatting the scraped news stories Once the data lake / news storage has plenty of scraped stories saved for the day, we are able to get into the main part of this automation. This kicks off off with a scheduled trigger that runs at 7pm each day and will: Search S3 bucket for all markdown files and tweets that were scraped for the day by using a prefix filter Download and extract text content from each markdown file Bundle everything into clean text blocks wrapped in XML tags for better LLM processing - This allows us to include important metadata with each story like the source it came from, links found on the page, and include engagement stats (for tweets). 3. Picking out the top stories Once everything is loaded and transformed into text, the automation moves on to executing a prompt that is responsible for picking out the top 3-5 stories suitable for an audience of AI enthusiasts and builder‚Äôs. The prompt is pretty big here and highly customized for my use case so you will need to make changes for this if you are going forward with implementing the automation itself. At a high level, this prompt will: Setup the main objective Provides a ‚Äúcuration framework‚Äù to follow over the list of news stories that we are passing int Outlines a process to follow while evaluating the stories Details the structured output format we are expecting in order to avoid getting bad data back ```jsx\n<objective>\nAnalyze the provided daily digest of AI news and select the top 3-5 stories most suitable for short-form video content. Your primary goal is to maximize audience engagement (likes, comments, shares, saves). The date for today's curation is {{ new Date(new Date($('schedule_trigger').item.json.timestamp).getTime() + (12 * 60 * 60 * 1000)).format(\"yyyy-MM-dd\", \"America/Chicago\") }} . Use this to prioritize the most recent and relevant news. You MUST avoid selecting stories that are more than 1 day in the past for this date.\n</objective> <curation_framework>\nTo identify winning stories, apply the following virality principles. A story must have a strong \"hook\" and fit into one of these categories: Impactful: A major breakthrough, industry-shifting event, or a significant new model release (e.g., \"OpenAI releases GPT-5,\" \"Google achieves AGI\"). Practical: A new tool, technique, or application that the audience can use now (e.g., \"This new AI removes backgrounds from video for free\"). Provocative: A story that sparks debate, covers industry drama, or explores an ethical controversy (e.g., \"AI art wins state fair, artists outraged\"). Astonishing: A \"wow-factor\" demonstration that is highly visual and easily understood (e.g., \"Watch this robot solve a Rubik's Cube in 0.5 seconds\"). Hard Filters (Ignore stories that are): * Ad-driven: Primarily promoting a paid course, webinar, or subscription service.\n* Purely Political: Lacks a strong, central AI or tech component.\n* Substanceless: Merely amusing without a deeper point or technological significance.\n</curation_framework> <hook_angle_framework>\nFor each selected story, create 2-3 compelling hook angles that could open a TikTok or Instagram Reel. Each hook should be designed to stop the scroll and immediately capture attention. Use these proven hook types: Hook Types: - Question Hook: Start with an intriguing question that makes viewers want to know the answer\n- Shock/Surprise Hook: Lead with the most surprising or counterintuitive element\n- Problem/Solution Hook: Present a common problem, then reveal the AI solution\n- Before/After Hook: Show the transformation or comparison\n- Breaking News Hook: Emphasize urgency and newsworthiness\n- Challenge/Test Hook: Position as something to try or challenge viewers\n- Conspiracy/Secret Hook: Frame as insider knowledge or hidden information\n- Personal Impact Hook: Connect directly to viewer's life or work Hook Guidelines: - Keep hooks under 10 words when possible\n- Use active voice and strong verbs\n- Include emotional triggers (curiosity, fear, excitement, surprise)\n- Avoid technical jargon - make it accessible\n- Consider adding numbers or specific claims for credibility\n</hook_angle_framework> <process>\n1. Ingest: Review the entire raw text content provided below.\n2. Deduplicate: Identify stories covering the same core event. Group these together, treating them as a single story. All associated links will be consolidated in the final output.\n3. Select & Rank: Apply the Curation Framework to select the 3-5 best stories. Rank them from most to least viral potential.\n4. Generate Hooks: For each selected story, create 2-3 compelling hook angles using the Hook Angle Framework .\n</process> <output_format>\nYour final output must be a single, valid JSON object and nothing else. Do not include any text, explanations, or markdown formatting like ` json before or after the JSON object. The JSON object must have a single root key, stories , which contains an array of story objects. Each story object must contain the following keys:\n- title (string): A catchy, viral-optimized title for the story.\n- summary (string): A concise, 1-2 sentence summary explaining the story's hook and why it's compelling for a social media audience.\n- hook_angles (array of objects): 2-3 hook angles for opening the video. Each hook object contains:\n  - hook (string): The actual hook text/opening line\n  - type (string): The type of hook being used (from the Hook Angle Framework)\n  - rationale (string): Brief explanation of why this hook works for this story\n- sources (array of strings): A list of all consolidated source URLs for the story. These MUST be extracted from the provided context. You may NOT include URLs here that were not found in the provided source context. The url you include in your output MUST be the exact verbatim url that was included in the source material. The value you output MUST be like a copy/paste operation. You MUST extract this url exactly as it appears in the source context, character for character. Treat this as a literal copy-paste operation into the designated output field. Accuracy here is paramount; the extracted value must be identical to the source value for downstream referencing to work. You are strictly forbidden from creating, guessing, modifying, shortening, or completing URLs. If a URL is incomplete or looks incorrect in the source, copy it exactly as it is. Users will click this URL; therefore, it must precisely match the source to potentially function as intended. You cannot make a mistake here.\n``` After I get the top 3-5 stories picked out from this prompt, I share those results in slack so I have an easy to follow trail of stories for each news day. 4. Loop to generate each script For each of the selected top stories, I then continue to the final part of this workflow which is responsible for actually writing the TikTok / IG Reel video scripts. Instead of trying to 1-shot this and generate them all at once, I am iterating over each selected story and writing them one by one. Each of the selected stories will go through a process like this: Start by additional sources from the story URLs to get more context and primary source material Feeds the full story context into a viral script writing prompt Generates multiple different hook options for me to later pick from Creates two different 50-60 second scripts optimized for talking-head style videos (so I can pick out when one is most compelling) Uses examples of previously successful scripts to maintain consistent style and format Shares each completed script in Slack for me to review before passing off to the video editor. Script Writing Prompt ```jsx\nYou are a viral short-form video scriptwriter for David Roberts, host of \"The Recap.\" Follow the workflow below each run to produce two 50-60-second scripts (140-160 words). Before you write your final output, I want you to closely review each of the provided REFERENCE_SCRIPTS and think deeploy about what makes them great. Each script that you output must be considered a great script. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 1 ‚Äì Ideate ‚Ä¢ Generate five distinct hook sentences (‚â§ 12 words each) drawn from the STORY_CONTEXT. STEP 2 ‚Äì Reflect & Choose ‚Ä¢ Compare hooks for stopping power, clarity, curiosity. ‚Ä¢ Select the two strongest hooks (label TOP HOOK 1 and TOP HOOK 2). ‚Ä¢ Do not reveal the reflection‚Äîonly output the winners. STEP 3 ‚Äì Write Two Scripts For each top hook, craft one flowing script ‚âà 55 seconds (140-160 words). Structure (no internal labels): ‚Äì Open with the chosen hook. ‚Äì One-sentence explainer. ‚Äì 5-7 rapid wow-facts / numbers / analogies. ‚Äì 2-3 sentences on why it matters or possible risk. ‚Äì Final line = a single CTA ‚Ä¢ Ask viewers to comment with a forward-looking question or ‚Ä¢ Invite them to follow The Recap for more AI updates. Style: confident insider, plain English, light attitude; active voice, present tense; mostly ‚â§ 12-word sentences; explain unavoidable jargon in ‚â§ 3 words. OPTIONAL POWER-UPS (use when natural) ‚Ä¢ Authority bump ‚Äì Cite a notable person or org early for credibility. ‚Ä¢ Hook spice ‚Äì Pair an eye-opening number with a bold consequence. ‚Ä¢ Then-vs-Now snapshot ‚Äì Contrast past vs present to dramatize change. ‚Ä¢ Stat escalation ‚Äì List comparable figures in rising or falling order. ‚Ä¢ Real-world fallout ‚Äì Include 1-3 niche impact stats to ground the story. ‚Ä¢ Zoom-out line ‚Äì Add one sentence framing the story as a systemic shift. ‚Ä¢ CTA variety ‚Äì If using a comment CTA, pose a provocative question tied to stakes. ‚Ä¢ Rhythm check ‚Äì Sprinkle a few 3-5-word sentences for punch. OUTPUT FORMAT  (return exactly this‚Äîno extra commentary, no hashtags) HOOK OPTIONS ‚Ä¢ Hook 1 ‚Ä¢ Hook 2 ‚Ä¢ Hook 3 ‚Ä¢ Hook 4 ‚Ä¢ Hook 5 TOP HOOK 1 SCRIPT [finished 140-160-word script] TOP HOOK 2 SCRIPT [finished 140-160-word script] REFERENCE_SCRIPTS <Pass in example scripts that you want to follow and the news content loaded from before>\n``` 5. Extending this workflow to automate further So right now my process for creating the final video is semi-automated with human in the loop step that involves us copying the output of this automation into other tools like HeyGen to generate the talking avatar using the final script and then handing that over to my video editor to add in the b-roll footage that appears on the top part of each short form video. My plan is to automate this further over time by adding another human-in-the-loop step at the end to pick out the script we want to go forward with ‚Üí Using another prompt that will be responsible for coming up with good b-roll ideas at certain timestamps in the script ‚Üí use a videogen model to generate that b-roll ‚Üí finally stitching it all together with json2video. Depending on your workflow and other constraints, It is really up to you how far you want to automate each of these steps. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=7WsmUlbyjMM The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need help: workflow automations needed within the realm of content marketing",
          "link": "https://old.reddit.com/r/n8n/comments/1lzz02o/how_much_is_my_workflow_worth/",
          "author": "Nocare420",
          "time": "2025-07-14T21:22:57+00:00",
          "body_text": "With the new ChatGPT-4o image generator, I've been considering whether I could move away from actively using LinkedIn. The constant need to create organic content and post daily to gain followers or connect with people feels like a significant time investment. Providing free content and value every day is demanding, and I'm looking for a more efficient approach.\nMy idea is to use Gemini 2.5 for writing the text and ChatGPT-4o for generating images. I have a template in mind, and I'd like a workflow where the AI generates a LinkedIn post based on this template, sends it to me for approval, and only posts if I give it the go-ahead. However, my current setup doesn't integrate with ChatGPT-4o, and I'm unsure how to create this workflow. (And i would like to add a node  where i send them like every week or every month a linkedin post or linkedin profile to inspire from it and post similar like him)\nAny help or suggestions on how to integrate these tools and automate this process would be greatly appreciated)\nP.S. I found a helpful YouTube channel https://youtu.be/o_oSYl6gSO8?si=rj7hAaFtimFn4iIk that inspired this idea, and the template I'm referring to can be found there.\"\nIf i did find  a way how this is created i will post the template next post :)"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "üöÄ New Workflow added of Short Video Creation üé•",
          "link": "https://old.reddit.com/r/n8n/comments/1j7rod6/n8n_workflow_automate_ugc_video_creation_from_any/",
          "author": "ProEditor69",
          "time": "2025-03-10T05:38:26+00:00",
          "body_text": "after going through over 1,000 n8n templates , I‚Äôm convinced most of them are either half-baked, broken, or riddled with bugs. So many look good on GitHub or in the n8n library, but once you import them‚Ä¶ missing nodes, failed integrations, or completely outdated logic. It‚Äôs like people just export random experiments and call them ‚Äúproduction-ready.‚Äù Anyway, after way too many months of testing, debugging, and cursing at my screen, I found a few actually solid workflows worth using especially if you're doing lead gen, enrichment, or routing work. Firstly here are OG n8n Templates these are Awesome and works enescingoz/awesome-n8n-templates ‚Äì A well-curated repo with real-world workflows, including one for lead scoring and CRM integration. https://github.com/wassupjay/n8n-free-templates Bonus : How I Vet GitHub Repos Automatically After getting burned a few times, I stopped trusting stars or even READMEs. So now, I run a quick reputation check before using any GitHub template. Specifically: Is it active? Are people talking about it on Reddit or X ? Are there better forks or open issues? Does it look like a side project someone forgot about? To automate this, I ask BhindiAI to scan Reddit and X for the most relevant discussions about a repo, summarize the general vibe (positive, meh, sketchy), and extract key takeaways. It then logs that info repo link + commentary + score straight into my Google Sheet. That way I‚Äôm not wasting time setting up stuff the internet already knows is broken. Reddit and X Discussions are helpful to know what is a bs better than github stars. The worst part? Some templates had security vulnerabilities that weren't obvious until I dug deeper. One workflow was storing API keys in plain text comments, and another was making unencrypted calls to external services. I now have a strict checklist I run through before implementing anything in production. I've also started forking promising templates immediately and maintaining my own versions. Too many times I've seen maintainers abandon projects or push breaking changes without warning. Now I control my own destiny with these workflows. The community aspect is real though - I've found some of my best templates through Reddit and random Twitter threads where people share their actual working setups. The official n8n community forums are hit or miss, but when you find someone who's actually using these workflows at scale, their insights are gold."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Power Up Your n8n Workflows with Diverse Content: LumenFeed.com Offers 10k Free API Requests/Month!",
          "link": "https://old.reddit.com/r/n8n/comments/1ltsocy/i_have_reviewed_over_1000_n8n_templates_heres_how/",
          "author": "Valuable_Simple3860",
          "time": "2025-07-07T12:19:18+00:00",
          "body_text": "I run an AI Tools directory and a large growth opportunity for us is writing \"roundup\" articles that highlight the best AI tools in a particular software category such as \"Best AI Image Editors\", \"Best AI Sales Tools\" , and many more. The problem is, this content takes a ton of time to research, write, and build out rankings for each tool. With the major improvements that have rolled out to \"Deep Research\" tools, we decide to see if we could leverage the output of these deep research reports, feed it into our own prompts, and scale out the process of creating great content. This automation is what we were able to build and I'm happy to report this process has allowed us to significantly scale up our content creation process. I'm pretty amazed at how much depth Chat GPT's deep research tool is able to provide and I think in many cases it is able to provide better insights than if we were doing this by hand. Here's the automation breakdown 1. Trigger / Inputs We use a simple form trigger for this automation that takes in some details specific to the structure of our website such as the slug for the category and the target audience for this article We also accept an optional parameter here for any tools that we know we want to be included in the \"Best Tools by Category\" roundup in order to give us a bit more control of the final output. 2. Building Necessary Context The next few nodes in here our very specific to our own website. We get started by making an HTTP request into our own API to get details about the category we are going to write about. After that, we make another HTTP request to our own API to get a list of all tools that are inside the category we are going to write about. Depending on the type of article you are going to write, this initial \"build context\" step is likely going to look very different for you so be sure you start with the end vision in mind of what you are trying to create and then work backwards from there to build the context you need. 3. Building The Deep Research Prompt The next step here is to take all of that context we just gathered and bundle it together into a prompt that we will feed into a deep research tool like Chat GPT deep research or Gemini Deep research. In our case, we build a prompt that focuses on telling the deep research tool to focus on picking out the top 6-8 tools in the provided category with a strong focus on social signals and real reviews left by users of the tool. Once again, this is going to be something that you will need to customize for your own use case. You should expect to spend a lot of time iterating on this prompt to get it in a good spot. I think we probably went through 30+ iterations of this prompt until we were happy with the deep research output. 4. Human-In-The-Loop Step Once the prompt is generated, we pause the automation with a sendAndWait slack message that will post the full prompt in slack. We then copy that prompt into Chat GPT deep research and let that run until we get the deep research report back After the deep research process finishes, we click the \"Add Result\" button in slack which will let us paste in the output of deep research into a form and continue with the automation. The Chat GPT deep research output is just so good so we opted to go this way. 5. Generating Article Content With the deep research report content now inside our n8n execution, we are then able to proceed with generation our article. We opted to take an approach where we build this section by section (like lego block) instead of trying to one-shot the entire article. We found this to be more effective and result in better output. Here's how we split it up: Intro paragraph and article meta description get written first in two separate prompts After that, we write another paragraph that details the ranking criteria that was used to pick out the top tools and how they were evaluated We then move on to writing the main content for each of the selected tools. We have a single prompt and a format we follow for each tool, so we loop over the selected tools and generate this content for each. Lastly, we take the output for each of these sections and pass that into a final prompt that is responsible for writing a conclusion. 6. Pushing To Our CMS In the final section of this automation, we take the output of each of the nodes from before and bundle it up into into a format that can easily get inserted into our CMS. We use Sanity CMS for hosting all of our content which doesn't have an out of the box API that we can just push content too, so we opted to create our own API endpoint to handle this for us. If you are hosting your content on something like Webflow / WordPress / another CMS, you should be able to replace the node here with an HTTP request that publishes your content to replicate this behavior. Workflow Link + Other Resources Github workflow link for this automation: https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json YouTube video that walks through this workflow step-by-step: https://youtu.be/uDrkgEuEOBA Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I analysed 2,000+ n8n workflows and this is what I learned",
          "link": "https://old.reddit.com/r/n8n/comments/1l1f6n8/i_analysed_2000_n8n_workflows_and_this_is_what_i/",
          "author": "Majestic-Fix-3857",
          "time": "2025-06-02T11:05:59+00:00",
          "body_text": "So I downloaded 2,050 public n8n workflows and then used claude opus 4 to help me vibe code my way through a detailed analysis. I used cursor as my code running tool, ran the claude scripts over the 2,000 JSON files, created a report, and then summarised into the below actionable doc Here is a video walkthrough of me visually going over the insights + also exploring the recommendations on the n8n canvas: https://youtu.be/BvBa_npD4Og Or if you just wanna read, here is the claude actionable report (hope you legends enjoy and find useful) -- n8n Workflow Best Practices Guide Learnings from Analyzing 2,000+ Production Workflows This guide is based on insights gathered from analyzing 2,050 production n8n workflows containing 29,363 nodes. It highlights common patterns, critical issues, and best practices for building robust, secure, and maintainable automation workflows. üìä Executive Summary Our analysis revealed critical gaps in error handling (97% of workflows lack it), security vulnerabilities (320 public webhooks without auth), and efficiency issues (7% contain unused nodes). This guide provides actionable recommendations to address these issues and build better workflows. Key Statistics: 2,050 workflows analyzed 29,363 total nodes 14.3 average nodes per workflow 97% lack error handling 472 security vulnerabilities found 34.7% are AI/ML workflows üö® Critical Issue #1: Error Handling (97% Gap) The Problem Only 62 out of 2,050 workflows (3%) have any error handling mechanism. This means when things fail, workflows silently break without notification or recovery. Best Practices 1. Always Use Error Triggers // Add an Error Trigger node at the beginning of every workflow\n// Connect it to a notification system (Email, Slack, etc.)\nError Trigger ‚Üí Format Error Message ‚Üí Send Notification 2. Implement Node-Level Error Handling For critical nodes (HTTP requests, database operations, API calls): Enable \"Continue On Fail\" for non-critical operations Add retry logic with exponential backoff Set appropriate timeout values 3. Error Handling Template Start ‚Üí Error Trigger ‚Üí Error Handler\n  ‚Üì\nMain Workflow Logic\n  ‚Üì\nCritical Operation (with retry: 3, delay: 1000ms)\n  ‚Üì\nSuccess Path / Error Path 4. Monitoring Pattern Log all errors to a centralized system Include workflow name, node name, error message, and timestamp Set up alerts for repeated failures üîí Critical Issue #2: Security Vulnerabilities The Problems 320 public webhooks without authentication 152 unsecure HTTP calls 3 workflows with hardcoded secrets Security Best Practices 1. Webhook Security // Always enable authentication on webhooks\nWebhook Settings:\n  - Authentication: Header Auth / Basic Auth\n  - Use HTTPS only\n  - Implement IP whitelisting where possible\n  - Add rate limiting 2. Secure API Communications Never use HTTP - always use HTTPS Store credentials in n8n's credential system, never hardcode Use OAuth2 when available (694 workflows do this correctly) Implement API key rotation policies 3. Authentication Methods (from most to least secure) OAuth2 - Use for major integrations API Keys - Store securely, rotate regularly Basic Auth - Only when necessary, always over HTTPS No Auth - Never for public endpoints 4. Secret Management Checklist [ ] No hardcoded API keys in Code/Function nodes [ ] All credentials stored in n8n credential manager [ ] Regular credential audit and rotation [ ] Environment-specific credentials (dev/staging/prod) üéØ Critical Issue #3: Workflow Efficiency The Problems 144 workflows with unused nodes (264 total unused nodes) 133 workflows with API calls inside loops 175 workflows with redundant transformations Efficiency Best Practices 1. Clean Architecture Input ‚Üí Validate ‚Üí Transform ‚Üí Process ‚Üí Output\n         ‚Üì (fail)\n      Error Handler 2. Avoid Common Anti-Patterns ‚ùå Bad: API in Loop Loop ‚Üí HTTP Request ‚Üí Process Each ‚úÖ Good: Batch Processing Collect Items ‚Üí Single HTTP Request (batch) ‚Üí Process Results 3. Node Optimization Remove unused nodes (7% of workflows have them) Combine multiple Set nodes into one Use Code node for complex transformations instead of chaining Set nodes Cache API responses when possible 4. Performance Guidelines Average workflow should complete in < 10 seconds Use Split In Batches for large datasets Implement parallel processing where possible (only 4.8% currently do) Add progress logging for long-running workflows ü§ñ AI/ML Workflow Best Practices (34.7% of workflows) Common Patterns Observed 346 agent-based workflows 267 multi-model workflows 201 with memory systems 0 with vector databases (RAG pattern opportunity) AI Workflow Best Practices 1. Prompt Engineering // Structure prompts with clear sections\nconst prompt = `\nSystem: ${systemContext}\nContext: ${relevantData}\nTask: ${specificTask}\nFormat: ${outputFormat}\n`; 2. Cost Optimization Use GPT-3.5 for simple tasks, GPT-4 for complex reasoning Implement caching for repeated queries Batch similar requests Monitor token usage 3. Agent Workflow Pattern Trigger ‚Üí Context Builder ‚Üí Agent (with tools) ‚Üí Output Parser ‚Üí Response\n                                ‚Üì\n                          Memory System 4. Error Handling for AI Handle rate limits gracefully Implement fallback models Validate AI outputs Log prompts and responses for debugging üìã Workflow Organization Best Practices The Problem 74.7% of workflows categorized as \"general\" Poor documentation and organization Organization Best Practices 1. Naming Conventions [Category]_[Function]_[Version]\nExamples:\n- Sales_LeadScoring_v2\n- HR_OnboardingAutomation_v1\n- DataSync_Salesforce_Daily_v3 2. Tagging Strategy Essential tags to use: Environment: prod, staging, dev Category: sales, hr, finance, it-ops Frequency: real-time, hourly, daily, weekly Status: active, testing, deprecated 3. Documentation with Sticky Notes The #1 most used node (7,024 times) - use it well: Document complex logic Explain business rules Note dependencies Include contact information 4. Workflow Structure üìù Sticky Note: Workflow Overview\n    ‚Üì\n‚öôÔ∏è Configuration & Setup\n    ‚Üì\nüîÑ Main Process Logic\n    ‚Üì\n‚úÖ Success Handling | ‚ùå Error Handling\n    ‚Üì\nüìä Logging & Monitoring üîÑ Common Node Sequences (Best Patterns) Based on the most frequent node connections: 1. Data Transformation Pattern Set ‚Üí HTTP Request (379 occurrences) Best for: Preparing data before API calls 2. Chained API Pattern HTTP Request ‚Üí HTTP Request (350 occurrences) Best for: Sequential API operations (auth ‚Üí action) 3. Conditional Processing If ‚Üí Set (267 occurrences)\nSwitch ‚Üí Set (245 occurrences) Best for: Data routing based on conditions 4. Data Aggregation Set ‚Üí Merge (229 occurrences) Best for: Combining multiple data sources üõ°Ô∏è Security Checklist for Every Workflow Before Deployment [ ] No hardcoded credentials [ ] All webhooks have authentication [ ] All external calls use HTTPS [ ] Sensitive data is encrypted [ ] Access controls are implemented [ ] Error messages don't expose sensitive info Regular Audits [ ] Review webhook authentication monthly [ ] Rotate API keys quarterly [ ] Check for unused credentials [ ] Verify HTTPS usage [ ] Review access logs üìà Optimization Opportunities 1. For Complex Workflows (17.5%) Break into sub-workflows Use Execute Workflow node Implement proper error boundaries Add performance monitoring 2. For Slow Workflows Identify bottlenecks (usually API calls) Implement caching Use batch operations Add parallel processing 3. For Maintenance Remove unused nodes (found in 7% of workflows) Consolidate redundant operations Update deprecated node versions Document business logic üéØ Top 10 Actionable Recommendations Implement Error Handling - Add Error Trigger to all production workflows Secure Webhooks - Enable authentication on all 320 public webhooks Use HTTPS - Migrate 152 HTTP calls to HTTPS Clean Workflows - Remove 264 unused nodes Batch API Calls - Refactor 133 workflows with APIs in loops Add Monitoring - Implement centralized logging Document Workflows - Use Sticky Notes effectively Categorize Properly - Move from 74.7% \"general\" to specific categories Implement Retry Logic - Add to all critical operations Regular Audits - Monthly security and performance reviews üöÄ Quick Start Templates 1. Error-Handled Webhook Workflow Webhook (with auth) ‚Üí Validate Input ‚Üí Process ‚Üí Success Response\n         ‚Üì                    ‚Üì (error)\n   Error Trigger ‚Üê Error Formatter ‚Üê Error Response 2. Secure API Integration Schedule Trigger ‚Üí Get Credentials ‚Üí HTTPS Request (with retry) ‚Üí Process Data\n                                            ‚Üì (fail)\n                                     Error Handler ‚Üí Notification 3. AI Workflow with Error Handling Trigger ‚Üí Build Context ‚Üí AI Agent ‚Üí Validate Output ‚Üí Use Result\n    ‚Üì            ‚Üì             ‚Üì            ‚Üì\nError Handler ‚Üê Rate Limit ‚Üê Timeout ‚Üê Invalid Output üìö Resources and Next Steps Create Workflow Templates - Build standard templates with error handling Security Audit Tool - Scan all workflows for vulnerabilities Performance Dashboard - Monitor execution times and failures Training Program - Educate team on best practices Governance Policy - Establish workflow development standards üéâ Success Metrics After implementing these practices, aim for: < 5% workflows without error handling 0 public webhooks without authentication 0 HTTP calls (all HTTPS) < 3% workflows with unused nodes > 90% properly categorized workflows < 10s average execution time This guide is based on real-world analysis of 2,050 production workflows. Implement these practices to build more reliable, secure, and maintainable n8n automations."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m0o40i/i_built_an_ai_workflow_that_analyzes_longform/",
          "author": "dudeson55",
          "time": "2025-07-15T17:31:23+00:00",
          "body_text": "Clipping youtube videos and twitch VODs into tiktoks/reels/shorts is a super common practice for content creators and major brands where they take their long form video content like podcasts and video streams then turn it into many different video clips that later get posted and shared on TikTok + IG Reels. Since I don‚Äôt have an entire team of editors to work on creating these video clips for me, I decided to build an automation that does the heavy lifting for me. This is what I was able to come up with: Here's how the automation works 1. Workflow Trigger / Inputs The workflow starts with a simple form trigger that accepts a YouTube video URL. In your system, you could automate this further by setting up an RSS feed for your youtube channel or podcast. 2. Initial Video Processing Request Once the URL is submitted, the workflow makes an HTTP POST request to the Vizard API to start processing the video: The request includes the YouTube video URL and processing parameters like max_clip_number - IMO the defaults actually work pretty well here so I‚Äôd leave most alone to let their system analyze for the most viral moments in the video By default, it will also add in captions. If you want to customize the style of the video / keep captions consistent with your brand you can also specify a template id in your request The API returns a project ID and initial status code that we'll use to poll for results after the video analysis completes 3. Polling Loop for Processing Status Since video processing can take significant time (especially for longer videos), the workflow uses a simple polling system which will loop over: A simple Wait node pauses execution for 10 seconds between status checks (analyzing long form videos will take a fair bit of time so this will check many times) An HTTP GET request checks the processing status using the project ID from the initial request If the status code is 1000 (still processing), the workflow loops back to wait and check again When the status reaches 2000 (completed), the workflow continues to the next section 4. Filtering and Processing Results Once the video analysis/processing is complete, I get all the video clip results back in the response and I‚Äôm able to continue with further processing. The response I get back from this include a virality score of 1/10 based on the clips potential. Clips are filtered based on virality score - I only keep clips with a score of 9 or higher In my testing, this reduces a lot of the noise / worthless clips from the output After those videos get filtered, I then share a summary message in slack with the title, virality score, and download link for each clip You can also take this further and auto-generate a social media caption + pickout ideal hashtags to use based on the content of the video and where you plan to post it. If you want to auto-post, you would use another tool like blotato to publish to each social media platform you need I personally really like using slack to review all the clips because it centralizes all clips into a single spot for me to review before posting. Costs I‚Äôm currently just on the ‚ÄúCreator‚Äù plan for Vizard which costs $29 / month for 600 upload minutes (of source YouTube material). This fits my needs for the content that I create but if you are running a larger scale clipping operation or working with multiple brands that cost is going to scale up linearly for the minutes of source material you use. Workflow Link + Other Resources YouTube video that walks through this workflow node-by-node: https://www.youtube.com/watch?v=Yb-mZmvHh-I The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lunl3h/stealing_someone_elses_project_and_claiming_it_as/",
          "author": "hncvj",
          "time": "2025-07-08T12:43:31+00:00",
          "body_text": "Saw a post here today about an \"I built an AI that runs a viral ASMR channel hands-free.\" Bold claim. Except‚Ä¶ it's not their work. The entire idea, workflow, and execution is taken straight from Nate Herk' YouTube channel and his free S kool community, without a single credit or change. Original here: https://youtu.be/RvuPRX-b7Lc?si=a2iHUF3pqPWDdhAN What's worse? They didn't even bother innovating or acknowledging. Just slapped a \"Workflow - Code Not Included\" banner and called it theirs. This trend of stealing real creators' work for fake clout needs to stop. It's not \"inspired,\" it's not \"repurposed\", it's a direct plagiarism, plain and simple. It's my humble request, respect the creators. Don't build your karma on someone else's grind. What do you guys think?"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "My n8n Workflows Site ( update )- Find Quality Automations Easily!",
          "link": "https://old.reddit.com/r/n8n/comments/1kgxgo4/i_made_a_docker_compose_for_n8n_queue_mode_with/",
          "author": "conor_is_my_name",
          "time": "2025-05-07T13:32:53+00:00",
          "body_text": "UPDATE: Check the 2nd branch if you want to use cloudflared. TLDR:  Put simply, this is the pro level install that you have been looking for, even if you aren't a power user (yet). I can't be the only one who has struggled with queue mode (the documentation is terrible), but I finally nailed it.  Please take this code and use it so no one else has to suffer through what I did building it.  This version is better in every way than the regular install.  Just leave me a GitHub star. https://github.com/conor-is-my-name/n8n-autoscaling First off, who is this for? Anyone who wants to run n8n either locally or on a single server of any size (ram should be 2gb+, but I'd recommend  8gb+ if using with the other containers linked at the bottom, the scrapers are ram hogs) You want simple setup Desire higher parallel throughput (it won't make single jobs faster) Why is queue mode great? No execution limit bottlenecks scales up and scales down based on load if a worker fails, the jobs gets reassigned Whats inside: A Docker-based autoscaling solution for n8n workflow automation platform. Dynamically scales worker containers based on Redis queue length. No need to deal with k8s or any other container scaling provider, a simple script runs it all and is easily configurable. Includes Puppeteer and Chrome built-in for pro level scraping directly from the n8n code node.  It makes it so much easier to do advanced scraping compared to using the community nodes.  Just paste your puppeteer script in a regular code node and you are rolling.  Use this in conjunction with my Headful Chrome Docker that is linked at the bottom for great results on tricky websites. Everything installs and configures automatically, only prerequisite is having docker installed.  Works on all platforms, but the puppeteer install requires some dependency tweaks if you are using a ARM cpu.  (an AI will know what to do for the dependency changes) Install instructions: Windows or Mac: Install the docker desktop app. Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d Linux: Follow the instructions for the Docker Convenience Script . Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d That's it.  (But remember to change the passwords) Default settings are for 50 simultaneous workflow executions.  See GitHub page for instructions on changing the worker count and concurrency. A tip for those who are in the process of leveling up their n8n game: move away from google sheets and airtable - they are slow and unstable embrace Postgres - with AI its really easy, just ask it what to do and how to set up the tables Tested on a Netcup 8 core 16gb Root VPS - RS 2000 G11.  Easily ran hundreds of simultaneous executions.  Lower end hardware should work fine too, but you might want to limit the number of worker instances to something that makes sense for your own hardware. If this post inspires you to get a server, use this link .  Or don't, just run this locally for free. I do n8n consulting, send me a message if you need help on a project. check out my other n8n specific GitHub repos: Extremely fast google maps scraper - this one is a masterpiece web scraper server using crawlee for deep scraping - I've scraped millions of pages using this Headful Chrome Docker with Puppeteer for precise web scraping and persistent sessions - for tricky websites and those requiring logins"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that analyzes long-form YouTube videos and generates short form clips optimized for TikTok / IG Reels / YT Shorts",
          "link": "https://old.reddit.com/r/n8n/comments/1lkbx2b/i_built_this_ai_automation_that_generates_viral/",
          "author": "dudeson55",
          "time": "2025-06-25T17:28:56+00:00",
          "body_text": "There‚Äôs been a huge trend of Bigfoot / Yeti vlog videos exploding across IG and TikTok all created with Veo 3 and I wanted to see if I could replicate and automate the full process of: Taking a simple idea as input Generate an entire story around that simple idea Turn that into a Veo 3 prompt Finally generate those videos inside n8n using FAL. Had a lot of fun building this and am pretty happy with final output . Here‚Äôs the workflow breakdown. 1. Input / Trigger The input and trigger for this workflow is a simple Form Trigger that has a single text field. What goes into here is a simple idea for for what bigfoot will be doing that will later get turned into a fully fleshed-out story. It doesn‚Äôt need any crazy detail, but just needs something the story can be anchored around. Here‚Äôs an example of one of the ones I used earlier to give you a better idea: jsx\nBigfoot discovers a world war 2 plane crash while on a hike through the deep forest that he hasn't explored yet 2. The Narrative Writer Prompt The next main node of this automation is what I call the ‚Äúnarrative writer‚Äù. Its function is very similar to a storyboard artist where it will accept the basic ideas as input and will generate an outline for each clip that needs to be generated for the story. Since Veo 3 has a hard limit of 8 seconds per video generation, that was a constraint I had to define here. So after this runs, I get an outline that splits up the story into 8 distinct clips that are each 8 seconds long. I also added in extra constraints here like what I want bigfoots personality to be like on camera to help guide the dialog and I also specified that I want the first out of the 8 clips to always be an introduction to the video. Here‚Äôs the full prompt I am using: ```jsx Role: You are a creative director specializing in short-form, character-driven video content. Goal: Generate a storyboard outline for a short vlog based on a user-provided concept. The output must strictly adhere to the Persona, Creative Mandate, and Output Specification defined below. [Persona: Bigfoot the Vlogger] Identity: A gentle giant named \"Sam,\" who is an endlessly curious and optimistic explorer. His vibe is that of a friendly, slightly clumsy, outdoorsy influencer discovering the human world for the first time. Voice & Tone: Consistently jolly, heartwarming, and filled with childlike wonder. He is easily impressed and finds joy in small details. His language is simple, and he might gently misuse human slang. PG-rated, but occasional mild exasperation like \"geez\" or \"oh, nuts\" is authentic. His dialog and lines MUST be based around the \"Outdoor Boys\" YouTube channel and he must speak like the main character from that Channel. Avoid super generic language. Physicality: An 8-foot male with shaggy, cedar-brown fur ( #6d6048 ) and faint moss specks. His silhouette is soft and \"huggable\" due to fluffy fur on his cheeks and shoulders. Features soft, medium-amber eyes, rounded cheeks, a broad nose, and short, blunt lower canines visible when he smiles. He holds a simple selfie stick at all times. [Creative Mandate] Visual Style: All scenes are shot 16:9 from a selfie-stick perspective held by Bigfoot. The style must feel like authentic, slightly shaky \"found footage.\" The camera is always on him, not his POV. Narrative Goal: The primary objective is to create audience affection. Each scene must showcase Bigfoot's charm through his gentle humor, endearing discoveries, or moments of vulnerability. The 8-scene arc must have a satisfying and heartwarming payoff. [Output Specification] Structure: Provide a storyboard with exactly 8 sequential scenes, formatted as shown below. Introduction Rule: Scene 1 must be a direct-to-camera introduction. In it, Bigfoot should enthusiastically greet his viewers (e.g., \"Hey everybody!\" or \"Hi friends!\") and briefly state the goal or adventure for the vlog, based on the user's concept. Duration: Each scene represents 8 seconds of footage. Content per Scene: For each scene, provide a single, descriptive paragraph. This paragraph must seamlessly weave together the visual action, Bigfoot's expressions, and his spoken dialogue. Each scene you create should be part of a cohesive story. Example Formats: SCENE 1 (0:00-0:08): Sam the Bigfoot grins warmly into his selfie stick, the background slightly out of focus. He waves a large, furry hand and says, \"Hey friends, Sam here! Today, we're going to try and build a brand new shelter out of... well, whatever we can find! Wish me luck!\" SCENE 3 (0:32-0:40): Sam holds up a rusty, bent bicycle wheel, peering at it curiously with his head tilted. He tries to spin it, a look of concentration on his face. \"Hmm. This is a weird-looking rock. Very... holey. Not good for a wall, I don't think.\" Task Using the rules above, create the storyboard outline for the following concept: {{ $json['Bigfoot Video Idea'] }} ``` And here‚Äôs what the output looks like after this prompt runs: ```jsx\nTitle: Finding History in the Deep Woods - WWII Plane Discovery!\nConcept: Bigfoot discovers a world war 2 plane crash while on a hike through the deep forest Scene 1 | 0:00-0:08\nSam the Bigfoot grins warmly into his selfie stick, towering evergreens framing his fuzzy shoulders. He adjusts his grip on the stick with both hands and beams at the camera. \"Hey everybody, Sam here! Today we're heading way deep into the back country for some serious exploring. You never know what kind of treasures you might stumble across out here!\" Scene 2 | 0:08-0:16\nSam trudges through dense undergrowth, his selfie stick bouncing slightly as he navigates around massive fir trees. Moss hangs like curtains around him, and his amber eyes dart curiously from side to side. \"Man, this forest just keeps getting thicker and thicker. Perfect day for it though - nice and cool, birds are singing. This is what I call the good life, friends!\" Scene 3 | 0:16-0:24\nSam suddenly stops mid-stride, his eyes widening as he stares off-camera. The selfie stick trembles slightly in his grip, showing his surprised expression clearly. \"Whoa, hold on a second here...\" He tilts his shaggy head to one side, his mouth forming a perfect 'O' of amazement. \"Guys, I think I'm seeing something pretty incredible through these trees.\" Scene 4 | 0:24-0:32\nSam approaches cautiously, pushing aside hanging branches with his free hand while keeping the camera steady. His expression shifts from wonder to respectful awe as he gets closer to his discovery. \"Oh my goodness... friends, this is... this is an old airplane. Like, really old. Look at the size of this thing!\" His voice drops to a whisper filled with reverence. Scene 5 | 0:32-0:40\nSam extends the selfie stick to show himself standing next to the moss-covered wreckage of a WWII fighter plane, its metal frame twisted but still recognizable. His expression is one of deep respect and fascination. \"This has got to be from way back in the day - World War Two maybe? The forest has just been taking care of it all these years. Nature's got its own way of honoring history, doesn't it?\" Scene 6 | 0:40-0:48\nSam crouches down carefully, his camera capturing his gentle examination of some scattered debris. He doesn't touch anything, just observes with his hands clasped respectfully. \"You know what, guys? Someone's story ended right here, and that's... that's something worth remembering. This pilot was probably somebody's son, maybe somebody's dad.\" His usual cheerfulness is tempered with genuine thoughtfulness. Scene 7 | 0:48-0:56\nSam stands and takes a step back, his expression shifting from contemplation to gentle resolve. He looks directly into the camera with his characteristic warmth, but there's a new depth in his amber eyes. \"I think the right thing to do here is let the proper folks know about this. Some family out there might still be wondering what happened to their loved one.\" Scene 8 | 0:56-1:04\nSam gives the camera one final, heartfelt look as he begins to back away from the site, leaving it undisturbed. His trademark smile returns, but it's softer now, more meaningful. \"Sometimes the best adventures aren't about what you take with you - they're about what you leave behind and who you help along the way. Thanks for exploring with me today, friends. Until next time, this is Sam, reminding you to always respect the stories the forest shares with us.\"\n``` 3. The Scene Director Prompt The next step is to take this story outline and turn it into a real prompt that can get passed into Veo 3. If we just took the output from the outline and tried to create a video, we‚Äôd get all sorts of issues where the character would not be consistent across scenes, his voice would change, the camera used would change, and things like that. So the next step of this process is to build out a highly detailed script with all technical details necessary to give us a cohesive video across all 8 clips / scenes we need to generate. The prompt here is very large so I won‚Äôt include it here (it is included inside the workflow) but I will share the desired output we are going for. For every single 8 second clip we generate, we are creating something exactly like that will cover: Scene overview Scene description Technical specs like duration, aspect ratio, camera lens Details of the main subject (Bigfoot) Camera motion Lighting Atmosphere Sound FX Audio Bigfoot dialog Really the main goal here is to be as specific as possible so we can get consistent results across each and every scene we generate. ```jsx SCENE 4  ‚ñ∏  ‚ÄúTrail to the Lake‚Äù  ‚ñ∏  0 ‚Äì 8 s Selfie-stick POV. Bigfoot strolls through dense cedar woods toward a sun-sparkled lake in the distance. No spoken dialogue in this beat‚Äîjust ambient forest sound and foot-fall crunches. Keeps reference camera-shake, color grade, and the plush, lovable design. SCENE DESCRIPTION POV selfie-stick vlog: Bigfoot walks along a pine-needle path, ferns brushing both\nsides. Sunbeams flicker through the canopy. At the 6-second mark the shimmering\nsurface of a lake appears through the trees; Bigfoot subtly tilts the stick to\nhint at the destination. TECHNICAL SPECS ‚Ä¢ Duration‚ÄÉ8 s‚ÄÉ‚Ä¢ 29.97 fps‚ÄÉ‚Ä¢ 4 K UHD‚ÄÉ‚Ä¢ 16 : 9 horizontal ‚Ä¢ Lens‚ÄÉ24 mm eq, ∆í/2.8‚ÄÉ‚Ä¢ Shutter‚ÄÉ1/60 s (subtle motion-blur) ‚Ä¢ Hand-held wobble amplitude cloned from reference clip (small ¬±2¬∞ yaw/roll). SUBJECT DETAILS  (LOCK ACROSS ALL CUTS) ‚Ä¢ 8-ft male Bigfoot, cedar-brown shaggy fur #6d6048 with faint moss specks. ‚Ä¢ Fluffier cheek & shoulder fur ‚Üí plush, huggable silhouette. ‚Ä¢ Eyes: soft medium-amber, natural catch-lights only ‚Äî no glow or excess brightness. ‚Ä¢ Face: rounded cheeks, gentle smile crease; broad flat nose; short blunt lower canines. ‚Ä¢ Hands: dark leathery palms, 4-inch black claws; right paw grips 12-inch carbon selfie stick. ‚Ä¢ Friendly, lovable, gentle vibe. CAMERA MOTION 0 ‚Äì 2 s‚ÄÉStick angled toward Bigfoot‚Äôs chest/face as he steps onto path. 2 ‚Äì 6 s‚ÄÉSmooth forward walk; slight vertical bob; ferns brush lens edges. 6 ‚Äì 8 s‚ÄÉStick tilts ~20¬∞ left, revealing glinting lake through trees; light breeze ripples fur. LIGHTING & GRADE Late-morning sun stripes across trail; teal-olive mid-tones, warm highlights,\ngentle film grain, faint right-edge lens smudge (clone reference look). ATMOSPHERE FX ‚Ä¢ Dust motes / pollen drifting in sunbeams. ‚Ä¢ Occasional leaf flutter from breeze. AUDIO BED  (NO SPOKEN VOICE) Continuous forest ambience: songbirds, light wind, distant woodpecker;\nsoft foot-crunch on pine needles; faint lake-lap audible after 6 s. END FRAME Freeze at 7.8 s with lake shimmering through trees; insert one-frame white-noise\npop to preserve the series‚Äô hard-cut rhythm.\n``` 3. Human in the loop approval The middle section of this workflow is a human in the loop process where we send the details of the script to a slack channel we have setup and wait for a human to approve or deny it before we continue with the video generation. Because generation videos this way is so expensive ($6 per 8 seconds of video), we want to review this before before potentially being left with a bad video. 4. Generate the video with FAL API The final section of this automation is where actually take the scripts generated from before, iterate over each, and call in to FAL‚Äôs Veo 3 endpoint to queue up the video generation request and wait for it to generate. I have a simple polling loop setup to check its status every 10 seconds which will loop until the video is completely rendered. After that is done, the loop will move onto the next clip/scene it needs to generate until all 8 video clips are rendered. Each clip get‚Äôs uploaded to a Google Drive I have configured so my editor can jump in and stitch them together into a full video. If you wanted to extend this even further, you could likely use the json2video API to do that stitching yourself, but that ultimately depends on how far or not you want to automate. Notes on keeping costs down Like I mentioned above, the full cost of running this is currently very expensive. Through the FAL API it costs $6 for 8 seconds of video so this probably doesn‚Äôt make sense for everyone‚Äôs use case. If you want to keep costs down, you can still use this exact same workflow and drop the 3rd section that uses the FAL API. Each of the prompts that get generated for the full script can simply be copied and pasted into Gemini or Flow to generate a video of the same quality but it will be much cheaper to do so. Workflow Link + Other Resources Github n8n workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=C65c8itWvf4 Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI that's smarter than most real estate agents. Here's the n8n blueprint.",
          "link": "https://old.reddit.com/r/n8n/comments/1kml8st/i_made_a_google_maps_scraper_designed/",
          "author": "conor_is_my_name",
          "time": "2025-05-14T17:33:42+00:00",
          "body_text": "Hey everyone! Today I am sharing my custom built google maps scraper.  It's extremely fast compared to most other maps scraping services and produces more reliable results as well. I've spent thousands of dollars over the years on scraping using APIFY, phantom buster, and other services.  They were ok but I also got many formatting issues which required significant data cleanup. Finally went ahead and just coded my own.  Here's the link to the GitHub repo, just give me a star: https://github.com/conor-is-my-name/google-maps-scraper It includes example json for n8n workflows to get started in the n8n nodes folder.  Also included the Postgres code you need to get basic tables up and running in your database. These scrapers are designed to be used in conjunction with my n8n build linked below.  They will work with any n8n install, but you will need to update the IP address rather than just using the container name like in the example. https://github.com/conor-is-my-name/n8n-autoscaling If using the 2 together, make sure that you set up the external docker network as described in the instructions.  Doing so makes it much easier to get the networking working. Why use this scraper? Best in class speed and reliability You can scale up with multiple containers on multiple computers/servers, just change the IP. A word of warning: Google will rate limit you if you just blast this a million times.  Slow and steady wins the race.  I'd recommend starting at no more than 1 per minute per IP address.  There are 1440 minutes in a day x 100 results per search = 144,000 results per day. https://preview.redd.it/ozwqpz31is0f1.png?width=644&format=png&auto=webp&s=a0b472a70594a4b3fbd72ffc7e429e7ae7acbb11 Example Search: Query = Hotels in 98392 (you can put anything here) language = en limit results = 1 (any number) headless = true [\n  {\n    \"name\": \"Comfort Inn On The Bay\",\n    \"place_id\": \"0x549037bf4a7fd889:0x7091242f04ffff4f\",\n    \"coordinates\": {\n      \"latitude\": 47.543005199999996,\n      \"longitude\": -122.6300069\n    },\n    \"address\": \"1121 Bay St, Port Orchard, WA 98366\",\n    \"rating\": 4,\n    \"reviews_count\": 735,\n    \"categories\": [\n      \"Hotel\"\n    ],\n    \"website\": \"https://www.choicehotels.com/washington/port-orchard/comfort-inn-hotels/wa167\",\n    \"phone\": \"3603294051\",\n    \"link\": \"https://www.google.com/maps/place/Comfort+Inn+On+The+Bay/data=!4m10!3m9!1s0x549037bf4a7fd889:0x7091242f04ffff4f!5m2!4m1!1i2!8m2!3d47.5430052!4d-122.6300069!16s%2Fg%2F1tfz9wzs!19sChIJidh_Sr83kFQRT___BC8kkXA?authuser=0&hl=en&rclk=1\"\n  },"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l0nj7m/i_built_a_workflow_that_generates_longform_blog/",
          "author": "LargePay1357",
          "time": "2025-06-01T12:16:04+00:00",
          "body_text": "First, an AI agent goes back and forth with you to finalize the title and headings. Then, it calls the next workflow to write the blog post. Code for both workflows: https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json Video tutorial: https://www.youtube.com/watch?v=6AgfcKUMkVY"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking for Review!",
          "link": "https://old.reddit.com/r/n8n/comments/1lineqi/how_you_can_setup_and_use_n8n_as_your_backend_for/",
          "author": "dudeson55",
          "time": "2025-06-23T17:52:36+00:00",
          "body_text": "I wanted to put together a quick guide and walk through on how you can use n8n to be the backend that powers your mobile apps / web apps / internal tools. I‚Äôve been using Lovable a lot lately and thought this would be the perfect opportunity to put together this tutorial and showcase this setup working end to end. The Goal - Clone the main app functionality Cal AI I thought a fun challenge for this would be cloning the core feature of the Cal AI mobile app which is an AI calorie tracker that let‚Äôs you snap a picture of your meal and get a breakdown of all nutritional info in the meal. I suspected this all could be done with a well written prompt + an API call into Open AI‚Äôs vision API (and it turns out I was right). 1. Setting up a basic API call between lovable and n8n Before building the whole frontend, the first thing I wanted to do was make sure I could get data flowing back and forth between a lovable app and a n8n workflow. So instead of building the full app UI in lovable, I made a very simple lovable project with 3 main components: Text input that accepts a webhook url (which will be our n8n API endpoint) File uploader that let‚Äôs me upload an image file for our meal we want scanned Submit button to make the HTTP request to n8n When I click the button, I want to see the request actually work from lovable ‚Üí n8n and then view the response data that actually comes back (just like a real API call). Here‚Äôs the prompt I used: jsx\nPlease build me a simple web app that contains three components. Number one, a text input that allows me to enter a URL. Number two, a file upload component that lets me upload an image of a meal. And number three, a button that will submit an HTTP request to the URL that was provided in the text input from before. Once that response is received from the HTTP request, I want you to print out JSON of the full details of the successful response. If there's any validation errors or any errors that come up during this process, please display that in an info box above. Here‚Äôs the lovable project if you would like to see the prompts / fork for your own testing: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 2. Setting up the n8n workflow for our backend Next up we need to setup the n8n workflow that will be our ‚Äúbackend‚Äù for the app. This step is actually pretty simple to get n8n working as your backend, all you need is the following: A Webhook Trigger on your workflow Some sort of data processing in the middle (like loading results from your database or making an LLM-chain call into an LLM like GPT) A Respond To Webhook node at the very end of the workflow to return the data that was processed On your initial Webhook Trigger it is very important that you change the Respond option set to Using ‚ÄòRespond To Webhook‚Äô Node. If you don‚Äôt have this option set, the webhook is going to return data immediately instead of waiting for any of your custom logic to process such as loading data from your database or calling into a LLM with a prompt. In the middle processing nodes, I ended up using Open AI‚Äôs vision API to upload the meal image that will be passed in through the API call from lovable and ran a prompt over it to extract the nutritional information from the image itself. Once that prompt finished running, I used another LLM-chain call with an extraction prompt to get the final analysis results into a structured JSON object that will be used for the final result. I found that using the Auto-fixing output parser helped a lot here to make this process more reliable and avoided errors during my testing. Meal image analysis prompt: ```jsx\n<identity>\nYou are a world-class AI Nutrition Analyst.\n</identity> <mission>\nYour mission is to perform a detailed nutritional analysis of a meal from a single image. You will identify the food, estimate portion sizes, calculate nutritional values, and provide a holistic health assessment.\n</mission> Analysis Protocol\n1. Identify: Scrutinize the image to identify the meal and all its distinct components. Use visual cues and any visible text or branding for accurate identification.\n2. Estimate: For each component, estimate the portion size in grams or standard units (e.g., 1 cup, 1 filet). This is critical for accuracy.\n3. Calculate: Based on the identification and portion estimates, calculate the total nutritional information for the entire meal.\n4. Assess & Justify: Evaluate the meal's overall healthiness and your confidence in the analysis. Justify your assessments based on the provided rubrics. Output Instructions\nYour final output MUST be a single, valid JSON object and nothing else. Do not include json markers or any text before or after the object. Error Handling\nIf the image does not contain food or is too ambiguous to analyze, return a JSON object where confidenceScore is 0.0 , mealName is \"Unidentifiable\", and all other numeric fields are 0 . OUTPUT_SCHEMA json\n{\n  \"mealName\": \"string\",\n  \"calories\": \"integer\",\n  \"protein\": \"integer\",\n  \"carbs\": \"integer\",\n  \"fat\": \"integer\",\n  \"fiber\": \"integer\",\n  \"sugar\": \"integer\",\n  \"sodium\": \"integer\",\n  \"confidenceScore\": \"float\",\n  \"healthScore\": \"integer\",\n  \"rationale\": \"string\"\n} Field Definitions\n*   ** mealName : A concise name for the meal (e.g., \"Chicken Caesar Salad\", \"Starbucks Grande Latte with Whole Milk\"). If multiple items of food are present in the image, include that in the name like \"2 Big Macs\".\n*   ** calories : Total estimated kilocalories.\n*   ** protein : Total estimated grams of protein.\n*   ** carbs : Total estimated grams of carbohydrates.\n*   ** fat : Total estimated grams of fat.\n*   ** fiber : Total estimated grams of fiber.\n*   ** sugar : Total estimated grams of sugar (a subset of carbohydrates).\n*   ** sodium : Total estimated milligrams (mg) of sodium.\n*   ** confidenceScore : A float from 0.0 to 1.0 indicating your certainty. Base this on:\n    *   Image clarity and quality.\n    *   How easily the food and its components are identified.\n    *   Ambiguity in portion size or hidden ingredients (e.g., sauces, oils).\n*   ** healthScore : An integer from 0 (extremely unhealthy) to 10 (highly nutritious and balanced). Base this on a holistic view of:\n    *   Level of processing (whole foods vs. ultra-processed).\n    *   Macronutrient balance.\n    *   Sugar and sodium content.\n    *   Estimated micronutrient density.\n*   ** rationale **: A brief (1-2 sentence) explanation justifying the healthScore and confidenceScore . State key assumptions made (e.g., \"Assumed dressing was a standard caesar\" or \"Portion size for rice was difficult to estimate\").\n``` On the final Respond To Webhook node it is also important to node that this is the spot where we will be cleaning up the final data setting the response Body for the HTTP request / API call. For my use-case where we are wanting to send back nutritional info for the provided image, I ended up formatting my response as JSON to look like this: jsx\n{\n  \"mealName\": \"Grilled Salmon with Roasted Potatoes and Kale Salad\",\n  \"calories\": 550,\n  \"protein\": 38,\n  \"carbs\": 32,\n  \"fat\": 30,\n  \"fiber\": 7,\n  \"sugar\": 4,\n  \"sodium\": 520,\n  \"confidenceScore\": 0.9,\n  \"healthScore\": 4\n} 3. Building the final lovable UI and connecting it to n8n With the full n8n backend now in place, it is time to spin up a new Lovable project and build the full functionality we want and style it to look exactly how we would like. You should expect this to be a pretty iterative process. I was not able to get a fully working app in 1-shot and had to chat back and forth in lovable to get the functionality working as expected. Here‚Äôs some of the key points in the prompt / conversation that had a large impact on the final result: Initial create app prompt: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8pekjpfeyrs52bdf1m1dm7 Style app to more closely match Cal AI: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rbd2wfvkrxxy7pc022n0e Setting up iphone mockup container: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rs1b8e7btc03gak9q4rbc Wiring up the app to make an API call to our n8n webhook: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxajea31e2xvtwbr1kytdxbb Updating app functionality to use real API response data instead of mocked dummy data (important - you may have to do something similar): https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxapb65ree5a18q99fsvdege If I was doing this again from the start, I think it would actually be much easier to get the lovable functionality working with default styles to start with and then finish up development by styling everything you need to change at the very end. The more styles, animations, other visual elements that get added in the beginning, the more complex it is to change as you get deeper into prompting. Lovable project with all prompts used: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 4. Extending this for more complex cases + security considerations This example is a very simple case and is not a complete app by any means. If you were to extend this functionality, you would likely need to add in many more endpoints to take care of other app logic + features like saving your history of scanned meals, loading up your history of scanned meals, other analysis features that can surface trends. So this tutorial is really meant to show you a bit of what is possible between lovable + n8n. The other really important thing I need to mention here is the security aspect of a workflow like this. When following my instructions above, your webhook url will not be secure. This means that if your webhook url leaks, it is completely possible for someone to make API requests into your backend and eat up your entire quota for n8n executions and run up your Open AI bill. In order to get around this for a production use-case, you will need to implement some form of authentication to protect your webhook url from malicious actors. This can be something as simple as basic auth where web apps that consume your API need to have a username / password or you could build out a more advanced auth system to protect your endpoints. My main point here is, make sure you know what you are doing before you publically rollout a n8n workflow like this or else you could be hit with a nasty bill or users of your app could be accessing things they should not have access to. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json Lovable project links: Simple n8n API request: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 Full Cal AI cloned project: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=4c-kYOiksFg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How you can setup and use n8n as your backend for a Lovable.dev app (I cloned the mobile app Cal AI)",
          "link": "https://old.reddit.com/r/n8n/comments/1lvlb8n/i_built_a_content_repurposing_system_that_turns/",
          "author": "dudeson55",
          "time": "2025-07-09T15:12:29+00:00",
          "body_text": "I built a content repurposing system that I have been using for the past several weeks that my YouTube video as input ‚Üí scrapes the transcript ‚Üí repurposes it into a post that is optimized for engagement on the platform I am posting to (right now just Twitter and LinkedIn but can be extended to many more). My social accounts are still pretty young so I don‚Äôt have great before/after stats to share, but I‚Äôm confident that the output quality here is on-par with what other creators are making and going viral with. My goal with this is to share a basic setup that you can take an run with in your own business to be customized for your niche / industry and add additional target platforms that you want to repurpose to. You could even change the main input to a long form blog post as your starting point instead of a youtube video. Here's a full breakdown of the automation 1. Workflow Trigger / Input The workflow starts with a simple form trigger that accepts a YouTube video URL as input. This is specific to our business since we always start with creating YouTube content first and then repurpose it into other formats. Form trigger accepts YouTube video URL as required text input If your content workflow starts with blog posts or other formats, you'll need to modify this trigger accordingly The URL gets passed through to the scraping operation (If your company and or your client‚Äôs company starts with a blog post first, I‚Äôd suggested simply using a tool to scrape that web page to load of that text content) 2. Scrape YouTube Video This is where we extract the video metadata and full transcript using a YouTube Scraper on Apify. Starts by using the streamers/youtube-scraper actor from the apify store (Costs $5 per 1,000 videos you scrape) Makes an HTTP request to the /run-sync-get-dataset-items endpoint to start scraping / get results back I like using this endpoint when consuming apify actors as it returns data back in the same http request we make. No need to setup polling or extra n8n nodes to use The scraper extracts title, metadata, and most importantly the full transcript in SRT format (timestamps w/ the text that was said in the video) 3. Generate Twitter Post The Twitter repurposing path follows a structured approach using a few examples I want to replicate + a detailed prompt. Set Twitter Examples : Simple ‚ÄúSet Field‚Äù node where I curated and put in 8 high-performing tweet examples that define the style and structure I want to replicate Build Master Prompt : Another Set Field node where I build a prompt that will tell the LLM to: Analyze the source YouTube transcript material Study the Twitter examples for structure and tone Generate 3 unique viral tweet options based on the content LLM Chain Call : Pass the complete prompt to Claude Sonnet Format and Share : Clean up the output and share the best 3 tweet options to Slack for me to review ```jsx ROLE: You are a world-class social media copywriter and viral growth hacker. Your expertise is in the AI, automation, and no-code space on Twitter/X. You are a master at deconstructing viral content and applying its core principles to generate new, successful posts. OBJECTIVE: Your mission is to generate three distinct, high-potential viral tweets . This tweet will promote a specific n8n automation, with the ultimate goal of getting people to follow my profile, retweet the post, and comment a specific keyword to receive the n8n workflow template via DM. STEP 1: ANALYZE SOURCE MATERIAL First, meticulously analyze the provided YouTube video transcript below. Do not summarize it. Instead, your goal is to extract the following key elements:\n1. The Core Pain Point: What is the single most frustrating, time-consuming, or tedious manual task that this automation eliminates?\n2. The \"Magic\" Solution: What is the most impressive or \"wow\" moment of the automation? What does it enable the user to do that felt impossible or difficult before?\n3. The Quantifiable Outcome: Identify any specific metrics of success mentioned (e.g., \"saves 10 hours a week,\" \"processes 100 leads a day,\" \"automates 90% of the workflow\"). If none are mentioned, create a powerful and believable one. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES Next, study the structure, tone, and psychological hooks of the following successful tweets. These examples are your primary source for determining the structure of the tweets you will generate. <twitter_tweet_examples>\n{{ $('set_twitter_examples').item.json.twitter_examples }}\n</twitter_tweet_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE TWEETS Now you will generate the 3 unique, viral tweet options. Your primary task is to act as a structural analyst: analyze the provided examples, identify the most effective structures, and then apply those structures to the content from Step 1. Your process: 1. Identify Core Structures: Analyze the <twitter_tweet_examples> . Identify the different underlying formats. For instance, is there a \"Problem ‚Üí Solution\" structure? A \"Shocking Result ‚Üí How-to\" structure? A \"Controversial Statement ‚Üí Justification\" structure? Identify the 3 most distinct and powerful structures present.\n2. Map Content to Structures: For each of the 3 structures you identified, map the \"Pain Point,\" \"Magic Solution,\" and \"Outcome\" from Step 1 into that framework.\n3. Craft the Tweets: Generate one tweet for each of the 3 structures you've chosen. The structure of each tweet (the hook, the flow, the tone) should directly mirror the style of the example it is based on. Essential Components: While you choose the overall structure, ensure each tweet you craft contains these four key elements, integrated naturally within the chosen format:\n- A Powerful Hook: The opening line that grabs attention.\n- A Clear Value Proposition: The \"what's in it for me\" for the reader.\n- An Irresistible Offer: The free n8n workflow template.\n- A High-Engagement Call to Action (CTA): The final call to action must include elements the ask for a follow, a retweet, and a comment of the \"[KEYWORD]\". CONSTRAINTS: -   Vary light use of emojis to add personality and break up the text. Not all Tweets you write should have emojis.\n-   Keep the tone energetic, confident, and educational, mirroring the tone found in the examples.\n-   Ensure the chosen [KEYWORD] is simple, relevant, and in all caps. Now, generate the 3 distinct tweet options, clearly labeled as Tweet Option 1 , Tweet Option 2 , and Tweet Option 3 . For each option, briefly state which example structure you are applying. (e.g., \"Tweet Option 1: Applying the 'Problem ‚Üí Solution' structure from Example 2.\").\n``` 4. Generate LinkedIn Post The LinkedIn path follows a similar but platform-specific approach (better grammar and different call to action): Set LinkedIn Examples : Curated examples of high-performing LinkedIn posts with different formatting and professional tone Build LinkedIn-Specific Prompt : Modified prompt that positions the LLM as a \"B2B content strategist and LinkedIn growth expert\" rather than a viral Twitter copywriter Generate Multiple Options : Creates 3 different LinkedIn post variations optimized for professional engagement Review Process : Posts all options to Slack for me to review The key difference is tone and structure - LinkedIn posts are longer, more professional, minimize emoji usage, and focus on business value rather than viral hooks. It is important to know your audience here and have a deep understanding of the types of posts that will do well. ```jsx ROLE: You are a world-class B2B content strategist and LinkedIn growth expert. Your expertise lies in creating compelling professional content around AI, automation, and no-code solutions. You are a master of professional storytelling, turning technical case studies into insightful, engaging posts that drive meaningful connections and establish thought leadership. OBJECTIVE: Your mission is to generate three distinct, high-potential LinkedIn posts . Each post will promote a specific n8n automation, framing it as a professional case study. The ultimate goals are to:\n1.  Grow my LinkedIn professional network (followers).\n2.  Establish my profile as a go-to resource for AI and automation.\n3.  Drive awareness and interest in my YouTube channel.\n4.  Get users to comment for a lead magnet (the n8n workflow). STEP 1: ANALYZE SOURCE MATERIAL (THE BUSINESS CASE) First, meticulously analyze the provided YouTube video transcript. Do not summarize it. Instead, extract the following key business-oriented elements:\n1. The Business Pain Point: What common, frustrating, or inefficient business process does this automation solve? Frame it in terms of lost time, potential for human error, or missed opportunities.\n2. The Strategic Solution: How does the n8n automation provide a smart, strategic solution? What is the core \"insight\" or \"lever\" it uses to create value?\n3. The Quantifiable Business Impact: What is the measurable outcome? Frame it in business terms (e.g., \"reclaimed 10+ hours for strategic work,\" \"achieved 99% accuracy in data processing,\" \"reduced new client onboarding time by 50%\"). If not explicitly mentioned, create a powerful and believable metric. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES (LINKEDIN POSTS) Next, study the structure, tone, and especially the Call to Action (CTA) of the following successful LinkedIn posts. These examples are your primary source for determining the structure of the posts you will generate. Pay close attention to the length of the examples as they \"feel\" right in length. <linkedin_post_examples>\n{{ $('set_linked_in_examples').item.json.linked_in_examples }}\n</linkedin_post_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE POSTS Now you will generate 3 unique LinkedIn post options. Your primary task is to act as a content strategist: analyze the provided LinkedIn examples, identify the most effective post structures, and then apply those structures to the business case from Step 1. Your process: 1. Identify Core Structures: Analyze the <linkedin_post_examples> . Identify 3 distinct formats (e.g., \"Problem/Agitate/Solve,\" \"Personal Story ‚Üí Business Lesson,\" \"Contrarian Take ‚Üí Justification\").\n2. Map Content to Structures: For each structure, weave the \"Business Pain Point,\" \"Strategic Solution,\" and \"Business Impact\" into a compelling narrative.\n3. Craft the Posts: Generate one post for each chosen structure. The post should be highly readable, using short paragraphs and ample white space. Essential Components for each LinkedIn Post: - An Intriguing Hook: A first line that stops the scroll and speaks to a professional ambition or frustration.\n- A Relatable Story/Problem: Briefly set the scene using the \"Business Pain Point.\"\n- The Insightful Solution: Explain the \"Strategic Solution\" as the turning point.\n- A Dynamic, High-Engagement Call to Action (CTA): This is critical. Instead of a fixed format, you will craft the most effective CTA by analyzing the examples provided. Your CTA must accomplish two things:\n    1.  Clearly state how to get the free n8n workflow template by commenting with a specific [KEYWORD] .\n    2.  Naturally encourage following my profile and sharing the post. Draw inspiration for the wording and style directly from the successful CTAs in the examples. If it fits the narrative, you can subtly mention that more deep dives are on my YouTube. CONSTRAINTS: -   Use emojis sparingly and professionally (e.g., ‚úÖ, üí°, üöÄ) to enhance readability.\n-   The tone must be professional, insightful, and helpful.\n-   The [KEYWORD] should be a professional, single word in all caps (e.g., BLUEPRINT, WORKFLOW, SYSTEM). FINAL OUTPUT FORMAT: You MUST format your entire response as a single, valid JSON object. The root of the object should be a key named \"post_options\", which contains an array of three post objects. Adhere strictly to the following structure for each object:\n{\n  \"analysis\": \"<string: Explain which LinkedIn example structure was applied>\",\n  \"post_text\": \"<string: The full text of the LinkedIn post, with line breaks>\"\n}\nDo not include any text or explanations outside of the JSON object.\n``` 5. Final Output Review Both paths conclude by sharing the generated content to Slack channels for human review. This gives me 3 Twitter options and 3 LinkedIn options to choose from, each optimized for best engagement. All I have to do is copy and paste the one I like the most into my social media scheduling tool then I‚Äôm done. Extending the System The best part about this is it is very easy to extend this system for any type of repurposing you need to do. LinkedIn / Twitter is only the starting point, it can be taken much further. Instagram carousel posts - Take the transcript ‚Üí pull out a few quotes ‚Üí generate an image using either Canva an AI Image generator Newsletter sections - Take the transcript + video url ‚Üí build a prompt that will write a mini-promo section for your video to be included in your newsletter Blog post / tutorial post - Take the transcript ‚Üí write a prompt that will turn it into a text-based tutorial to be published on your blog. Each new path would follow the same pattern: curate platform-specific examples, build targeted prompts, and generate multiple options for review. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=u9gwOtjiYnI The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "MVP Generator",
          "link": "https://old.reddit.com/r/n8n/comments/1ktun9q/you_guys_liked_my_custom_n8n_autoscaling_build/",
          "author": "conor_is_my_name",
          "time": "2025-05-23T21:08:42+00:00",
          "body_text": "I've shared a number of custom n8n builds and custom scrapers: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/crawlee-server What do you want next? Things I'm considering: a real enterprise grade chatbot using Chatwoot and AI agents Scraper deep dive using crawlee Workflow I used to update 50k product listings AI content creation for wordpress that doesn't seem like AI spam.  Actual scholarly articles. anything else? I'll probably do all 4 on the list at some point. Bonus points if you send me a real database of stuff to connect to, or a specific site you want data from.  I can demo a whole integration then."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1kkhh1m/i_built_an_aipowered_automation_that_creates/",
          "author": "hashpanak",
          "time": "2025-05-12T02:12:37+00:00",
          "body_text": "I automated the creation of long-form, faceless YouTube videos using AI and n8n. This system churns out engaging videos and uploads them to Google Drive and YouTube, all in a no-code workflow. It‚Äôs perfect for anyone wanting to run a YouTube channel on autopilot without breaking the bank. Use It For: Creating faceless YouTube channels for motivational, educational, or storytelling content Generating video ideas and scripts for niches like history, self-improvement, or book summaries or any other use case. Automating daily, hourlyvideo uploads to grow your audience I explain the setup here -> https://youtu.be/8sIJdMwifSE I automated from ideation to publishing using n8n, OpenAI, and Flux AI (Fal.ai or GPT 4o image generation), with no third-party video assembly APIs - no JSON2VIDEO or CREATOMATE. Here‚Äôs the blueprint. How It Works This workflow turns a niche idea into a polished YouTube video in 5 stages: Video Ideation AI-Powered Ideas: Input a niche (e.g., ‚Äúmotivational‚Äù or ‚Äúhistory‚Äù) and topics (e.g., ‚Äúphilosophy‚Äù), and an AI agent (GPT-4.1 mini) generates JSON-formatted video ideas with category, story, duration, and tone. Google Sheets: Ideas are stored in Google Sheets, ready for manual tweaks or approval. Script and Audio Generation Story Agent: A second AI agent crafts a script based on the selected idea, tailored to the duration. Text-to-Speech: OpenAI‚Äôs TTS (or 11 Labs) converts the script into audio, with word-by-word transcription for precise timing. Scene Cutting and Image Generation Scene Editor: An AI agent splits the transcript into 2-5 second scenes, ensuring dynamic pacing. achieves this by transcript granularity set to words for whisper model and using an ai agent to cut at the end of a word Visuals via Flux  (Fal.ai): Another agent generates cinematic image prompts for each scene, rendered by Flux  Schnell model ($0.003/megapixel) - one of the cheapest. Video Assembly FFmpeg Magic: A custom 700-line code node uses FFmpeg to combine audio, images, and captions, cutting scenes to match spoken words with zoom-pan effects. No Third-Party APIs: Unlike JSON2Video or Creatomate, this runs locally or via a hosted API you control, or with a Code node. Publishing Google Drive Upload: The final video lands in a designated Drive folder. YouTube Push: The workflow creates a YouTube upload link and streams the video, setting title, description, and privacy status. Why This Works Save 10+ Hours/Week: Automates scripting, editing, and uploading - no manual work, except approvals. Grow your audience on autopilot Engaging Content: Dynamic scene cuts and AI-generated visuals keep viewers hooked. Budget-Friendly: NO CREATOMATE OR JSON2VIDEO - no manual editing I explain the setup here -> https://youtu.be/8sIJdMwifSE https://preview.redd.it/rja5cl4my80f1.png?width=2604&format=png&auto=webp&s=912478508f8f565686611262d7af5afc3a7f91ae https://preview.redd.it/wa7x7c7ny80f1.png?width=1236&format=png&auto=webp&s=63b19e6d209a23ebd8a5ec478992b2d4eeb29c54 Example: https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4/view?usp=drive_link"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Automation That Creates Long-Form YouTube Videos From Scratch ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1mk0e/i_built_an_ai_workflow_that_monitors_twitter_x/",
          "author": "dudeson55",
          "time": "2025-06-02T16:30:13+00:00",
          "body_text": "Now before I get started, I know this automation may be a bit controversial as there's a lot of spam already on Twitter, but I truly believe it is possible to build a Twitter / X reply bot that is useful to people if you get your messaging down and do a good job of filtering out irrelevant messages that don't make much sense to reply to. I currently run an AI Tools directory and we noticed that each day, there are a bunch of Tweets that get posted that ask for advice on choosing the best AI Tool for a specific task or job such as \"What is the best AI Tool for writing blog posts?\" or \"What is the best AI Tool for clipping short form videos?\" Tweets like this are perfect opportunity for us to jump in, and share a link to a category page or list of tools on our directory to help them find and explore exactly what they are looking for. The problem with this is it just would take forever to do this manually as I'd have to be in front of the screen all day watching Twitter instead of doing 'real work'. So, we decided to build an AI automation that completely automates this. At a high level, we use Mention.com to monitor and alert for AI Tool questions getting asked on twitter -> use a prompt to evaluate each of these tweets individually to see if it is a good and relevant question -> fetch a list of category pages from our own website -> write a helpful reply that mentions we have a page specifically for the type of tools they are looking for. Each reply we share here doesn't amount to a ton of impressions or traffic, but ultimately this is something we believe will compound over time as it lets us have this marketing motion turned on that wasn't feasible before. Here's a full breakdown of the automation 1. Trigger / Inputs The entry point into this whole automation starts with Mention.com , we setup a new keyword alert that monitors for phrases like \"Is there any AI Tool\" or \"How can I use AI to\", etc. This setup is really important as you need to filter out a bunch of the noise that doesn't make sense to reply to. It is also important that your alert that you have setup is going to be your target customer or persona you are trying to get in front of. After the alert is configured, we used the Mention.com <> Slack integration to post the feed of all alerts into a dedicated slack channel setup just for this. 2. Initial Filtering & Validation The next couple of nodes are responsible for further filtering out ineligible Tweets that we don't want to respond too. This includes checking if the Tweet from the alert is a Retweet or if the Tweet from the alert actually was from the account we want to with (avoid our own reply causing an infinite execution loop) 3. Evaluation Prompt + LLM Call The first LLM call we make here is a simple prompt that checks the text content of the Tweet from the alert and makes a decision if we want to proceed with creating a reply or if we should exit early out of the workflow. If you are taking this workflow and extending it for your own use-case, it will be important that you change this for your own goals. In this prompt, I found it most effective to include examples of Tweets that we did want to reply to and Tweets that we wanted to skip over. 4. Build Context for Tweet Reply This step is also going to be very specific to your own goals and how you want to modify this workflow. In our case, we are making an HTTP request to our own API in order to get back a JSON list of all category pages on our website. We then take that JSON and format it nicely into more LLM-friendly text We finally take that text and will include it in our next prompt to actually write the Tweet reply If you are going to use this workflow / automation, this step must be changed and customized for the kind of reply you are trying to create. If you are trying to share helpful resources with potential leads and customers, it would be a good idea to retrieve and build up that context at this step. 5. Write The Tweet Reply In this step we take all of the context created from before and use Claude to write a Tweet reply. For our reply, we like to keep it short + include a link to one of the category pages on the AI Tools website. Since our goal is to share these pages with people asking for AI Tool suggestions, we found it most effective to include Tweet input + good examples of a reply Tweet that we would personally write if we were doing this manually. 6. Posting The Reply + Notifying In Slack The final step here was actually using the X / Twiter node in n8n to post the reply to the original Tweet we got an alert for. All that is needed here is to pass in the initial Tweet Id we need to reply to and the output of our LLM call to claude which wrote the Tweet. After that, we have a couple of Slack nodes hooked up that leave a checkmark reaction and will share the Tweet output that claude decided to go with so we can easily monitor and make changes to the prompt if we found that the reply was not quite what we were looking for. Most of the work here comes from iterating on the prompt so its important to have a good feedback loop in place so you can see what is happening as the automation runs over more and more Tweets. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Q_b5uPndsLY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that monitors Twitter (X) for relevant keywords and posts a reply to promote my business (Mention.com + X API)",
          "link": "https://old.reddit.com/r/n8n/comments/1kc5kin/efficient_serp_analysis_export_results_to_google/",
          "author": "Marvomatic",
          "time": "2025-05-01T10:37:34+00:00",
          "body_text": "Hey everyone, I wanted to share something I‚Äôve been using in my own workflow that‚Äôs saved me a ton of time: a set of free n8n templates for automating SERP analysis. I built these mainly to speed up keyword research and competitor analysis for content creation, and thought they might be useful for others here too. What these workflows do: Basically, you enter a focus keyword and a target country, and the workflow fetches organic search results, related searches, and FAQs from Google (using either SerpAPI or Serper). It grabs the top results for both mobile and desktop, crawls the content of those pages (using either Crawl4AI or Firecrawl), and then runs some analysis on the content with an LLM (I‚Äôm using GPT-4o-mini, but you can swap in any LLM you prefer). How it works: You start by filling out a simple form in n8n with your keyword and country. The workflow pulls SERP data (organic results, related searches, FAQs) for both device types. It then crawls the top 3 results (you can adjust this) and analyzes the content by using an LLM. The analysis includes article summaries, potential focus keywords, long-tail keyword ideas, and even n-gram analysis if there‚Äôs enough content. All the data gets saved to Google Sheets, so you can easily review or use it for further research. What the output looks like: At the end, you get a Google Soreadsheet with: The top organic results (URLs, titles, snippets) Summaries of each top result Extracted FAQs and related searches Lists of suggested keywords and long-tail variations N-gram breakdowns for deeper content analysis Why Three Templates? I included three templates to give you flexibility based on your preferred tools, budget, and how quickly you want to get started. Each template uses a different combination of SERP data providers (SerpApi or Serper) and content crawlers (Crawl4AI or Firecrawl). This way, you can choose the setup that best fits your needs‚Äîwhether you want the most cost-effective option, the fastest setup, or a balance of both. Personally, I‚Äôm using the version with Serper and Crawl4AI, which is pretty cost-effective (though you do need to set up Crawl4AI). If you want to get started even faster, there‚Äôs also a version that uses Firecrawl instead. You can find the templates on my GitHub profile https://github.com/Marvomatic/n8n-templates . Each template has it's own set up instructions in a sticky node. If anyone‚Äôs interested, I‚Äôm happy to answer questions. Would love to hear any feedback or suggestions for improvement!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "So far this is what I have learnt in last 6 months in n8n, what else do I need to do next so as to be able to say... work for an enterprise client needing AI automation ...may be using n8n.",
          "link": "https://old.reddit.com/r/n8n/comments/1lfcnar/from_reddit_problem_to_business_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-19T14:46:50+00:00",
          "body_text": "The best way to make money is by finding and solving real-world problems that people are actively discussing. This guide provides a step-by-step framework for building a no-code AI agent that automatically finds these business problems on Reddit and generates potential money-making solutions, helping you uncover profitable niches you might have otherwise missed. This workflow automates the entire discovery process. Here are the actionable tips to build this idea factory yourself: Step 1: Find the Problems (Reddit Node) Start your n8n workflow with the \"Reddit\" node. Configure it to search specific subreddits, like r/smallbusiness , for posts containing keywords that indicate a need, such as \"looking for a solution\" or \"recommendation\". Step 2: Filter the Noise (IF Node) Add an \"IF\" node to filter the results for quality. You can set conditions to only process posts with real engagement, like having at least two upvotes, and ensure the idea is recent by only including posts from the last six months. Step 3: AI Problem Identification Connect an AI Agent to the filtered posts. Its only job is to read the post and determine if it describes a business-related problem, then output a simple \"yes\" or \"no\". Step 4: AI Solution Generation For every post that the AI identifies as a problem, send it to another AI tool, like the OpenAI node. Prompt this AI to suggest a business idea or service that could solve the problem described in the post. Step 5: Store Your Goldmine (Google Sheets) Finally, take all the organized data‚Äîthe original post, the problem summary, and the new business idea‚Äîand save it into a Google Sheet for easy review and analysis.\nIf you can do this, you will have an automated system that finds profitable niches and delivers validated business ideas straight to a spreadsheet. Made a full video to explain everything:- https://youtu.be/Aovh878cJc4 What subreddits would you monitor with a system like this? Let me know in the comments!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How To SelfHost n8n On Railway, Setup Community Nodes, and Integrate FireCrawl MCP Server without touching a Single Line Of Code !",
          "link": "https://old.reddit.com/r/n8n/comments/1lyd7pe/four_types_of_ai_systems_that_actually_sell_and/",
          "author": "ChaseAI",
          "time": "2025-07-12T22:41:32+00:00",
          "body_text": "There was a great post on r/AI_Agents the other week that went through 6 months of Upwork data to get some insight on what sorts of AI systems companies are no shit paying money for. Believe it or not, it wasn't ASMR glass fruit cutting videos. Here is the quote: The Common Pain Points I Saw Repeated Over and Over: ‚ÄúI‚Äôm drowning in lead gen, I need this to run on autopilot‚Äù\n‚ÄúI get too many junk messages on WhatsApp / LinkedIn ‚Äî need something to filter and qualify leads‚Äù\n‚ÄúI have 10,000 rows of customer data and no time to sort through it manually‚Äù\n‚ÄúI want to turn YouTube videos into blog posts, tweets, summaries‚Ä¶ automatically‚Äù\n‚ÄúCan someone just connect GPT to my CRM and make it smart?‚Äù Exact Automations Clients Paid For: WhatsApp ‚Üí GPT lead qualification ‚Üí Google Sheets CRM\nAuto-reply bots for DMs that qualify and tag leads\nBrowser automations for LinkedIn scraping & DM follow-ups\nn8n flows that monitor RSS feeds and creates a custom news aggregator for finance companies For anyone whose been working with real clients in this space for any amount of time, this should come as no surprise. I think you can divide this up into roughly four categories: lead generation automations, lead qualification automations, content creation agents, and CRM integrated RAG agents. Those four alone cover probably 80% of the solutions I've sold since I started my agency so it makes sense to see that echoed in the data. I think people who want to get into the ai agency business get extremely overwhelmed with the breadth of \"viral\" n8n workflows shoved in their face on social media so I thought it would be helpful to not only call out the ones mentioned in this post but also run through an example for each. I'll link my YouTube video that goes over each of them in depth. The workflows for each are also freely available in the respective video description. ----- 1) Lead Generation https://www.youtube.com/watch?v=bFxWRkWAFzs User inputs the leads he wants, research is conducted on the sourced leads via Apollo + Tavily, research is used to create custom messaging, all info is then sent to Instantly.ai. I like this one since it all gets rolled into Instantly which handles all the cold email BS (email warm up, etc) and you won't have to bother with creating some fancy dashboard from scratch. 2) Lead Qualification https://www.youtube.com/watch?v=Vmgmva3dL44 This is a very simple foundation you could take a build upon to qualify leads. This uses gmail, but that could obviously be swapped for a true CRM input. Lead's info comes in, AI sorts / classifies / qualifies, and then you go down whatever path is appropriate from there. Really easy to customize and implement for a client. 3) Content Creation https://www.youtube.com/watch?v=mzxdtDCZbNY https://www.youtube.com/watch?v=Qp3h7WLYpH0 Obviously content comes in a billion different forms but it doesn't need to be complicated. Honestly I've found that clients are more pressed when it comes to just consistently posting the content across multiple platforms than they are about creating it but almost everyone wants some mega-simple LinkedIn post generator. 4) RAG https://www.youtube.com/watch?v=nwR5519zTC8 This form of RAG is as simple as it gets but I'm telling you most client's \"RAG Agent\" is a glorified FAQ chatbot that is able to locate and link internal documents to lazy employees. Understanding this very basic form of RAG will get you 90% of the way there. ----- The big takeaway is that the technical bar you must cross in order to make money selling AI solutions is not nearly as high as you think it is (the real hurdle is sourcing clients but that's an entirely different problem). The main issue is people get pulled into a million different directions trying to copy these over-engineered and flashy n8n workflows that are usually completely worthless instead of just mastering a handful of tried and true value generators. Hope this helps."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lt91sn/i_built_an_n8n_ai_agent_that_produces_educational/",
          "author": "Silent-Willow-7543",
          "time": "2025-07-06T18:59:32+00:00",
          "body_text": "In this step-by-step tutorial, I show you how to build a fully automated workflow using AI Agents and n8n to generate geography explainer videos on autopilot ‚Äî perfect for teachers, creators, or anyone looking to share knowledge at scale. What you‚Äôll learn:\n‚úÖ Generate video scripts with AI Agents\n‚úÖ Automatically source images, videos, and maps\n‚úÖ Assemble and edit videos hands-free\n‚úÖ Publish educational content consistently This workflow saves hours and helps you teach the world smarter without coding or manual editing. Tools used: LLM model (Gemini)\nGoogle APIs (Image generation)\nVideo generation: Runway ML\nVoice over (Elevenlabs)\nCaptions (Elevenlabs)\nFFmeg for combining the audio, video and captions into one MP4 file Sample video - Life in the Amazon: Uncontacted Tribes https://youtu.be/EZySnlX_yyQ"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1j7vk1j/ai_news_reporter_ai_video_ai_audio_ai_music_ai/",
          "author": "gochapachi1",
          "time": "2025-03-10T10:28:56+00:00",
          "body_text": "Workflow Do give an upvote you guys, Discover how to create a professional AI news reporter video using an automated n8n workflow! In this video, we demonstrate an end-to-end process that integrates various AI tools and automated video editing techniques to produce a fully polished news video. Here's what you'll learn: AI Video Model Generation: Automatically generate realistic video models using AI. AI Audio Creation: Generate high-quality AI audio for the model with perfect lipsync. AI Music Generation: Create custom background music using AI to add the perfect vibe to your video. Automated Editing & Transitions: Utilize advanced video editing techniques and seamless transitions with ffmpeg integrated into the n8n workflow. Complete End-to-End Automation: Watch as the entire process‚Äîfrom content creation to final editing‚Äîis fully automated, saving time and effort. Whether you're a content creator, media professional, or just curious about the power of automation and AI, this workflow offers a glimpse into the future of video production. Workflow:- https://github.com/gochapachi/AI-news-Reporter Youtube :- https://youtu.be/Km2u6193pDU If you enjoyed this video, please like, comment, and subscribe for more content on AI-driven automation and innovative video production techniques. Let's revolutionize content creation with AI and automation! üëâ Follow Us on Social Media for More Updates: üß† Reddit: https://www.reddit.com/user/gochapachi1/ üìò Facebook: https://facebook.com/gochapachi/ üì∏ Instagram: https://www.instagram.com/gochapachi/ üé• YouTube: https://www.youtube.com/@gochapachi üíº LinkedIn: https://www.linkedin.com/in/gochapachi/ üìû whatsapp: +91-8400210108 üì© Email:¬†[ sanjeevcs0034@gmail.com ](mailto: sanjeevcs0034@gmail.com )"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How I turned n8n automation into software with Supabase and Windsurf",
          "link": "https://old.reddit.com/r/n8n/comments/1lvynu2/a_fully_modular_ai_agentfree_code_with_mcps_for/",
          "author": "croos-sime",
          "time": "2025-07-10T00:12:36+00:00",
          "body_text": "This is how you can empower your AI Agents using MCPs (Model Context Protocols). MCPs are a great way to extract and reuse logic between different agents. If you're reading this and you're a developer, a quick comparison would be with a microservice or an API: you encapsulate specific logic within an MCP. If you're not, it's a very cool way to save code (rare to say this in a no-code tool) and reuse it. It's recommended that each MCP does one thing well, that it be a service focused on a single functionality. For example, one MCP specializes in Reddit-related actions, another for Calendar, and another for Supabase. A very powerful approach to starting to implement MCPs in your agents (especially since it's so easy to integrate with N8n) is to leverage this tool as the primary means of communication with the agents. It's the best way to do it because each MCP can clearly explain what it needs, and if you follow a pattern like the one I use (where each MCP has multiple actions, and each action requires a specific JSON), implementation becomes much simpler. I've been developing software for over eight years, and in my experience, this clear, modular structure allows you to maintain scalability and order as your system grows. It's a much more organized alternative than connecting 10 different nodes for each functionality within the agent. You'll simplify the structure, reduce friction in prompts, and make the agent's job easier. This is an agent I use daily because I need to search for content on Reddit, read comments, keep my database up to date, and always know if I have an upcoming meeting or class. It's tailored to my real problems and needs. But you can create as many agents as you want, and the most interesting thing is that MCPs can be reused across different agents, which increases their value. The approach I use for each MCP is that it calls a node in N8n that executes another workflow. All MCPs call the same base workflow, but they are passed a value called an action. This action determines which path the workflow will take when processing the request. Each workflow may require a different JSON, and when writing the description of the MCP (and its actions) for the agent, that content serves as a reference to know, based on the user's request, what action to use, how to use it, and what JSON to send to the MCP. This allows for flexible and precise interaction, without the need to code rigid paths for each feature. All code here: https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md In the link you have a youtube video to dive deep on it"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m4tjkj/share_a_wordpress_autopublishing_workflow/",
          "author": "gnosjah",
          "time": "2025-07-20T16:41:56+00:00",
          "body_text": "full workflow My customer recently asked me to build a fully automated content publishing system with WordPress and n8n. Naturally, I went looking for inspiration‚Äîbut to my surprise, there wasn‚Äôt a single public or usable WordPress + n8n basic workflow out there. So I rolled up my sleeves and built one from scratch. üíÄ It took more time than I expected, but the result was worth it: This workflow now runs daily, hands-free, and has already brought my client real traffic and positive ROI. They even started recommending it to others. That‚Äôs why I‚Äôve decided to share it with the community‚Äîso you can use it, improve it, or even build on it to do something better. I have posted the code here: https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json ‚∏ª Happy to give back to this amazing community üôå"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Why is reading 2 inputs so difficult in a code node?",
          "link": "https://old.reddit.com/r/n8n/comments/1m55gyt/built_an_ai_agent_in_n8n_for_youtube_market/",
          "author": "kyle4real",
          "time": "2025-07-21T01:05:56+00:00",
          "body_text": "https://preview.redd.it/3gav577mn4ef1.png?width=1920&format=png&auto=webp&s=dd788d8ebfe15a113d6bcd973cba8162c95fcfc3 Hey r/n8n !! If you're into AI automation and content creation, I just dropped a tutorial on building an AI agent in n8n that analyzes YouTube channels on the fly. You chat with it about any channel, and it scrapes video data (views, likes, duration, etc.), saves it to a Google Sheet, and answers smart market analysis questions to help brainstorm better content strategies. Why it's cool for creators/businesses: Ask stuff like: \"What are this channel‚Äôs top 5 videos?\" \"Which ones went viral?\" \"Average engagement rate?\" \"Videos with unusual view-to-like ratios?\" or \"Which underperformed?\" It pulls insights you can't get from just browsing a channel's popular vids ‚Äì perfect for niche research or competitor spying. Handles multiple channels for comparisons, with optimizations to avoid redundant API calls. Video structure: Quick demo (00:53) High-level overview (02:13) Step-by-step build: Get Channel Info (04:18), Get Channel Data (09:13), AI Agent setup (19:16), System Prompt (22:40) Testing & thoughts (24:43) What you'll learn in n8n: Creating a chat-based AI agent (using OpenAI) Setting up multi-agent workflows Capturing YouTube data via APIs/HTTP nodes Reading/updating Google Sheets Connecting credentials for YouTube, Sheets, and OpenAI It's a great example of chaining n8n nodes for real-world AI tools ‚Äì from HTTP requests and code parsing to tool calling and prompts. Watch here: https://www.youtube.com/watch?v=h_oPOkDLw28 Download template/code here: https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI?usp=sharing If you build this, share your results or mods! Anyone used n8n for similar YouTube/API projects? Tips on scaling for big channels or adding more features? Let's chat ‚Äì open to suggestions for future vids too. Thanks for the support! üöÄ (Music creds in vid desc)"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "N8n workflow for graphics/infographics with templates",
          "link": "https://old.reddit.com/r/n8n/comments/1lyb3nc/reddit_curation_automated/",
          "author": "kenmiranda",
          "time": "2025-07-12T21:04:34+00:00",
          "body_text": "Snippet of the workflow I noticed an uptick in content here and in some of the other subreddits I follow. Like a lot of people, I kept thinking ‚ÄúI just want to see more of X instead of Y\". So I built an n8n workflow that pulls posts from your favorite subreddits, runs them through a filter using upvote ratio and AI Agents, and then sends the best ones to a Slack channel. It also summarizes longer posts and checks for duplicates using Supabase. I included the SQL DDL as this will be required for this setup. It's currently setup to use supabase as the database, Openai as the LLM, and slack as the communication app. Feel free to tear it apart and make it your own. Have fun! Here is the GitHub repo: https://github.com/kenandrewmiranda/reddit-content-filter-n8n Here is just the JSON: https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json Example of an output: https://preview.redd.it/q8kd44w9cicf1.png?width=1906&format=png&auto=webp&s=9735474a772c45980318a1ea3844b9f5d2a07534"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Novice question about changing a step in workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1lrkam5/minitutorial_how_to_easily_scrape_data_from/",
          "author": "dudeson55",
          "time": "2025-07-04T14:36:43+00:00",
          "body_text": "I‚Äôve gotten a bunch of questions from a previous post I made about how I go about scraping Twitter / X data to generate my AI newsletter so I figured I‚Äôd put together and share a mini-tutorial on how we do it. Here's a full breakdown of the workflow / approaches to scrape Twitter data This workflow handles three core scraping scenarios using Apify's tweet scraper actor ( Tweet Scraper V2 ) and saves the result in a single Google Sheet (in a production workflow you should likely use a different method to persist the tweets you scrape) 1. Scraping Tweets by Username Pass in a Twitter username and number of tweets you want to retrieve The workflow makes an HTTP POST request to Apify's API using their \"run actor synchronously and get dataset items\" endpoint I like using this when working with Apify because it returns results in the response of the initial http request. Otherwise you need to setup a polling loop and this just keeps things simple. Request body includes maxItems for the limit and twitterHandles as an array containing the usernames Results come back with full tweet text, engagement stats (likes, retweets, replies), and metadata All scraped data gets appended to a Google Sheet for easy access ‚Äî This is for example only in the workflow above, so be sure to replace this with your own persistence layer such as S3 bucket, Supabase DB, Google Drive, etc Since twitterHandles is an array, this can be easily extended if you want to build your own list of accounts to scrape. 2. Scraping Tweets by Search Query This is a very useful and flexible approach to scraping tweets for a given topic you want to follow. You can really customize and drill into a good output by using twitter‚Äôs search operations. Documentation link here: https://developer.x.com/en/docs/x-api/v1/rules-and-filtering/search-operators Input any search term just like you would use on Twitter's search function Uses the same Apify API endpoint (but with different parameters in the JSON body) Key difference is using searchTerms array instead of twitterHandles I set onlyTwitterBlue: true and onlyVerifiedUsers: true to filter out spam and low-quality posts The sort parameter lets you choose between \"Top\" or \"Latest\" just like Twitter's search interface This approach gives us much higher signal-to-noise ratio for curating content around a specific topic like ‚ÄúAI research‚Äù 3. Scraping Tweets from Twitter Lists This is my favorite approach and is personally the main one we use to capture and save Tweet data to write our AI Newsletter - It allows us to first curate a list on twitter of all of the accounts we want to be included. We then pass the url of that twitter list into the request body that get‚Äôs sent to apify and we get back a list of all tweets from users who are on that list. We‚Äôve found this to be very effective when filtering out a lot of the noise on twitter and keeping costs down for number of tweets we have to process. Takes a Twitter list URL as input (we use our manually curated list of 400 AI news accounts) Uses the startUrls parameter in the API request instead of usernames or search terms Returns tweets from all list members in a single result stream Cost Breakdown and Business Impact Using this actor costs 40 cents per 1,000 tweets versus Twitter's $200 for 15,000 tweets a month. We scrape close to 100 stories daily across multiple feeds and the cost is negligible compared to what we'd have to pay Twitter directly. Tips for Implementation and working with Apify Use Apify's manual interface first to test your parameters before building the n8n workflow. You can configure your scraping settings in their UI, switch to JSON mode, and copy the exact request structure into your HTTP node. The \"run actor synchronously and get dataset items\" endpoint is much simpler than setting up polling mechanisms. You make one request and get all results back in a single response. For search queries, you can use Twitter's advanced search syntax to build more targeted queries. Check Apify's documentation for the full list of supported operators. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=otK0ILpn4GQ The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Learn Faster: 8 Common Mistakes I Correct with Every Student I Mentor",
          "link": "https://old.reddit.com/r/n8n/comments/1kr506d/i_made_a_docker_chrome_with_full_vnc_access_for/",
          "author": "conor_is_my_name",
          "time": "2025-05-20T13:34:10+00:00",
          "body_text": "TLDR: This Docker container gives you full visual control of Chrome with VNC access‚Äîperfect for scraping tricky sites, testing, or logged-in sessions. If you are new to web scraping this makes a lot of things easier! üîó GitHub Link: https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer Who is this for? Scrapers battling sites requiring logins, CAPTCHAs, or dynamic content. Developers who need to debug visually or automate complex interactions. Anyone who has wasted hours trying to make Puppeteer/Playwright work headlessly when a real browser would‚Äôve taken 5 minutes. (this is me) Stealth mode users who want the most realistic browser usage with minimal chance of detection. I made this because I wanted to do analysis on long form journalism articles.  All of my sources required logins to read the articles, and had pretty strong subscription and login checking protocols.  Even though I actually do pay for these subscriptions and have valid credentials, it was tricky to get the logins to work in headless mode. Basically, you can connect to a full GUI chrome running on a server, raspberry pi, even your own local machine, and then control it programmatically.  In my case, I remote into the GUI, log into the website as needed in a fully normal chrome browser instance, and then run my scripts. Use page.close() instead of browser.close() to end your scripts.  This will keep the browser open and ready for a new command. You will need to restart the container if you pass a browser.close() command. Why this beats headless mode: Full Chrome GUI in a container‚Äîjust like your local browser, but remote-controlled. VNC access (with audio support if needed). Pre-loaded with Puppeteer for scripting inside or outside the container. Persistent sessions (no more re-logging in every scrape). Downsides: S low R esource Heavy (but sometimes it doesn't matter: skipping login scripting and captchas can more than make up for a slow scraper) What‚Äôs inside? Chrome Stable (+ all dependencies). VNC for multiple remote access options. Puppeteer/Playwright-compatible ‚Äîuse your existing scripts. Easy volume mounts to save profiles/sessions. n8n json starter Install in 2 commands: git clone https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\n\ndocker compose up -d Then connect via VNC (default password: password ) Example n8n nodes are included: U pdate the IP address, everything else will be automatic. Use Code Node for your scripts.  This allows way more customization than using the community nodes. Tested on: 4GB+ RAM recommended for smooth VNC. Raspberry Pi 4 (ARM support included!). Here's the link to the ARM version Pair this with my n8n autoscaling setup for next-level workflow execution speed . Need n8n consulting? DM me for help with your projects."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Mini-Tutorial: How to easily scrape data from Twitter / X using Apify",
          "link": "https://old.reddit.com/r/n8n/comments/1leypz5/ai_lofi_girl_youtube_channel_automation/",
          "author": "Silent-Willow-7543",
          "time": "2025-06-19T01:45:36+00:00",
          "body_text": "This workflow is designed to fully automate your AI-powered LoFi Girl YouTube channel, enabling you to scale content creation and grow your channel with minimal ongoing effort. It handles everything‚Äîfrom generating visuals and animations to composing music and posting directly to YouTube‚Äîso you can focus on strategy, branding, or other creative pursuits. The workflow leverages multiple AI tools and APIs to streamline each part of the content creation pipeline. First, it uses Google image APIs to generate unique background images based on prompt inputs or trending aesthetic themes. These images serve as the visual foundation for each video. Next, Runway ML‚Äôs API is integrated to animate the LoFi Girl and other scene elements, bringing your visuals to life with smooth, cinematic motion. This part of the workflow adds a professional polish to your videos and helps differentiate your content in a crowded niche. Note that while Runway ML offers a powerful animation engine, it is the only paid tool in this stack. For music, the workflow connects to Segmind, a free AI tool that generates LoFi-style music tracks. These tracks are unique, royalty-free, and tuned to the vibe you specify, making it easy to build a consistent sonic brand across your uploads. Finally, the entire asset bundle‚Äîimage, animation, and audio‚Äîis compiled into a video. Using YouTube‚Äôs Data API, the workflow can auto-upload the video with relevant metadata, thumbnails, and scheduling options. You can even predefine tags and titles for SEO optimization. Here‚Äôs an example output from the workflow, showing how all these tools work together to produce a high-quality LoFi video‚Äîautomatically.\n - https://youtu.be/ISNGL9IvQik"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an agentic recruiting pipeline with n8n that analyzes resumes against job requirements - here's how",
          "link": "https://old.reddit.com/r/n8n/comments/1luuwkp/how_i_built_a_100_free_aipowered_faceless_video/",
          "author": "Valuable-Range-6785",
          "time": "2025-07-08T17:36:20+00:00",
          "body_text": "Hi everyone, I‚Äôve been automating my content creation and distribution workflow lately, and I thought I‚Äôd share something that might help those of you building with AI + no-code tools. A few days ago I created a system that: Generates faceless, illustrated AI videos automatically Schedules & posts them to all major social platforms (YouTube Shorts, TikTok, Instagram Reels, LinkedIn) Does 100% for free using open-source and free-tier tools Powered by n8n , with triggers, GPT prompts, video-generation, and posting all set up in a workflow I go through: How to set up your n8n environment (no server, no subscription) How to generate the visuals, script, and voice from text How to stitch the video together and post automatically Customizations: branding, posting cadence, scheduling logic For anyone looking to build a hands-free content pipeline or learn how to combine AI + no-code, this could be a helpful reference. The setup runs entirely on the free tier of tools! Watch the full tutorial here: üëâ https://youtu.be/TMGsnqit6o4?si=Y7sxXSV7y4yZ0D0p"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m1oqa9/i_built_a_workflow_that_creates_edit_comments_for/",
          "author": "ComfortableArm121",
          "time": "2025-07-16T21:12:34+00:00",
          "body_text": "I got tired of editing my essays on chatgpt because it was annoying to pinpoint the corrections. Now, I just get a comment of what could be better, and I can choose to accept or deny the edit. I generated this with https://alpha.osly.ai/ and I've attached the code here https://github.com/Osly-AI/google-doc-editor ."
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lvht0i/i_built_an_automation_to_autoorganize_my_inbox/",
          "author": "Milan_SmoothWorkAI",
          "time": "2025-07-09T12:42:44+00:00",
          "body_text": "Discover how to easily extend an AI-generated video from a single image using KAI.AI , VEO3, and automation tools like NCA Toolkit and FFmpeg. Learn how to automate video sequences, extract key frames, and create longer, more dynamic videos ‚Äî perfect for creative projects or content creation! Video: https://www.youtube.com/watch?v=qc00jJ7bCBA Workflow: https://pub-7710cf36e9f24295acffe6938f40f147.r2.dev/Extend%20Video%20with%20NCA%20Toolkit.json"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lebpkb/from_reddit_complaint_to_saas_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-18T08:18:32+00:00",
          "body_text": "I automated a full AI-powered video pipeline that creates faceless vertical stories like the miniature worker / tiny character genre you‚Äôre seeing blow up right now. Why? Because writing, designing, animating, soundtracking, and publishing even one of these manually is a multi-hour grind - and I wanted to publish daily without touching CapCut. So I built an end-to-end workflow in n8n that does it all, 100% automated. Perfect for: AI-generated short stories & mini animations YouTube Shorts / Instagram Reels / TikTok Faceless vertical video channels (well you can adapt it for 16:9 as well - change the aspect ratio) Clients who want storytelling content with AI flair Follow along this tutorial here. Explanation here : https://youtu.be/Z1n6nU9O0BA Example video with the lowest settings https://youtube.com/shorts/LN46liFamoY üß† How It Works The system turns a scheduled trigger into a fully animated AI-generated story , complete with voiceover, visuals, and sound FX, and background music. Here‚Äôs the full 8-step pipeline: 1. Story Idea Generation GPT-4 creates original video ideas e.g., ‚ÄúTiny workers making a large pizza‚Äù 2. Scene Breakdown + Visual Planning Each story is split into multiple scenes with structured prompts and reference descriptions. 3. Reference + Scene Images via FAL / FreePik FAL‚Äôs image generation API creates high-quality scene visuals - one per segment, based on the story and setting. In the workflow you have option for using fal/general for using lora's and controlnets if you wish. Freepik integration is also there - Started with this, kept in the workflow (good images, but slow generation) - absolutely free image generation - with rate limiting of course. 4. Scene Animation via Kling  - or Minimax (ran out of minimax's $50 dollar credit so Swtiched to Kling via Fal) Each image is passed to an animation engine that gives movement, transitions, and subtle effects (like camera pan or zoom) for that short-form storytelling vibe. 5. Bacground music + SFX from ElevenLabs AI-generated voiceover narrates the story, matched with ambient music and sound effects, all created using ElevenLabs. 6. Video Compilation via FFmpeg A custom command stitches the scenes, SFX, and music together, handles crossfades, syncs timing, and applies volume balancing. No JSON2Video or Creatomate - they can become expensive. 7. YouTube Upload The finished video is uploaded directly to YouTube Shorts via resumable upload. Upload to reels and Tiktok (coming soon) 8. Logged in Google Sheets Each published video is logged for tracking, repurposing, or reuse. üîß Tools Used Function Tool Idea & Script OpenAI GPT-4 Images & Scenes FAL (Flux) with lora support Animation Hailuo API / Kling Voice + SFX ElevenLabs Stitching FFmpeg Automation n8n Publishing YouTube API Tracking Google Sheets ‚öôÔ∏è Key Features No video editing required. Fully modular, works scene-by-scene. Customize prompt style for any genre (horror, fantasy, sci-fi, slice of life). Built in n8n , no-code setup. Ideal for scaling faceless content at volume. AI is not perfect always, it will make mistakes üöÄ Why This Works Saves 10 - 15 hours/week Produces daily, high-quality, AI-first content Scalable for multiple channels or client brands Zero editing tools or manual uploads needed Great for storytelling niches, explainer content, or abstract visual art Explanation here: https://youtu.be/Z1n6nU9O0BA"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Create Powerful AI Videos with Veo 3 Fast in n8n ‚Äì No KYC or Org Verification Needed!",
          "link": "https://old.reddit.com/r/n8n/comments/1m1octi/create_powerful_ai_videos_with_veo_3_fast_in_n8n/",
          "author": "automayweather",
          "time": "2025-07-16T20:57:43+00:00",
          "body_text": "Just built a seamless automation using n8n and the Veo 3 Fast API from Ulazai.com. Automatically generates AI videos in seconds Cheapest pricing available for Veo 3 Fast (just $1 per video!) No KYC, instant API access Supports the new vertical (9:16) aspect ratio Bonus: use OpenAI‚Äôs image generation without organizational verification‚Äîespecially helpful now after OpenAI‚Äôs recent model update! ulazai.com - VEO 3 Fast Ideal for automation enthusiasts, marketers, and developers looking to leverage cutting-edge AI content creation without complex setup. üëâ Quick-start guide & API docs + Workflow**:** https://ulazai.com download the workflow: https://gist.github.com/smrht/82c072241a679ae827421a1c61fd35b9"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Built my first advanced flow! Help me simplify it?",
          "link": "https://old.reddit.com/r/n8n/comments/1i66chs/i_used_ai_and_n8n_to_automate_youtube_shorts/",
          "author": "Horizon-Dev",
          "time": "2025-01-21T00:54:02+00:00",
          "body_text": "Hey everyone! I've been experimenting with n8n and AI automation to streamline the process of creating YouTube Shorts , and I wanted to share my results. In this video, I walk through how I: ‚úÖ Used n8n to automate the entire workflow ‚úÖ Integrated AI tools to generate content dynamically ‚úÖ Automated script creation ‚úÖ Grabbed Videos to clip together ‚úÖ Generated voice, and grabbed music ‚úÖ Optimized the system to make the process scalable If you guys have questions, or would do something differently let me know! üîó Watch the full breakdown here: https://youtu.be/g3XsT_55xTA"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Is AI about to become the most groundbreaking productivity tool in human history, outpacing the wheel and the internet?",
          "link": "https://old.reddit.com/r/n8n/comments/1lq3glp/showoff_i_automated_trustpilot_outreach_review/",
          "author": "automayweather",
          "time": "2025-07-02T18:28:50+00:00",
          "body_text": "I just built an automation that turns Trustpilot reviews into actual lead gen opportunities ‚Äî especially useful for freelancers, outreach agencies, and DFY reputation management services. üîß What it does (fully automated via n8n): Scans any Trustpilot profile (even unclaimed ones) Detects if the business already responded to reviews Uses Google Gemini (via OpenRouter) to write realistic, helpful replies Sends a clean HTML report via Gmail to the business Logs everything in Google Sheets with timestamps & outreach status Works with Dutch üá≥üá± and English üá¨üáß reviews (auto-detect) üí° Why it matters: Most ecommerce stores don‚Äôt reply to reviews at all. That‚Äôs a huge reputational gap ‚Äî and a perfect entry point for value-based outreach. Instead of spamming \"hey, want SEO?\", you send: üì¨ ‚ÄúHere‚Äôs what your customers said. Here‚Äôs how you could reply. Want to automate it?‚Äù ‚Üí Value first. Pitch second. üí• Built with: n8n (open source Zapier alternative) OpenRouter (Google Gemini model) Gmail Google Sheets üìΩÔ∏è Loom walkthrough + full workflow template available. https://reddit.com/link/1lq3glp/video/0tte6vqi7iaf1/player {\n  \"name\": \"Review Responder & Content Trustpilot V2 Samautomation.work\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"operation\": \"appendOrUpdate\",\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"columns\": {\n          \"mappingMode\": \"defineBelow\",\n          \"value\": {\n            \"TrustPilotUrL\": \"={{ $('Limit').item.json.TrustPilotUrL }}\",\n            \"Claimed\": \"={{ $('Limit').item.json.Claimed }}\",\n            \"Contacted\": \"={{ $now }}\",\n            \"Email\": \"={{ $('Limit').item.json.Email }}\"\n          },\n          \"matchingColumns\": [\n            \"TrustPilotUrL\"\n          ],\n          \"schema\": [\n            {\n              \"id\": \"TrustPilotUrL\",\n              \"displayName\": \"TrustPilotUrL\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Claimed\",\n              \"displayName\": \"Claimed\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Added\",\n              \"displayName\": \"Added\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Contacted\",\n              \"displayName\": \"Contacted\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Email\",\n              \"displayName\": \"Email\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            }\n          ],\n          \"attemptToConvertTypes\": false,\n          \"convertFieldsToString\": false\n        },\n        \"options\": {\n          \"useAppend\": true\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        2520,\n        -180\n      ],\n      \"id\": \"d0c3ea12-2a69-47ba-822a-b79f5eb4a44c\",\n      \"name\": \"Update Row Geplaatst\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"extractHtmlContent\",\n        \"extractionValues\": {\n          \"values\": [\n            {\n              \"key\": \"article_html\",\n              \"cssSelector\": \".styles_reviewCard__Qwhpy\",\n              \"returnValue\": \"html\",\n              \"returnArray\": true\n            },\n            {\n              \"key\": \"email_html\",\n              \"cssSelector\": \"a[href^=\\\"mailto:\\\"]\"\n            },\n            {\n              \"key\": \"unclaimed_profile\",\n              \"cssSelector\": \"div[class*=\\\"styles_label\\\"]\",\n              \"returnArray\": true\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"8ecff1e8-b4ee-49b4-9441-7e8d83f1c676\",\n      \"name\": \"Extract Article URLs\",\n      \"type\": \"n8n-nodes-base.html\",\n      \"position\": [\n        2000,\n        -540\n      ],\n      \"typeVersion\": 1.2\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"data\"\n            }\n          ]\n        },\n        \"options\": {\n          \"mergeLists\": true\n        }\n      },\n      \"id\": \"6c617d6b-1651-4d62-b77b-b18bf86698e4\",\n      \"name\": \"Combine Articles\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        2420,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"html\": \"={{ $json.article_html }}\",\n        \"options\": {}\n      },\n      \"id\": \"88aa1a9b-ebd0-4e87-afd6-b5ae6007e426\",\n      \"name\": \"Markdown\",\n      \"type\": \"n8n-nodes-base.markdown\",\n      \"position\": [\n        2220,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.UpdatedTrustpilotURL }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        1780,\n        -540\n      ],\n      \"id\": \"0c289d7a-b90b-4a3d-974e-6f20babd9384\",\n      \"name\": \"HTTP Request1\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Je reageert als een echt persoon, niet als AI. Gebruik NOOIT volgende corporate/AI-zinnen in welke taal dan ook.\\n\\nINHOUD REVIEW:\\n{{ $json['data[0]'] }}\\n\\nInstructies:\\n- Reageer namens de webshop\\n- Gebruik dezelfde taal als de review\\n- Ga concreet in op de aangehaalde punten\\n- Houd het professioneel √©n persoonlijk\\n- Lengte: 2-4 zinnen\\n- Gebruik geen typische AI-woorden!\\n\\nGeef alleen de tekst van de reactie terug, zonder verdere opmaak.\\n\",\n        \"options\": {\n          \"systemMessage\": \"=Je bent een warme, persoonlijke klantenservicemedewerker voor Nederlandse webwinkels. Schrijf korte, oprechte reacties op klantbeoordelingen in vloeiend Nederlands.\\n\\n## EERSTE STAP - VERPLICHTE CONTROLE\\n\\n**DENK EERST NA:** Analyseer de ontvangen data grondig voordat je begint met schrijven.\\n\\n**CONTROLEER ALTIJD EERST:** Is er al een reactie van het bedrijf aanwezig in de data? \\n- Zoek naar termen zoals: \\\"reactie van\\\", \\\"antwoord van\\\", \\\"bedrijf reageerde\\\", \\\"webshop antwoordt\\\", of vergelijkbare tekst die duidt op een bestaande bedrijfsreactie\\n- Zoek naar gestructureerde reacties die niet van de klant afkomstig zijn\\n- Let op tekst die duidelijk een professioneel antwoord van de webshop is\\n\\n**ALS ER AL EEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Stop onmiddellijk met verder analyseren\\n- Schrijf geen nieuwe reactie\\n- GEEF ALLEEN DEZE OUTPUT: \\\"{bedrijfsnaam} heeft al gereageerd.\\\"\\n\\n**ALS ER GEEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Ga verder met het schrijven van een nieuwe reactie volgens onderstaande regels\\n\\n## BELANGRIJKE REGELS\\n\\n- Gebruik nooit een em-dash (‚Äî); gebruik een gewoon koppelteken (-)\\n- Laat elke reactie natuurlijk klinken ‚Äì nooit als AI\\n- Wissel begroetingen en afsluitingen af - nooit 2x dezelfde achter elkaar\\n\\n**Negatieve reviews:**\\n- Als {{ $('Limit').item.json.Email }} een waarde heeft ‚Üí eindig de reactie met: \\\"Mail ons op {{ $('Limit').item.json.Email }} zodat we je direct verder kunnen helpen.\\\"\\n- Heeft het veld g√©√©n waarde ‚Üí noem geen e-mail\\n\\n## SCHRIJFSTIJL\\n\\n- Begin met \\\"Beste [Naam]\\\". Geen naam? Gebruik \\\"Beste klant\\\"\\n- Verwerk concrete details uit de review (product, datum, levering, enz.)\\n- Toon oprechte empathie, vermijd corporate jargon\\n- Gebruik dezelfde aanspreekvorm (je/jij of u) als de klant\\n- Sluit af met een uitnodiging of hulpaanbod - wissel dit af\\n\\n## STRUCTUUR (2-4 zinnen)\\n\\n1. **Begroet & bedank:** \\\"Beste [Naam], dank voor ‚Ä¶\\\" - Wees creatief en varieer\\n2. **Reageer specifiek:** Ga in op de concrete punten uit de review\\n3. **Toon waardering/empathie:** \\n   - Positief ‚Üí benadruk blijdschap\\n   - Negatief ‚Üí erken probleem, bied oplossing (+ e-mail indien beschikbaar)\\n4. **Nodig uit tot vervolg:** Varieer je afsluitingen\\n\\n## RESULTAAT\\n\\nLever alleen de tekst van de reactie, zonder verdere opmaak; 2-4 zinnen, warm, menselijk en in helder Nederlands.\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        820,\n        -180\n      ],\n      \"id\": \"466305a5-312e-4080-95ad-a2785fe953e2\",\n      \"name\": \"Review Responder\",\n      \"executeOnce\": false\n    },\n    {\n      \"parameters\": {\n        \"fieldToSplitOut\": \"data[0]\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitOut\",\n      \"typeVersion\": 1,\n      \"position\": [\n        2620,\n        -540\n      ],\n      \"id\": \"a8cd687e-db79-46c9-a2dc-e11e6ad10504\",\n      \"name\": \"Split Out\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Haal de originele URL en het gewenste paginanummer op\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\nconst pageNumber = 1;\\n\\n// Controleer of de originele URL al een paginaquery bevat\\nlet updatedUrl;\\nif (originalUrl.includes('?page=')) {\\n  // Vervang bestaande pagina-parameter\\n  updatedUrl = originalUrl.replace(/(\\\\?page=)(\\\\d+)/, `$1${pageNumber}`);\\n} else if (originalUrl.includes('?')) {\\n  // Voeg toe als er al andere queryparameters zijn\\n  updatedUrl = `${originalUrl}&page=${pageNumber}`;\\n} else {\\n  // Voeg toe als er nog geen queryparameters zijn\\n  updatedUrl = `${originalUrl}?page=${pageNumber}`;\\n}\\n\\n// Geef de nieuwe URL terug in de output\\nreturn [\\n  {\\n    json: {\\n      ...$json,\\n      UpdatedTrustpilotURL: updatedUrl\\n    }\\n  }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1580,\n        -540\n      ],\n      \"id\": \"ed03ac93-fc67-4e54-b1b7-07cfcac5104d\",\n      \"name\": \"Add Page to URL\"\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"output\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1180,\n        -180\n      ],\n      \"id\": \"5b9793b6-a45b-496f-aa4a-797450ea6f52\",\n      \"name\": \"Aggregate\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üìà {{ $json.bedrijfsnaam }} klantrespons rapport - Gratis\",\n        \"message\": \"={{ $('HTML parser').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -20\n      ],\n      \"id\": \"2021c9b1-c1db-4580-a05e-ef71a7a27d7e\",\n      \"name\": \"Send Review Respsonse\",\n      \"webhookId\": \"32b66b21-c68d-4386-b1db-3a11f8e9e2ea\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"rule\": {\n          \"interval\": [\n            {\n              \"field\": \"minutes\",\n              \"minutesInterval\": 15\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        800,\n        -540\n      ],\n      \"id\": \"1f2df98b-7a80-425d-9831-cdb2d9825b97\",\n      \"name\": \"Schedule Trigger\"\n    },\n    {\n      \"parameters\": {\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"filtersUI\": {\n          \"values\": [\n            {\n              \"lookupColumn\": \"Claimed\",\n              \"lookupValue\": \"Geclaimd profiel\"\n            },\n            {\n              \"lookupColumn\": \"Contacted\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        1000,\n        -540\n      ],\n      \"id\": \"69b6abc7-6df7-4f7a-a01d-1dab75c550b0\",\n      \"name\": \"Google Sheets\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1360,\n        60\n      ],\n      \"id\": \"4fc10a76-c8a3-4651-8e4e-733db12b624e\",\n      \"name\": \"OpenRouter Chat Model2\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"={{ $('Customer Response Aggregatio').item.json.output }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Nederlandse Klantrespons Analyse Agent (JSON Output)\\n\\n\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1960,\n        -180\n      ],\n      \"id\": \"74977264-1c23-4b4a-ad54-9bd0894df187\",\n      \"name\": \"International Customer Response Analysis Agent\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Reviews: {{ $json.output }}\\nExtract van de trustpilot URL de bedrijfsnaam: {{ $('Limit').first().json.TrustPilotUrL }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Klantrespons-Aggregatie-Specialist met Gepersonaliseerde Ice Breaker\\n\\n## Rol: \\nn```\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1380,\n        -180\n      ],\n      \"id\": \"adac1c81-9ccf-4725-883a-f351d8ee7b83\",\n      \"name\": \"Customer Response Aggregatio\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"d83b559f-1dfb-414c-9365-69e7193b77af\",\n              \"leftValue\": \"={{ $json.Email }}\",\n              \"rightValue\": \"NO EMAIL\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"notEquals\"\n              }\n            }\n          ],\n          \"combinator\": \"and\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        1180,\n        -540\n      ],\n      \"id\": \"1b53040d-986f-4e2f-8d42-788481c65a8b\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {},\n      \"type\": \"n8n-nodes-base.limit\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1400,\n        -660\n      ],\n      \"id\": \"24401a8f-8657-411b-9723-a369ecb56efb\",\n      \"name\": \"Limit\"\n    },\n    {\n      \"parameters\": {\n        \"content\": \"## Create Subworkflow\\n\\nStuur de NO EMAIL naar de subworkflow.\"\n      },\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1360,\n        -900\n      ],\n      \"typeVersion\": 1,\n      \"id\": \"22618369-e14f-4818-898f-89eea60d889a\",\n      \"name\": \"Sticky Note\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {\n          \"frequencyPenalty\": 0.5,\n          \"presencePenalty\": 0.7,\n          \"temperature\": 0.9\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        800,\n        40\n      ],\n      \"id\": \"e266310b-6362-47b0-b399-e5845a28f859\",\n      \"name\": \"OpenRouter Chat Model\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -300\n      ],\n      \"id\": \"d1de90ed-0b98-4534-8d0b-a08de0dbeac4\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -20\n      ],\n      \"id\": \"63fb494e-6878-4fed-b70d-37075f81cb3c\",\n      \"name\": \"Code1\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üîç Dit zeggen klanten over {{ $json.bedrijfsnaam }}\",\n        \"message\": \"={{ $('Code2').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -300\n      ],\n      \"id\": \"e5491ac2-0fc7-4f4e-9b52-2ccc26de78ff\",\n      \"name\": \"Review Analyse Rapport\",\n      \"webhookId\": \"b70f3f19-ee55-4572-9445-abe53056511c\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"description\": \"Use the tool to think about and analyze the review data before responding. It will not obtain new information or change the database, but just append your analysis to the log.\\nUse it specifically for:\\n\\nReviewing data structure: Analyze the incoming review data to identify all components (reviewer name, content, rating, date, etc.)\\nDetecting existing responses: Check if there's already a business response present in the data before writing a new one\\nLanguage analysis: Determine the language and tone of the review to match it appropriately\\nContent analysis: Identify key points mentioned (products, service aspects, specific complaints/compliments)\\nStrategy planning: Decide on the appropriate response approach (positive reinforcement, problem resolution, etc.)\\nQuality checking: Review your planned response before finalizing to ensure it meets the guidelines\\n\\nAlways think first before writing any review response to ensure proper analysis and avoid duplicate responses.\"\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.toolThink\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1000,\n        40\n      ],\n      \"id\": \"f3a6ce09-2bea-4701-99e4-8b403a0f6957\",\n      \"name\": \"Think\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1940,\n        40\n      ],\n      \"id\": \"11a5d333-abb6-45a2-96b9-1b5efcdc7dc0\",\n      \"name\": \"OpenRouter Chat Model3\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"//timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1740,\n        -180\n      ],\n      \"id\": \"064c4a24-fe58-4944-b3fc-38b7d1750f6f\",\n      \"name\": \"HTML parser\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// \\n        timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2320,\n        -180\n      ],\n      \"id\": \"14b30bdd-c774-45b0-ad98-e9c3a14da488\",\n      \"name\": \"Code2\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Update Row Geplaatst\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Code1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Article URLs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Markdown\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Markdown\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Combine Articles\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTTP Request1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Extract Article URLs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Combine Articles\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Out\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Out\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Responder\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Add Page to URL\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Schedule Trigger\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Sheets\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Sheets\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model2\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"International Customer Response Analysis Agent\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code2\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Customer Response Aggregatio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTML parser\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Limit\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        []\n      ]\n    },\n    \"Limit\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Add Page to URL\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Analyse Rapport\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Review Respsonse\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Analyse Rapport\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"Think\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model3\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTML parser\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code2\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Update Row Geplaatst\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": true,\n  \"settings\": {\n    \"executionOrder\": \"v1\",\n    \"callerPolicy\": \"workflowsFromSameOwner\",\n    \"errorWorkflow\": \"SGT2yyLX0uoBgIBI\"\n  },\n  \"versionId\": \"895363e2-bac3-46f0-b678-f31d7b874eea\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"190e1b81e9845aeb2f58812b2e7f9c8a86cb13b1e8883f6c2bfaa8050710ae79\"\n  },\n  \"id\": \"kuYaiJDh24Ta0NuG\",\n  \"tags\": []\n}"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1luvgkn/create_an_aipowered_upwork_job_scorer_and/",
          "author": "paperjamai",
          "time": "2025-07-08T17:57:02+00:00",
          "body_text": "Workflow code: https://jsonblob.com/1392186102303088640 YouTube video walkthrough: https://youtu.be/fVegeKYAzz8 Disclaimer: In order to use this workflow you'll have to subscribe to Upwork's Freelancer Plus plan, which is $20/mo and includes job alert email notifications (among other things). I assume most people who frequently work on Upwork are already subscribed, but don't want any surprises. --- When applying for freelance jobs on Upwork, minutes matter. The first quality application is more often than not the one that‚Äôs ultimately selected. Upwork's job alert emails are helpful, but filters are very limited. As a result, you often receive several dozen emails a day and it takes a lot of time to manually go through each email and determine if each job fits your criteria. I built a workflow that scans your Gmail every few minutes, finds all Upwork job alerts, scores them based on your profile/preferences, and sends a Slack channel message for jobs that are strong potential matches. I use this workflow myself and find that I've been applying to (and winning) more jobs than before, despite the fact that it filters out most alerts. The ones that come through are the right ones for me, and I have more confidence that I'm acting quickly and not wasting my time. Hopefully you find it easy to both get up and running and customize to your liking. Very open to feedback/questions!"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From Star Trek to N8N: How I turned my 20 years of Corporate Ops experience into real-world automations",
          "link": "https://old.reddit.com/r/n8n/comments/1lkwqz7/multilevel_rag_ai_agent_guide/",
          "author": "Aggravating-Put-9464",
          "time": "2025-06-26T10:32:52+00:00",
          "body_text": "Hi! I had a client with >70 files in knowledge base - docs, presentation, tables and videos, who was needed to build RAG agent for internal use. The problem is that files have different overlapping data that doesn't suit for each request - we decided to add additional step for data retrieval to have more focused results. I built a two-step retrieval pipeline using Supabase + n8n, where you first filter files by their description/metadata, then only run vector search within those relevant files. The main problem that there is no way in original Supabase nodes in n8n to setup metadata filtration via array of values - there is hardcoded \"equal\" condition. To solve it I made custom tools with code and special Supabase functions. Video guide: https://youtu.be/asXVOHg89hs How it works: 1. When chat message received Starts the workflow whenever a user sends a question to the chatbot. 2. Find Files Takes the user‚Äôs query Generates an embedding (OpenAI) Runs a semantic search (via Supabase match_files function) Returns a list of files (with their IDs & similarity) that are the best match, based on the file description/metadata‚Äînot full file content yet 3. Filter Similarity Filters file matches so only those above a certain similarity score are kept Controls ‚Äúhow relevant‚Äù is relevant enough Useful for not passing irrelevant/weak matches 4. Aggregate Files Collects all matching file IDs into a list/array This is your shortlist of files to actually search in the next step 5. Database Retrieval (Knowledge Base) Takes the user‚Äôs original query Uses OpenAI to generate embedding again Runs Supabase vector search (match_documents) but filters by the file IDs from the previous step Returns only chunks from the shortlisted files, sorted by similarity, with file names and URLs 6. OpenAI (LLM Completion) Feeds the filtered, relevant chunks to the LLM to answer the user‚Äôs question Always returns the file URL as reference 7. Set Output Prepares and formats the final output for the user/chatbot"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lxxzcw/showcase_smarter_chatbots_with_n8n_agent_node/",
          "author": "Maamriya",
          "time": "2025-07-12T11:15:52+00:00",
          "body_text": "Smarter Chatbots with n8n Agent Node & OpenAI: Text + Voice, Step-by-Step I want to share a practical structure for building next-level chatbots and assistants by combining the n8n Agent node with OpenAI‚Äîhandling both text and voice messages in Telegram, all with an AI agent. What‚Äôs unique about this approach is how seamlessly n8n acts as the bridge: taking a message from Telegram, sending it to OpenAI via the API, receiving the AI-generated answer, and then passing it back to the user‚Äîall within your workflow. This transforms n8n into a true orchestrator of conversations, letting the Agent node manage message routing, AI logic, and response delivery in one automated loop. Here‚Äôs a high-level view: A user sends a message (text or voice) to your Telegram bot. n8n captures that message and, if needed, transcribes voice to text. n8n passes the user‚Äôs message to OpenAI via API (through the Agent node). OpenAI generates a reply ‚Äîas smart or as specific as you want, guided by your system prompt. The answer is returned to n8n , which then handles sending the reply straight back to your Telegram user‚Äîcompleting the loop. This lets you build real conversational AI bots with no code, using just nodes, flows, and your own creativity. üîπ The Core Idea AI everywhere: The Agent node lets you plug advanced LLMs (OpenAI, Claude, Grok, etc.) right into n8n. Not just text: My workflow also handles Telegram voice notes‚Äîthese get transcribed, then processed by the AI agent. Unified logic: Whether the user types or talks, the agent understands and replies‚Äîinstantly. üõ†Ô∏è How I Built It: Step-By-Step (Technical Outline) Here‚Äôs the practical structure so you can recreate (or adapt) it: Trigger (Telegram node): Set up a bot in Telegram and connect it to n8n. The trigger is ‚ÄúOn Message,‚Äù capturing every message (text/voice). Switch node (Type Check): Branch workflow: Is the message text or voice? Use a Switch node to check if message.text exists (text) or message.voice exists (voice). Text Path: If it‚Äôs text, pass the message content directly toward the Agent node. Voice Path: If it‚Äôs a voice note: Get File: Use Telegram‚Äôs ‚ÄúGet File‚Äù node to download the voice message using its File ID. Transcribe: Add the OpenAI ‚ÄúTranscribe Audio‚Äù node (Whisper) to convert voice to text. Output: You now have clean text, ready for the AI agent. Agent Node (The AI Core): Add the n8n Agent node after both paths (merge/join if needed). Select your model (e.g., OpenAI Chat). Configure a System Prompt to guide the AI agent‚Äôs tone/behavior (e.g., ‚ÄúYou are a helpful assistant. Answer every question clearly and professionally.‚Äù). Pass in the user message (original text or transcribed text). Behind the scenes: The Agent node sends the message to OpenAI‚Äôs API, gets the answer, and hands it back to your workflow. Reply (Telegram ‚ÄúSend Message‚Äù node): Take the output from the Agent node (AI reply). Send it back to the user in Telegram via their Chat ID. (Optional) : Log chats, add extra steps (e.g., Sheets, Notion), or expand the flow based on use case. No code needed‚Äîjust node configuration and logical connections! üí° What Makes This Special? Handles both text and voice in one clean flow. Supports multiple LLMs‚Äîswap OpenAI for Claude, Grok, Mistral, etc. System prompt makes it easy to customize your AI agent‚Äôs ‚Äúpersonality.‚Äù Reusable for other platforms: WhatsApp, Discord, web forms, and more. üöÄ What You Can Build 24/7 smart Telegram/WhatsApp bots. Voice-based Q&A or help desk agents. Multichannel support workflows, all using the same logic. üé• Full Video Tutorial Want the full step-by-step tutorial with screen sharing and live build ? https://www.youtube.com/watch?v=EYxBm42ja0k"
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1loejcq/how_i_save_time_with_n8n_as_a_content_creator/",
          "author": "prometheus_one",
          "time": "2025-06-30T18:02:14+00:00",
          "body_text": "I was spending 4+ hours per video just reformatting the same content for different platforms. YouTube descriptions, Twitter threads, LinkedIn posts, Instagram captions - pure manual hell. So I built an AI workflow that does it all automatically. The magic: Paste transcript ‚Üí Hit \"start\" ‚Üí Get optimized content for 5+ platforms in 2 minutes. My N8N + Airtable + Gemini AI system generates everything: SEO titles, platform-specific captions, hashtag strategies, even creates organized Google Drive folders. Manual content creation in 2025 = digital manual labor. Work smarter, not harder. Want the complete workflow? I'm sharing the N8N JSON, Airtable templates, and setup guide for free: https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk/ Next i will build the content validation and autoposting with postiz. i found with ai you typically have to challenge the initial outcome to get a better result."
        }
      }
    ],
    "Code": [
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Now Hiring or Looking for Cofounder",
          "link": "https://old.reddit.com/r/n8n/comments/1kzaysv/i_built_a_workflow_to_scrape_virtually_any_news/",
          "author": "dudeson55",
          "time": "2025-05-30T18:00:38+00:00",
          "body_text": "I run a daily AI Newsletter called The Recap and a huge chunk of work we do each day is scraping the web for interesting news stories happening in the AI space. In order to avoid spending hours scrolling, we decided to automate this process by building this scraping pipeline that can hook into Google News feeds, blog pages from AI companies, and almost any other \"feed\" you can find on the internet. Once we have the scraping results saved for the day, we load the markdown for each story into another automation that prompts against this data and helps us pick out the best stories for the day. Here's how it works 1. Trigger / Inputs The workflow is build with multiple scheduled triggers that run on varying intervals depending on the news source. For instance, we may only want to check  feed for Open AI's research blog every few hours while we want to trigger our check more frequently for the 2. Sourcing Data For every news source we want to integrate with, we setup a new feed for that source inside rss.app. Their platform makes it super easy to plug in a url like the blog page of a company's website or give it a url that has articles filtered on Google News. Once we have each of those sources configured in rss.app, we connect it to our scheduled trigger and make a simple HTTP request to the url rss.app gives us to get a list of news story urls back. 3. Scraping Data For each url that is passed in from the rss.app feed, we then make an API request to the the Firecrawl /scrape endpoint to get back the content of the news article formatted completely in markdown. Firecrawl's API allows you to specify a paramter called onlyMainContent but we found this didn't work great in our testing. We'd often get junk back in the final markdown like copy from the sidebar or extra call to action copy in the final result. In order to get around this, we opted to actually to use their LLM extract feature and passed in our own prompt to get the main content markdown we needed (prompt is included in the n8n workflow download). 4. Persisting Scraped Data Once the API request to Firecrawl is finished, we simply write that output to a .md file and push it into the Google Drive folder we have configured. Extending this workflow With this workflow + rss.app approach to sourcing news data, you can hook-in as many data feeds as you would like and run it through a central scraping node. I also think for production use-cases it would be a good idea to set a unique identifier on each news article scraped from the web so you can first check if it was already saved to Google Drive. If you have any overlap in news stories from your feed(s), you are going to end up getting re-scraping the same articles over and over. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=2uwV4aUyGIg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json",
            "https://www.youtube.com/watch?v=2uwV4aUyGIg"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Introducing My AI Automation Workflow with n8n for Social Media Posting!",
          "link": "https://old.reddit.com/r/n8n/comments/1jv9ikl/i_built_an_aipowered_social_media_machine_that/",
          "author": "hashpanak",
          "time": "2025-04-09T16:07:19+00:00",
          "body_text": "I automated social posting to 6 different platforms. Why? I wanted to build authority on my socials in the AI space, and I was putting it off for a while simply because I haven't got the time or the energy to do social media. But now I have leverage. and don't have to sign up for expensive tools. Use it for: - inbound lead-gen - growing authority on socials - set up for your clients I explain the setup here -> https://youtu.be/D1iKOayS270 I automated everything ‚Äîfrom research to posting‚Äîusing n8n ,  here's the blueprint, also see the images. This system generates viral-ready content (of course you can customise the style in the prompt), lets you approve it from your phone, and posts across LinkedIn, Instagram, X, Threads, YouTube, and more. How It Works This workflow turns raw topics into polished posts in 4 stages: AI-Powered Research & Content Creation Topic Deep Dive : Input a keyword (e.g., ‚ÄúSEO‚Äù), and AI scours Google, extracts trending subtopics, and builds a research doc. (goes through people also search for and sub topics) Multi-Platform Content : Generates LinkedIn posts, Twitter threads, Instagram reels scripts, and YouTube shorts scripts‚Äîall hyper-tailored to each platform‚Äôs style. Give examples to train in your style in the Agent prompts) Dynamic Scripts depending on the research found : For reels/shorts, get ready-to-record scripts like: ‚ÄúI wish someone told me this SEO secret sooner. Google doesn‚Äôt care about your website. Here‚Äôs why‚Ä¶‚Äù On-the-Go Review via Telegram Review and approve from your phone : Get notified on Telegram to review drafts. Approve, request edits, or upload video links directly from your phone ( So you can be productive even if you are out and about ) . Automated Publishing & Scheduling Independent Schedules in one System : Set different schedules for different platforms, add more platforms because of the modular nature of the workflow.  Go live at optimal times (e.g., LinkedIn every Tuesday at 2 PM). Reels and Shorts (TikTok upload is manual) : Upload Instagram reels/YouTube shorts by pasting a Google Drive link‚ÄîThe agent downloads and posts the video to shorts and reels, TikTok is not yet done, (if you know how to do it, let me know, there does not seem to be an API) Cross-Platform Sync One System, All Apps : Posts auto-sync to LinkedIn, X, Facebook, Threads, YouTube, and Instagram. Key Features - No-Code Setup : Built entirely in n8n ‚Äîno coding skills needed. - Free Tools : Uses Serper (2.5k free searches), and platform APIs. - Video Ready : Downloads filmed videos from Google Drive and posts as Instagram reels, and Youtube shorts (This filming bit is manual  but the agent will send you the script to your phone, and if it is easy enough you can record it then - currently looking at hooking in my Faceless video automation into this as well - but that is work in progress) - Customizable : Tweak prompts, schedules, or platforms in minutes. Why This Works Save 10+ Hours/Week : Saves a ton of time in research, and coming up with content ideas, just tweak or it will give you a lot of drafts, a single topic can produce up to 200 + pieces of content. Scale Your Brand : Post daily on 6+ platforms without lifting a finger. PS : This setup replaces tools like Hootsuite ($50+/mo) or Later ($30+/mo) with a free AI-powered machine. üî• Turn your social media into a 24/7 inbound lead-gen engine. üî• I explain the setup here -> https://youtu.be/D1iKOayS270 publishing schedule The whole system for social media review system short form video review system topic research sample content generated for linked in. Of course the tone and everything else is customisable. Topic cold email outreach X/Twitter and Threads",
          "extracted_links": [
            "https://youtu.be/D1iKOayS270"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From news headline to Instagram post in 2 minutes, all automated! (CODE INCLUDED)",
          "link": "https://old.reddit.com/r/n8n/comments/1l9pff8/i_built_an_ai_system_that_scrapes_stories_off_the/",
          "author": "dudeson55",
          "time": "2025-06-12T15:31:02+00:00",
          "body_text": "So I built an AI newsletter that isn‚Äôt written by me ‚Äî it‚Äôs completely written by an n8n workflow that I built. Each day, the system scrapes close to 100 AI news stories off the internet ‚Üí saves the stories in a data lake as markdown file ‚Üí and then runs those through this n8n workflow to generate a final newsletter that gets sent out to the subscribers. I‚Äôve been iterating on the main prompts used in this workflow over the past 5 months and have got it to the point where it is handling 95% of the process for writing each edition of the newsletter. It currently automatically handles: Scraping news stories sourced all over the internet from Twitter / Reddit / HackerNews / AI Blogs / Google News Feeds Loading all of those stories up and having an \"AI Editor\" pick the top 3-4 we want to feature in the newsletter Taking the source material and actually writing each core newsletter segment Writing all of the supplementary sections like the intro + a \"Shortlist\" section that includes other AI story links Formatting all of that output as markdown so it is easy to copy into Beehiiv and schedule with a few clicks What started as an interesting pet project AI newsletter now has several thousand subscribers and has an open rate above 20% Data Ingestion Workflow Breakdown This is the foundation of the newsletter system as I wanted complete control of where the stories are getting sourced from and need the content of each story in an easy to consume format like markdown so I can easily prompt against it. I wrote a bit more about this automation on this reddit post but will cover the key parts again here: The approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each one: This is done by calling into a scrape_url sub-workflow that I built out. This uses the Firecrawl API /scrape endpoint to scrape the contents of the news story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. Newsletter Generator Workflow Breakdown This workflow is the big one that actually loads up all scraped news content, picks the top stories, and writes the full newsletter. 1. Trigger / Inputs I use an n8n form trigger that simply let‚Äôs me pick the date I want to generate the newsletter for I can optionally pass in the previous day‚Äôs newsletter text content which gets loaded into the prompts I build to write the story so I can avoid duplicated stories on back to back days. 2. Loading Scraped News Stories from the Data Lake Once the workflow is started, the first two sections are going to load up all of the news stories that were scraped over the course of the day. I do this by: Running a simple search operation on our S3 bucket prefixed by the date like: 2025-06-10/ (gives me all stories scraped on June 10th) Filtering these results to only give me back the markdown files that end in an .md extension (needed because I am also scraping and saving the raw HTML as well) Finally read each of these files and load the text content of each file and format it nicely so I can include that text in each prompt to later generate the newsletter. 3. AI Editor Prompt With all of that text content in hand, I move on to the AI Editor section of the automation responsible for picking out the top 3-4 stories for the day relevant to the audience. This prompt is very specific to what I‚Äôm going for with this specific content, so if you want to build something similar you should expect a lot of trial and error to get this to do what you want to. It's pretty beefy. Once the top stories are selected, that selection is shared in a slack channel using a \"Human in the loop\" approach where it will wait for me to approve the selected stories or provide feedback. For example, I may disagree with the top selected story on that day and I can type out in plain english to \"Look for another story in the top spot, I don't like it for XYZ reason\". The workflow will either look for my approval or take my feedback into consideration and try selecting the top stories again before continuing on. 4. Subject Line Prompt Once the top stories are approved, the automation moves on to a very similar step for writing the subject line. It will give me its top selected option and 3-5 alternatives for me to review. Once again this get's shared to slack, and I can approve the selected subject line or tell it to use a different one in plain english. 5. Write ‚ÄúCore‚Äù Newsletter Segments Next up, I move on to the part of the automation that is responsible for writing the \"core\" content of the newsletter. There's quite a bit going on here: The action inside this section of the workflow is to split out each of the stop news stories from before and start looping over them. This allows me to write each section one by one instead of needing a prompt to one-shot the entire thing. In my testing, I found this to follow my instructions / constraints in the prompt much better. For each top story selected, I have a list of \"content identifiers\" attached to it which corresponds to a file stored in the S3 bucket. Before I start writing, I go back to our S3 bucket and download each of these markdown files so the system is only looking at and passing in the relevant context when it comes time to prompt. The number of tokens used on the API calls to LLMs get very big when passing in all news stories to a prompt so this should be as focused as possible. With all of this context in hand, I then make the LLM call and run a mega-prompt that is setup to generate a single core newsletter section. The core newsletter sections follow a very structured format so this was relatively easier to prompt against (compared to picking out the top stories). If that is not the case for you, you may need to get a bit creative to vary the structure / final output. This process repeats until I have a newsletter section written out for each of the top selected stories for the day. You may have also noticed there is a branch here that goes off and will conditionally try to scrape more URLs. We do this to try and scrape more ‚Äúprimary source‚Äù materials from any news story we have loaded into context. Say Open AI releases a new model and the story we scraped was from Tech Crunch. It‚Äôs unlikely that tech crunch is going to give me all details necessary to really write something really good about the new model so I look to see if there‚Äôs a url/link included on the scraped page back to the Open AI blog or some other announcement post. In short, I just want to get as many primary sources as possible here and build up better context for the main prompt that writes the newsletter section. 6. Final Touches (Final Nodes / Sections) I have a prompt to generate an intro section for the newsletter based off all of the previously generated content I then have a prompt to generate a newsletter section called \"The Shortlist\" which creates a list of other AI stories that were interesting but didn't quite make the cut for top selected stories Lastly, I take the output from all previous node, format it as markdown, and then post it into an internal slack channel so I can copy this final output and paste it into the Beehiiv editor and schedule to send for the next morning. Workflow Link + Other Resources Github workflow links: AI News Story / Data Ingestion Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json Firecrawl Scrape Url Sub-Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json AI Newsletter Generator Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Nv5_LU0q1IY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json",
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json",
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json",
            "https://www.youtube.com/watch?v=Nv5_LU0q1IY"
          ],
          "link_count": 4
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m5qnxn/i_built_a_content_generator_ai_workflow_using_n8n/",
          "author": "Altruistic-Brother37",
          "time": "2025-07-21T18:22:06+00:00",
          "body_text": "I've created and given up on multiple website and revenue potentials. Creating a site on WordPress is easy and fun, but writing the content is boring and tiresome. Since I don‚Äôt have a team of writers or editors to help churn out daily blog posts, I built an n8n automation system that posts 10 content a day. It scrapes fresh topics, generates long form blog posts from RSS feeds, repurpose them using AI, and publishes directly to WordPress. Here‚Äôs how the system works: 1. Daily Trigger & Topic Ideation The workflow starts with a daily schedule trigger. It pulls fresh news content from multiple sources (mostly big tech news sites) and uses embeddings to group them into potential topics. For example, if multiple articles are trending around ‚ÄúAI music tools,‚Äù the workflow merges those into one blog worthy topic. 2. AI Powered Blog Generation Once the topic is selected, the workflow sends a custom GPT prompt to generate a 1000 - 1500 word blog post. It uses summary + original content merging, plus markdown formatting for readability. You can configure tone, length, call to actions, or SEO keywords depending on your niche. 3. WordPress Publishing The final content is cleaned, formatted, and then sent to WordPress via the API. It handles: Categories and tags Featured image (auto generated using Leonardo AI) Author and post status Slug and permalink structure This means new blog posts go live without you touching a thing. Costs The workflow runs entirely on free or open source tools. . Your only costs would be Open AI API calls + your WordPress server. Component Daily Usage Monthly Cost Estimate OpenAI 10 posts per day ~$6 Leonardo AI 10 images per day (15 credits each) ~$14 (9 base + 5 top-up) MongoDB Free up to 512 MB $0 WordPress Hosting and domain ~$1.25 Total ~$21/month Alternatively, you could use open / cheaper models via local API or and image generation via other models on Hugging Face Limitations I noticed that sometimes the word count is below 1,500. Usually around 700 to 800 words. Its not bad, but not ideal I've received a few feedback from people that sometimes the source RSS websites change the slug URL of their site and it causes errors Further Improvement (I'm working on) I want this flow to be able to generate images and tables within the blog Will be cool to repurpose the content for other sites like Twritter and Linkedin I think the current setup is too linear, passing information from one AI agent to another. Perhaps, a flow where there are 3 agents in a loop. The task master who creates the task, sections and reading journey The content writer who writes the content, drafts the tables, generates the images The Quality Checker who validates the SEO score and overall consistency. Provides feedback and passes the article if it meets certain quality thresholds Currently, it only works for one category per workflow. Right now, i hard coded AI, but you can change it to whatever you want. I notice that most blogs use 3 to 5 content pillars. Will add more topics so readers dont get bored GitHub repo with the full n8n workflow code: https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress Hopefully its useful and inspire you to light up that WP blog u abandoned.",
          "extracted_links": [
            "https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Social Media Machine That Writes, Reviews, & Posts Content 24/7 ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1i5mp/i_made_a_crawlee_server_built_specifically_for/",
          "author": "conor_is_my_name",
          "time": "2025-06-02T13:33:25+00:00",
          "body_text": "Hello Everyone! Today I'm sharing my latest n8n tool - a very performant dockerized version of the crawlee web scraping package. https://github.com/conor-is-my-name/crawlee-server Who is this for: Want to scrape every page on a website customize the fields & objects that you scrape you already have a database setup - default is postgres Scaled scraping - can run multiple containers for parallelism Who this is not for: you don't have a database - the scraper is too fast to return results to google sheets or n8n I've used this to scrape millions of web pages, and this setup is the baseline that I use for my competitor analysis and content generation work.  This template is all you need to get good at web scraping.  If you can learn how to modify the selectors in the code of this package, you can scrape 99% of websites. Simply run this docker container & update the IP address and Port number in the workflow - example n8n http node is already included. http://100.XX.XX.XX:####/start-crawl?url=https://paulgraham.com&maxResults=10 Parameters to pass from n8n: url & max results (don't pass max results if you want full site scraped) The baseline code that I'm sharing is configured as a generic web scraper most suitable for blogs and news articles.  You can modify what you want returned in the results.js file. sitehomepage, article_url, title, bodyText, datePublished, \narticlecategories, tags, keywords, author, featuredImage, comments I have also included an example for scraping a e-commerce site that runs on Woo Commerce in the n8n-nodes folder.  You can use that as a template to adjust to just about any site by changing the selectors used in the routes.js file. If you don't know how to do this, I highly recommend using Roo Code in VS Code.  It's as simple as copying the HTML from the page and asking Roo Code to pick the specific selectors you want. It will make the adjustments in the routes.js file for you.  But note that you will have to make sure your database also has all of the matching fields you want scraped. Example SQL is also included for initial database setup.  I recommend using this in conjunction with my n8n-autoscaling build which already comes with postgres installed. Instructions: Clone the repository Update passwords in the .env file to match your setup docker compose up -d update the IP address and port number in the n8n workflow to match the running containers Optional: The docker compose file has a Deploy section that comes commented out by default.  If you want to run multiple instances of this container you can make your adjustments here. You can modify scraper concurrency in the .env file.  I'd advise you to stay in the 3-5 range unless you know the site doesn't have rate limiting. As always, be sure to check out my other n8n specific GitHub repositories: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM I do expert n8n consulting, send me a message if you need help on a project.",
          "extracted_links": [
            "https://github.com/conor-is-my-name/crawlee-server",
            "https://github.com/conor-is-my-name/n8n-autoscaling",
            "https://github.com/conor-is-my-name/google-maps-scraper",
            "https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer",
            "https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM"
          ],
          "link_count": 5
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lrlbdi/i_automated_my_content_creation_from_client/",
          "author": "piothur",
          "time": "2025-07-04T15:19:15+00:00",
          "body_text": "Hey everyone! üëã\n\nI've been working on a FREE project that solves a common challenge many of us face with n8n: tracking long-running and asynchronous tasks. I'm excited to share the n8n Task Manager - a complete orchestration solution built entirely with n8n workflows!\n\nüéØ What Problem Does It Solve?\n\nIf you've ever needed to:\n- Track ML model training jobs that take hours\n- Monitor video rendering or time consuming processing tasks\n- Manage API calls to services that work asynchronously (Kling, ElevenLabs, etc.)\n- Keep tabs on data pipeline executions\n- Handle webhook callbacks from external services\n\nThen this Task Manager is for you!\n\nüöÄ Key Features:\n\n- 100% n8n workflows - No external code needed\n- Automatic polling - Checks task status every 2 minutes\n- Real-time monitoring - React frontend with live updates\n- Database backed - Uses Supabase (free tier works!)\n- Slack alerts - Get notified when tasks fail\n- API endpoints - Create, update, and query tasks via webhooks\n- Batch processing - Handles multiple tasks efficiently\n\nüì¶ What You Get:\n\n1. 4 Core n8n Workflows:\n¬† ¬†- Task Creation (POST webhook)\n¬† ¬†- Task Monitor (Scheduled polling)\n¬† ¬†- Status Query (GET endpoint)\n¬† ¬†- Task Update (Callback handler)\n\n2. React Monitoring Dashboard:\n¬† ¬†- Real-time task status\n¬† ¬†- Media preview (images, videos, audio)\n¬† ¬†- Running time tracking\n\n3. 5 Demo Workflows - Complete AI creative automation:\n¬† ¬†- OpenAI image generation\n¬† ¬†- Kling video animation\n¬† ¬†- ElevenLabs text-to-speech\n¬† ¬†- FAL Tavus lipsync\n¬† ¬†- Full orchestration example\n\nüõ†Ô∏è How to Get Started:\n\n1. Clone the repo: https://github.com/lvalics/Task_Manager_N8N\n2. Set up Supabase (5 minutes, free account)\n3. Import n8n workflows (drag & drop JSON files)\n4. Configure credentials (Supabase connection)\n5. Start tracking tasks!\n\nüí° Real-World Use Cases:\n\n- AI Content Pipeline: Generate image ‚Üí animate ‚Üí add voice ‚Üí create lipsync\n- Data Processing: Track ETL jobs, report generation, batch processing\n- Media Processing: Monitor video encoding, image optimization, audio transcription\n- API Orchestration: Manage multi-step API workflows with different services\n\nüì∫ See It In Action:\n\nI've created a full tutorial video showing the system in action: [\\[YouTube Link\\]](\nhttps://www.youtube.com/watch?v=PckWZW2fhwQ\n)\n\nü§ù Contributing:\n\nThis is open source! I'd love to see:\n- New task type implementations\n- Additional monitoring features\n- Integration examples\n- Bug reports and improvements\n\nGitHub: https://github.com/lvalics/Task_Manager_N8N\n\nüôè Feedback Welcome!\n\nI built this to solve my own problems with async task management, but I'm sure many of you have similar challenges. What features would you like to see? How are you currently handling long-running tasks in n8n?\n\nDrop a comment here or open an issue on GitHub. Let's make n8n task management better together!",
          "extracted_links": [
            "https://github.com/lvalics/Task_Manager_N8N",
            "https://www.youtube.com/watch?v=PckWZW2fhwQ"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Automated Shorts and Reels Creation with N8N ‚Äì Full Workflow & Video Guide!",
          "link": "https://old.reddit.com/r/n8n/comments/1l0k5dh/opensource_task_manager_for_n8n_track_longrunning/",
          "author": "Silly_Chapter6498",
          "time": "2025-06-01T08:38:51+00:00",
          "body_text": "I wanted to share a workflow I've been refining. I was tired of manually finding content for a niche site I'm running, so I built a bot with N8N to do it for me. It automatically fetches news articles on a specific topic and posts them to my Ghost blog. The end result is a site that stays fresh with relevant content on autopilot. Figured some of you might find this useful for your own projects. Here's the stack: Data Source: LumenFeed API (Full disclosure, this is my project. The free tier gives 10k requests/month which is plenty for this). Automation: N8N (self-hosted) De-duplication: Redis (to make sure I don't post the same article twice) CMS: Ghost (but works with WordPress or any CMS with an API) The Step-by-Step Workflow: Here‚Äôs the basic logic, node by node. (1) Setup the API Key: First, grab a free API key from LumenFeed. In N8N, create a new \"Header Auth\" credential. Name: X-API-Key Value: [Your_LumenFeed_API_Key] (2) HTTP Request Node (Get the News): This node calls the API. URL: https://client.postgoo.com/api/v1/articles Authentication: Use the Header Auth credential you just made. Query Parameters: This is where you define what you want. For example, to get 10 articles with \"crypto\" in the title: q:¬†crypto query_by:¬†title language:¬†en per_page:¬†10 (3) Code Node (Clean up the Data): The API returns articles in a¬†data¬†array. This simple JS snippet pulls that array out for easier handling. return $node[\"HTTP Request\"].json[\"data\"]; (4) Redis \"Get\" Node (Check for Duplicates): Before we do anything else, we check if we've seen this article's URL before. Operation: Get Key: {{ $json.source_link }} (5) IF Node (Is it a New Article?): This node checks the output of the Redis node. If the¬†value¬†is empty, it's a new article and we continue. If not, we stop. Condition: {{ $node[\"Redis\"].json.value }}¬†->¬†Is Empty (6) Publishing to Ghost/WordPress: If the article is new, we send it to our CMS. In your Ghost/WordPress node, you map the fields: Title: {{ $json.title }} Content: {{ $json.content_excerpt }} Featured Image: {{ $json.image_url }} (7) Redis \"Set\" Node (Save the New Article): This is the final step for each¬†new¬†article. We add its URL to Redis so it won't get processed again. Operation: Set Key: {{ $json.source_link }} Value: true That's the core of it! You just set the Schedule Trigger to run every few hours and you're good to go. Happy to answer any questions about the setup in the comments! For those who prefer video or a more detailed write-up with all the screenshots: Full Video Guide: https://youtu.be/BNkxt48plFg Detailed Blog Post: https://medium.com/@contact_34329/how-to-build-an-automated-news-aggregator-with-n8n-and-lumenfeed-api-step-by-step-guide-c63b61d4c0ce",
          "extracted_links": [
            "https://youtu.be/BNkxt48plFg"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Keyword Research and Clustering Machine That Automates SEO Content Strategy ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1kahpiu/i_built_an_aipowered_keyword_research_and/",
          "author": "hashpanak",
          "time": "2025-04-29T06:55:11+00:00",
          "body_text": "I automated keyword research and clustering for SEO and content planning. Why? I was tired of spending hours manually digging through keywords and organizing them into clusters, only to end up with a chaotic mess. Now, I‚Äôve got a system that does it all‚Äîresearches keywords, builds clusters, and prioritizes opportunities‚Äîusing n8n and AI. It‚Äôs a game-changer for anyone looking to streamline their SEO strategy without breaking the bank. Use it for: Creating topic clusters and using it as a lead magnet for acquiring new clients. Using generated pillars and pages for social media content ideas from what people are actually searching for. Automating keyword research and clustering Saving time while uncovering high-impact topics I explain the setup here -> https://youtu.be/twFg_L9_07E I automated everything‚Äîfrom keyword discovery to clustering‚Äîusing n8n. Here‚Äôs the blueprint. This system pulls keyword data, organizes it into pillar pages and subpages, and even stores it neatly in Google Sheets, all while running on free or low-cost tools. How It Works This workflow turns a single seed keyword into a full SEO strategy in 4 stages: Automated Keyword Research (with human in the loop) Seed Keyword Start: Drop in a keyword (e.g., ‚Äún8n‚Äù or ‚ÄúSEO‚Äù), and the system uses the Google Ads API (or Data for SEO) to fetch related keywords, search volumes, competition, and trends. Replace it with other APIs if you so wish like \"DataForSEO\" Iterative Discovery: It keeps digging, researching related keywords to build a massive, relevant list‚Äîthink hundreds of terms in minutes. AI-Powered Clustering Smart Grouping: An AI model (like GPT-4.1 or Gemini) analyzes the keywords based on user intent (informational, transactional, navigational), search trends, and competition. Pillar & Subpage Structure: It creates thematic clusters‚Äîpillar pages for broad topics and subpages for specifics‚Äîthen ranks them by growth potential and ranking ease. Google Sheets Integration Central Hub: All keywords and clusters land in Google Sheets, ready for you to review or tweak. Manual Control: Spot a dud keyword? Delete it. Need to adjust a cluster? It‚Äôs all right there. n8n Automation No-Code Workflow: Built in n8n, this system runs on autopilot with scheduled triggers or manual kicks. Scalable Processing: Looping mechanisms handle huge keyword sets without breaking a sweat. Key Features No-Code Setup: Built entirely in n8n‚Äîno coding skills needed. Free/Low-Cost Tools: Uses Google Ads API (free tier) or Data for SEO ($1 signup credit and cheap alternative to Ahrefs or SEMRush). Scalable: Tackles thousands of keywords with batch processing. Customizable: Tweak AI prompts or schedules to match your needs. Why This Works Save 10+ Hours/Week: No more manual keyword grinding‚Äîautomation does the heavy lifting. Smarter SEO: AI prioritizes clusters with real data, targeting topics that rank and convert. Scale Effortlessly: Perfect for big content strategies without the overwhelm. Example: Feed it ‚Äún8n,‚Äù and it pulls keywords like ‚Äún8n automation,‚Äù ‚Äún8n workflows,‚Äù and ‚Äún8n integrations.‚Äù The AI then builds clusters like ‚Äún8n Automation Tutorials‚Äù (pillar) with subpages like ‚ÄúConnecting n8n to Salesforce‚Äù‚Äîcomplete with primary/secondary keywords and ranking priorities. PS: This replaces tools like Ahrefs ($99+/mo) or SEMrush ($129+/mo) with a free AI-powered system. üî• Turn your SEO strategy into a 24/7 content-planning engine. üî• I explain the setup here -> https://youtu.be/twFg_L9_07E https://preview.redd.it/bxjl5j430qxe1.png?width=1704&format=png&auto=webp&s=53e7bc67c1f76f46a3fbdc76b2acdef1bbc4ac51 https://preview.redd.it/xr50mgn40qxe1.png?width=2344&format=png&auto=webp&s=04768c89f7bc0947eac2fe1a1976c30634273a38 https://preview.redd.it/sobyfvw50qxe1.png?width=2716&format=png&auto=webp&s=bb7994483791ff33a21ec3e44aa766e2d6560388 https://preview.redd.it/r8zawm170qxe1.png?width=2886&format=png&auto=webp&s=a3f5803b70120d2c34de4d8547b264a2e84ca1ae https://reddit.com/link/1kahpiu/video/zz4xrkxg0qxe1/player",
          "extracted_links": [
            "https://youtu.be/twFg_L9_07E"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1l0gm98/try_this_podcast_generation_workflow_i_built/",
          "author": "teezworkspace",
          "time": "2025-06-01T04:49:03+00:00",
          "body_text": "Hey everyone, I built out this workflow in n8n to help me intake the highest quality AI content in the most digestible format for myself; audio. In short, the RSS Feed scrapes three (could be more if you want) of the most reputable sources in the AI space, goes through a Code node for scoring (looks for the highest quality content: whitepapers, research papers, etc) and calls AutoContentAPI (NOT free, but a NotebookLM alternative nonetheless) via HTTP Request and generates podcasts on the respective material and sends it to me via Telegram and Gmail, and updates my Google Drive as well. Provided below is a screenshot and the downloadable JSON in case anyone would like to try it. Feel free to DM me if you have any questions. I'm also not too familiar with how to share files on Reddit so the option I settled on was placing the JSON in this code block, hopefully that works? Again, feel free to DM me if you'd like to try it and I should be able to share it to you directly as downloadable JSON for you to import into n8n. {\n  \"name\": \"AI Podcast Generation (AutoContentAPI)\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"triggerTimes\": {\n          \"item\": [\n            {}\n          ]\n        }\n      },\n      \"name\": \"Schedule: Weekly Learning Run\",\n      \"type\": \"n8n-nodes-base.cron\",\n      \"typeVersion\": 1,\n      \"position\": [\n        -1820,\n        -200\n      ],\n      \"id\": \"7a78b92e-d75b-4cab-bf0c-6a9fd41c5683\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.url }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.rssFeedRead\",\n      \"typeVersion\": 1.1,\n      \"position\": [\n        -920,\n        -180\n      ],\n      \"id\": \"2a012472-2e03-451c-80d7-202d159c3959\",\n      \"name\": \"RSS Read\",\n      \"onError\": \"continueRegularOutput\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"return [\\n  { json: { url: \\\"https://huggingface.co/blog/feed\\\" } },\\n  { json: { url: \\\"https://machinelearningmastery.com/blog/feed/\\\" } },\\n  { json: { url: \\\"https://blog.tensorflow.org/feeds/posts/default\\\" } }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -1620,\n        -200\n      ],\n      \"id\": \"758b3629-43b5-4330-a1a0-2c1aabdfdf1e\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"const keywords = [\\n  \\\"whitepaper\\\", \\\"research\\\", \\\"study\\\", \\\"publication\\\", \\\"paper\\\", \\\"preprint\\\", \\\"abstract\\\",\\n  \\\"benchmark\\\", \\\"evaluation\\\", \\\"methodology\\\", \\\"experiment\\\", \\\"analysis\\\", \\\"dataset\\\",\\n  \\\"LLM\\\", \\\"GPT\\\", \\\"transformer\\\", \\\"language model\\\", \\\"fine-tuning\\\", \\\"pretraining\\\"\\n];\\n\\nconst now = new Date();\\nconst weekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000);\\nconst monthStart = new Date(now.getFullYear(), now.getMonth(), 1);\\nconst seenLinks = new Set();\\n\\n// Domains not supported by AutoContentAPI on free tier\\nconst blockedDomains = [\\n  \\\"arxiv.org\\\",\\n  \\\"ieeexplore.ieee.org\\\",\\n  \\\"springer.com\\\",\\n  \\\"sciencedirect.com\\\",\\n  \\\"dl.acm.org\\\"\\n];\\n\\n// Score and parse\\nlet scored = items.map(item => {\\n  const title = (item.json.title || \\\"\\\").toLowerCase();\\n  const description = (item.json.description || item.json.contentSnippet || item.json.content || \\\"\\\").toLowerCase();\\n  const link = item.json.link || item.json.url || \\\"\\\";\\n  const pubDateStr = item.json.pubDate || item.json.date || item.json.isoDate || \\\"\\\";\\n  const pubDate = pubDateStr && !isNaN(Date.parse(pubDateStr)) ? new Date(pubDateStr) : null;\\n\\n  let score = 0;\\n  keywords.forEach(keyword => {\\n    if (title.includes(keyword)) score += 2;\\n    if (description.includes(keyword)) score += 1;\\n  });\\n\\n  return {\\n    json: {\\n      title: item.json.title,\\n      link,\\n      pubDate: pubDateStr,\\n      pubDateObject: pubDate,\\n      content: item.json.content || item.json.contentSnippet || \\\"\\\",\\n      score\\n    }\\n  };\\n});\\n\\n// Filter: only allow whitelisted, non-duplicate, recent items\\nlet filtered = scored.filter(item =>\\n  item.json.score >= 2 &&\\n  item.json.pubDateObject instanceof Date &&\\n  !isNaN(item.json.pubDateObject) &&\\n  item.json.link &&\\n  !seenLinks.has(item.json.link) &&\\n  !blockedDomains.some(domain => item.json.link.includes(domain)) &&\\n  seenLinks.add(item.json.link)\\n);\\n\\n// Prioritize items from the last 7 days\\nlet pastWeek = filtered.filter(item => item.json.pubDateObject >= weekAgo);\\n\\n// If none found, fall back to items from this calendar month\\nif (pastWeek.length === 0) {\\n  pastWeek = filtered.filter(item =>\\n    item.json.pubDateObject >= monthStart && item.json.pubDateObject <= now\\n  );\\n}\\n\\n// Sort by score descending\\npastWeek.sort((a, b) => b.json.score - a.json.score);\\n\\n// Return top 3\\nreturn pastWeek.slice(0, 3);\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -700,\n        -180\n      ],\n      \"id\": \"3ffafffd-f20a-4197-a09c-b08dca6099a6\",\n      \"name\": \"Whitepaper Filter\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"0e2fb51a-8995-4b8d-bb41-ea78cf5c1904\",\n              \"name\": \"url\",\n              \"value\": \"={{ $json.url }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -1120,\n        -180\n      ],\n      \"id\": \"d0115844-b5fb-489c-83fe-4d2fbd11b7b9\",\n      \"name\": \"Edit Fields\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"ca3acbb3-9375-4335-b8b2-a951e72dff76\",\n              \"name\": \"request_id\",\n              \"value\": \"={{ $json.request_id }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        120,\n        -160\n      ],\n      \"id\": \"06ef9efc-88b3-470a-b7dd-b615e7700d09\",\n      \"name\": \"Extract Request ID\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"=https://api.autocontentapi.com/content/status/{{$json[\\\"request_id\\\"]}}\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer 5b62e1aa-54d0-4319-81e8-93320d9a58ef\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        320,\n        -160\n      ],\n      \"id\": \"50db4ed9-e412-48bd-b41f-1a764be41c74\",\n      \"name\": \"GET Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://api.autocontentapi.com/Content/Create\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer YOUR<API>KEY\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"raw\",\n        \"rawContentType\": \"application/json\",\n        \"body\": \"={{ \\n  JSON.stringify({\\n    resources: [\\n      {\\n        content: $json[\\\"link\\\"],\\n        type: \\\"website\\\"\\n      }\\n    ],\\n    text: \\\"Create a podcast summary of this article in a conversational, engaging tone.\\\",\\n    outputType: \\\"audio\\\"\\n  })\\n}}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -140,\n        -160\n      ],\n      \"id\": \"8ae2fffa-03ab-4053-9db0-388de34b5287\",\n      \"name\": \"Generate Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"9f869aa6-11f0-4664-8d16-d06a6ec52c9f\",\n              \"leftValue\": \"={{ $json.status }}\",\n              \"rightValue\": 100,\n              \"operator\": {\n                \"type\": \"number\",\n                \"operation\": \"equals\"\n              }\n            }\n          ],\n          \"combinator\": \"or\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        520,\n        -160\n      ],\n      \"id\": \"2785e08c-f859-4fa2-b752-9f114e6617bc\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"teezworkspace@gmail.com\",\n        \"subject\": \"={{ $json.audio_title }}\",\n        \"message\": \"={{ $json.audio_title }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"attachmentsUi\": {\n            \"attachmentsBinary\": [\n              {\n                \"property\": \"audio\"\n              }\n            ]\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        1080,\n        80\n      ],\n      \"id\": \"f07b9a91-aa2d-43a9-9095-41497180454f\",\n      \"name\": \"Send Audio to Email\",\n      \"webhookId\": \"0ff65219-e34a-4ad4-b600-f7238569c92d\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"\",\n          \"name\": \"Terry's Gmail\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"inputDataFieldName\": \"audio\",\n        \"name\": \"={{ $json.audio_title }}\",\n        \"driveId\": {\n          \"__rl\": true,\n          \"value\": \"My Drive\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\"\n        },\n        \"folderId\": {\n          \"__rl\": true,\n          \"value\": \"1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Weekly AI Research Audio\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"typeVersion\": 3,\n      \"position\": [\n        1080,\n        -120\n      ],\n      \"id\": \"5d9eec4c-f596-48f0-a81e-5f1bc37a082b\",\n      \"name\": \"Upload Audio Folder\",\n      \"credentials\": {\n        \"googleDriveOAuth2Api\": {\n          \"id\": \"\",\n          \"name\": \"Terry Google Drive\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"sendAudio\",\n        \"chatId\": \"6018770135\",\n        \"binaryData\": true,\n        \"binaryPropertyName\": \"audio\",\n        \"additionalFields\": {\n          \"caption\": \"={{ $json.audio_title }}\",\n          \"title\": \"={{ $json.audio_title }}\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.telegram\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        1080,\n        -340\n      ],\n      \"id\": \"6f21e927-a79b-48f3-a5ff-8dd9d460916f\",\n      \"name\": \"Send Audio to Telegram\",\n      \"webhookId\": \"97f48ead-3e73-4928-a555-455722196acc\",\n      \"credentials\": {\n        \"telegramApi\": {\n          \"id\": \"\",\n          \"name\": \"AutoContentAPI Bot \"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"batchSize\": 15,\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -1380,\n        -200\n      ],\n      \"id\": \"fb9a4a7c-2aba-4a17-89e4-6e856bd23d0a\",\n      \"name\": \"URL Loop\"\n    },\n    {\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -480,\n        -180\n      ],\n      \"id\": \"9ce3486f-0bd6-45fa-bdcc-392c72bfff97\",\n      \"name\": \"Podcast Gen Loop\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.audio_url }}\",\n        \"options\": {\n          \"response\": {\n            \"response\": {\n              \"responseFormat\": \"file\",\n              \"outputPropertyName\": \"audio\"\n            }\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        760,\n        -180\n      ],\n      \"id\": \"0afdf799-a612-4a07-a2e5-c65b262ef12e\",\n      \"name\": \"Download Audio\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Schedule: Weekly Learning Run\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"RSS Read\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Whitepaper Filter\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Whitepaper Filter\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Edit Fields\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"RSS Read\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Request ID\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"GET Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Extract Request ID\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"GET Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Download Audio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upload Audio Folder\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"URL Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Edit Fields\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Podcast Gen Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Generate Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Download Audio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Audio to Telegram\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Upload Audio Folder\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Send Audio to Email\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": false,\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"57ddc431-4059-4b0e-92dc-325c7296ac9a\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"f9bd58af1591f515777c160d7518c3e5cf0ad788d4a4c3831380e58e9febdfa6\"\n  },\n  \"id\": \"Ece8XCZeyPq6R0Uv\",\n  \"tags\": []\n} https://preview.redd.it/7nryas97v84f1.png?width=2694&format=png&auto=webp&s=1bb4c966619c7cfd520ab6bc47c1dbbd439871e6",
          "extracted_links": [
            "https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Help Please",
          "link": "https://old.reddit.com/r/n8n/comments/1ksnvdm/power_up_your_n8n_workflows_with_diverse_content/",
          "author": "PostGoo",
          "time": "2025-05-22T11:02:28+00:00",
          "body_text": "I run an Instagram account that publishes short form videos each week that cover the top AI news stories. I used to monitor twitter to write these scripts by hand, but it ended up becoming a huge bottleneck and limited the number of videos that could go out each week. In order to solve this, I decided to automate this entire process by building a system that scrapes the top AI news stories off the internet each day (from Twitter / Reddit / Hackernews / other sources), saves it in our data lake, loads up that text content to pick out the top stories and write video scripts for each. This has saved a ton of manual work having to monitor news sources all day and let‚Äôs me plug the script into ElevenLabs / HeyGen to produce the audio + avatar portion of each video. One of the recent videos we made this way got over 1.8 million views on Instagram and I‚Äôm confident there will be more hits in the future. It‚Äôs pretty random on what will go viral or not, so my plan is to take enough ‚Äúshots on goal‚Äù and continue tuning this prompt to increase my changes of making each video go viral. Here‚Äôs the workflow breakdown 1. Data Ingestion and AI News Scraping The first part of this system is actually in a separate workflow I have setup and running in the background. I actually made another reddit post that covers this in detail so I‚Äôd suggestion you check that out for the full breakdown + how to set it up. I‚Äôll still touch the highlights on how it works here: The main approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. Right now, there are around ~13 news sources that I have setup to pull stories from every single day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each  story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. 2. Loading up and formatting the scraped news stories Once the data lake / news storage has plenty of scraped stories saved for the day, we are able to get into the main part of this automation. This kicks off off with a scheduled trigger that runs at 7pm each day and will: Search S3 bucket for all markdown files and tweets that were scraped for the day by using a prefix filter Download and extract text content from each markdown file Bundle everything into clean text blocks wrapped in XML tags for better LLM processing - This allows us to include important metadata with each story like the source it came from, links found on the page, and include engagement stats (for tweets). 3. Picking out the top stories Once everything is loaded and transformed into text, the automation moves on to executing a prompt that is responsible for picking out the top 3-5 stories suitable for an audience of AI enthusiasts and builder‚Äôs. The prompt is pretty big here and highly customized for my use case so you will need to make changes for this if you are going forward with implementing the automation itself. At a high level, this prompt will: Setup the main objective Provides a ‚Äúcuration framework‚Äù to follow over the list of news stories that we are passing int Outlines a process to follow while evaluating the stories Details the structured output format we are expecting in order to avoid getting bad data back ```jsx\n<objective>\nAnalyze the provided daily digest of AI news and select the top 3-5 stories most suitable for short-form video content. Your primary goal is to maximize audience engagement (likes, comments, shares, saves). The date for today's curation is {{ new Date(new Date($('schedule_trigger').item.json.timestamp).getTime() + (12 * 60 * 60 * 1000)).format(\"yyyy-MM-dd\", \"America/Chicago\") }} . Use this to prioritize the most recent and relevant news. You MUST avoid selecting stories that are more than 1 day in the past for this date.\n</objective> <curation_framework>\nTo identify winning stories, apply the following virality principles. A story must have a strong \"hook\" and fit into one of these categories: Impactful: A major breakthrough, industry-shifting event, or a significant new model release (e.g., \"OpenAI releases GPT-5,\" \"Google achieves AGI\"). Practical: A new tool, technique, or application that the audience can use now (e.g., \"This new AI removes backgrounds from video for free\"). Provocative: A story that sparks debate, covers industry drama, or explores an ethical controversy (e.g., \"AI art wins state fair, artists outraged\"). Astonishing: A \"wow-factor\" demonstration that is highly visual and easily understood (e.g., \"Watch this robot solve a Rubik's Cube in 0.5 seconds\"). Hard Filters (Ignore stories that are): * Ad-driven: Primarily promoting a paid course, webinar, or subscription service.\n* Purely Political: Lacks a strong, central AI or tech component.\n* Substanceless: Merely amusing without a deeper point or technological significance.\n</curation_framework> <hook_angle_framework>\nFor each selected story, create 2-3 compelling hook angles that could open a TikTok or Instagram Reel. Each hook should be designed to stop the scroll and immediately capture attention. Use these proven hook types: Hook Types: - Question Hook: Start with an intriguing question that makes viewers want to know the answer\n- Shock/Surprise Hook: Lead with the most surprising or counterintuitive element\n- Problem/Solution Hook: Present a common problem, then reveal the AI solution\n- Before/After Hook: Show the transformation or comparison\n- Breaking News Hook: Emphasize urgency and newsworthiness\n- Challenge/Test Hook: Position as something to try or challenge viewers\n- Conspiracy/Secret Hook: Frame as insider knowledge or hidden information\n- Personal Impact Hook: Connect directly to viewer's life or work Hook Guidelines: - Keep hooks under 10 words when possible\n- Use active voice and strong verbs\n- Include emotional triggers (curiosity, fear, excitement, surprise)\n- Avoid technical jargon - make it accessible\n- Consider adding numbers or specific claims for credibility\n</hook_angle_framework> <process>\n1. Ingest: Review the entire raw text content provided below.\n2. Deduplicate: Identify stories covering the same core event. Group these together, treating them as a single story. All associated links will be consolidated in the final output.\n3. Select & Rank: Apply the Curation Framework to select the 3-5 best stories. Rank them from most to least viral potential.\n4. Generate Hooks: For each selected story, create 2-3 compelling hook angles using the Hook Angle Framework .\n</process> <output_format>\nYour final output must be a single, valid JSON object and nothing else. Do not include any text, explanations, or markdown formatting like ` json before or after the JSON object. The JSON object must have a single root key, stories , which contains an array of story objects. Each story object must contain the following keys:\n- title (string): A catchy, viral-optimized title for the story.\n- summary (string): A concise, 1-2 sentence summary explaining the story's hook and why it's compelling for a social media audience.\n- hook_angles (array of objects): 2-3 hook angles for opening the video. Each hook object contains:\n  - hook (string): The actual hook text/opening line\n  - type (string): The type of hook being used (from the Hook Angle Framework)\n  - rationale (string): Brief explanation of why this hook works for this story\n- sources (array of strings): A list of all consolidated source URLs for the story. These MUST be extracted from the provided context. You may NOT include URLs here that were not found in the provided source context. The url you include in your output MUST be the exact verbatim url that was included in the source material. The value you output MUST be like a copy/paste operation. You MUST extract this url exactly as it appears in the source context, character for character. Treat this as a literal copy-paste operation into the designated output field. Accuracy here is paramount; the extracted value must be identical to the source value for downstream referencing to work. You are strictly forbidden from creating, guessing, modifying, shortening, or completing URLs. If a URL is incomplete or looks incorrect in the source, copy it exactly as it is. Users will click this URL; therefore, it must precisely match the source to potentially function as intended. You cannot make a mistake here.\n``` After I get the top 3-5 stories picked out from this prompt, I share those results in slack so I have an easy to follow trail of stories for each news day. 4. Loop to generate each script For each of the selected top stories, I then continue to the final part of this workflow which is responsible for actually writing the TikTok / IG Reel video scripts. Instead of trying to 1-shot this and generate them all at once, I am iterating over each selected story and writing them one by one. Each of the selected stories will go through a process like this: Start by additional sources from the story URLs to get more context and primary source material Feeds the full story context into a viral script writing prompt Generates multiple different hook options for me to later pick from Creates two different 50-60 second scripts optimized for talking-head style videos (so I can pick out when one is most compelling) Uses examples of previously successful scripts to maintain consistent style and format Shares each completed script in Slack for me to review before passing off to the video editor. Script Writing Prompt ```jsx\nYou are a viral short-form video scriptwriter for David Roberts, host of \"The Recap.\" Follow the workflow below each run to produce two 50-60-second scripts (140-160 words). Before you write your final output, I want you to closely review each of the provided REFERENCE_SCRIPTS and think deeploy about what makes them great. Each script that you output must be considered a great script. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 1 ‚Äì Ideate ‚Ä¢ Generate five distinct hook sentences (‚â§ 12 words each) drawn from the STORY_CONTEXT. STEP 2 ‚Äì Reflect & Choose ‚Ä¢ Compare hooks for stopping power, clarity, curiosity. ‚Ä¢ Select the two strongest hooks (label TOP HOOK 1 and TOP HOOK 2). ‚Ä¢ Do not reveal the reflection‚Äîonly output the winners. STEP 3 ‚Äì Write Two Scripts For each top hook, craft one flowing script ‚âà 55 seconds (140-160 words). Structure (no internal labels): ‚Äì Open with the chosen hook. ‚Äì One-sentence explainer. ‚Äì 5-7 rapid wow-facts / numbers / analogies. ‚Äì 2-3 sentences on why it matters or possible risk. ‚Äì Final line = a single CTA ‚Ä¢ Ask viewers to comment with a forward-looking question or ‚Ä¢ Invite them to follow The Recap for more AI updates. Style: confident insider, plain English, light attitude; active voice, present tense; mostly ‚â§ 12-word sentences; explain unavoidable jargon in ‚â§ 3 words. OPTIONAL POWER-UPS (use when natural) ‚Ä¢ Authority bump ‚Äì Cite a notable person or org early for credibility. ‚Ä¢ Hook spice ‚Äì Pair an eye-opening number with a bold consequence. ‚Ä¢ Then-vs-Now snapshot ‚Äì Contrast past vs present to dramatize change. ‚Ä¢ Stat escalation ‚Äì List comparable figures in rising or falling order. ‚Ä¢ Real-world fallout ‚Äì Include 1-3 niche impact stats to ground the story. ‚Ä¢ Zoom-out line ‚Äì Add one sentence framing the story as a systemic shift. ‚Ä¢ CTA variety ‚Äì If using a comment CTA, pose a provocative question tied to stakes. ‚Ä¢ Rhythm check ‚Äì Sprinkle a few 3-5-word sentences for punch. OUTPUT FORMAT  (return exactly this‚Äîno extra commentary, no hashtags) HOOK OPTIONS ‚Ä¢ Hook 1 ‚Ä¢ Hook 2 ‚Ä¢ Hook 3 ‚Ä¢ Hook 4 ‚Ä¢ Hook 5 TOP HOOK 1 SCRIPT [finished 140-160-word script] TOP HOOK 2 SCRIPT [finished 140-160-word script] REFERENCE_SCRIPTS <Pass in example scripts that you want to follow and the news content loaded from before>\n``` 5. Extending this workflow to automate further So right now my process for creating the final video is semi-automated with human in the loop step that involves us copying the output of this automation into other tools like HeyGen to generate the talking avatar using the final script and then handing that over to my video editor to add in the b-roll footage that appears on the top part of each short form video. My plan is to automate this further over time by adding another human-in-the-loop step at the end to pick out the script we want to go forward with ‚Üí Using another prompt that will be responsible for coming up with good b-roll ideas at certain timestamps in the script ‚Üí use a videogen model to generate that b-roll ‚Üí finally stitching it all together with json2video. Depending on your workflow and other constraints, It is really up to you how far you want to automate each of these steps. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=7WsmUlbyjMM The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json",
            "https://www.youtube.com/watch?v=7WsmUlbyjMM"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Need help: workflow automations needed within the realm of content marketing",
          "link": "https://old.reddit.com/r/n8n/comments/1lzz02o/how_much_is_my_workflow_worth/",
          "author": "Nocare420",
          "time": "2025-07-14T21:22:57+00:00",
          "body_text": "With the new ChatGPT-4o image generator, I've been considering whether I could move away from actively using LinkedIn. The constant need to create organic content and post daily to gain followers or connect with people feels like a significant time investment. Providing free content and value every day is demanding, and I'm looking for a more efficient approach.\nMy idea is to use Gemini 2.5 for writing the text and ChatGPT-4o for generating images. I have a template in mind, and I'd like a workflow where the AI generates a LinkedIn post based on this template, sends it to me for approval, and only posts if I give it the go-ahead. However, my current setup doesn't integrate with ChatGPT-4o, and I'm unsure how to create this workflow. (And i would like to add a node  where i send them like every week or every month a linkedin post or linkedin profile to inspire from it and post similar like him)\nAny help or suggestions on how to integrate these tools and automate this process would be greatly appreciated)\nP.S. I found a helpful YouTube channel https://youtu.be/o_oSYl6gSO8?si=rj7hAaFtimFn4iIk that inspired this idea, and the template I'm referring to can be found there.\"\nIf i did find  a way how this is created i will post the template next post :)",
          "extracted_links": [
            "https://youtu.be/o_oSYl6gSO8"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "üöÄ New Workflow added of Short Video Creation üé•",
          "link": "https://old.reddit.com/r/n8n/comments/1j7rod6/n8n_workflow_automate_ugc_video_creation_from_any/",
          "author": "ProEditor69",
          "time": "2025-03-10T05:38:26+00:00",
          "body_text": "after going through over 1,000 n8n templates , I‚Äôm convinced most of them are either half-baked, broken, or riddled with bugs. So many look good on GitHub or in the n8n library, but once you import them‚Ä¶ missing nodes, failed integrations, or completely outdated logic. It‚Äôs like people just export random experiments and call them ‚Äúproduction-ready.‚Äù Anyway, after way too many months of testing, debugging, and cursing at my screen, I found a few actually solid workflows worth using especially if you're doing lead gen, enrichment, or routing work. Firstly here are OG n8n Templates these are Awesome and works enescingoz/awesome-n8n-templates ‚Äì A well-curated repo with real-world workflows, including one for lead scoring and CRM integration. https://github.com/wassupjay/n8n-free-templates Bonus : How I Vet GitHub Repos Automatically After getting burned a few times, I stopped trusting stars or even READMEs. So now, I run a quick reputation check before using any GitHub template. Specifically: Is it active? Are people talking about it on Reddit or X ? Are there better forks or open issues? Does it look like a side project someone forgot about? To automate this, I ask BhindiAI to scan Reddit and X for the most relevant discussions about a repo, summarize the general vibe (positive, meh, sketchy), and extract key takeaways. It then logs that info repo link + commentary + score straight into my Google Sheet. That way I‚Äôm not wasting time setting up stuff the internet already knows is broken. Reddit and X Discussions are helpful to know what is a bs better than github stars. The worst part? Some templates had security vulnerabilities that weren't obvious until I dug deeper. One workflow was storing API keys in plain text comments, and another was making unencrypted calls to external services. I now have a strict checklist I run through before implementing anything in production. I've also started forking promising templates immediately and maintaining my own versions. Too many times I've seen maintainers abandon projects or push breaking changes without warning. Now I control my own destiny with these workflows. The community aspect is real though - I've found some of my best templates through Reddit and random Twitter threads where people share their actual working setups. The official n8n community forums are hit or miss, but when you find someone who's actually using these workflows at scale, their insights are gold.",
          "extracted_links": [
            "https://github.com/wassupjay/n8n-free-templates"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Power Up Your n8n Workflows with Diverse Content: LumenFeed.com Offers 10k Free API Requests/Month!",
          "link": "https://old.reddit.com/r/n8n/comments/1ltsocy/i_have_reviewed_over_1000_n8n_templates_heres_how/",
          "author": "Valuable_Simple3860",
          "time": "2025-07-07T12:19:18+00:00",
          "body_text": "I run an AI Tools directory and a large growth opportunity for us is writing \"roundup\" articles that highlight the best AI tools in a particular software category such as \"Best AI Image Editors\", \"Best AI Sales Tools\" , and many more. The problem is, this content takes a ton of time to research, write, and build out rankings for each tool. With the major improvements that have rolled out to \"Deep Research\" tools, we decide to see if we could leverage the output of these deep research reports, feed it into our own prompts, and scale out the process of creating great content. This automation is what we were able to build and I'm happy to report this process has allowed us to significantly scale up our content creation process. I'm pretty amazed at how much depth Chat GPT's deep research tool is able to provide and I think in many cases it is able to provide better insights than if we were doing this by hand. Here's the automation breakdown 1. Trigger / Inputs We use a simple form trigger for this automation that takes in some details specific to the structure of our website such as the slug for the category and the target audience for this article We also accept an optional parameter here for any tools that we know we want to be included in the \"Best Tools by Category\" roundup in order to give us a bit more control of the final output. 2. Building Necessary Context The next few nodes in here our very specific to our own website. We get started by making an HTTP request into our own API to get details about the category we are going to write about. After that, we make another HTTP request to our own API to get a list of all tools that are inside the category we are going to write about. Depending on the type of article you are going to write, this initial \"build context\" step is likely going to look very different for you so be sure you start with the end vision in mind of what you are trying to create and then work backwards from there to build the context you need. 3. Building The Deep Research Prompt The next step here is to take all of that context we just gathered and bundle it together into a prompt that we will feed into a deep research tool like Chat GPT deep research or Gemini Deep research. In our case, we build a prompt that focuses on telling the deep research tool to focus on picking out the top 6-8 tools in the provided category with a strong focus on social signals and real reviews left by users of the tool. Once again, this is going to be something that you will need to customize for your own use case. You should expect to spend a lot of time iterating on this prompt to get it in a good spot. I think we probably went through 30+ iterations of this prompt until we were happy with the deep research output. 4. Human-In-The-Loop Step Once the prompt is generated, we pause the automation with a sendAndWait slack message that will post the full prompt in slack. We then copy that prompt into Chat GPT deep research and let that run until we get the deep research report back After the deep research process finishes, we click the \"Add Result\" button in slack which will let us paste in the output of deep research into a form and continue with the automation. The Chat GPT deep research output is just so good so we opted to go this way. 5. Generating Article Content With the deep research report content now inside our n8n execution, we are then able to proceed with generation our article. We opted to take an approach where we build this section by section (like lego block) instead of trying to one-shot the entire article. We found this to be more effective and result in better output. Here's how we split it up: Intro paragraph and article meta description get written first in two separate prompts After that, we write another paragraph that details the ranking criteria that was used to pick out the top tools and how they were evaluated We then move on to writing the main content for each of the selected tools. We have a single prompt and a format we follow for each tool, so we loop over the selected tools and generate this content for each. Lastly, we take the output for each of these sections and pass that into a final prompt that is responsible for writing a conclusion. 6. Pushing To Our CMS In the final section of this automation, we take the output of each of the nodes from before and bundle it up into into a format that can easily get inserted into our CMS. We use Sanity CMS for hosting all of our content which doesn't have an out of the box API that we can just push content too, so we opted to create our own API endpoint to handle this for us. If you are hosting your content on something like Webflow / WordPress / another CMS, you should be able to replace the node here with an HTTP request that publishes your content to replicate this behavior. Workflow Link + Other Resources Github workflow link for this automation: https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json YouTube video that walks through this workflow step-by-step: https://youtu.be/uDrkgEuEOBA Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json",
            "https://youtu.be/uDrkgEuEOBA"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I analysed 2,000+ n8n workflows and this is what I learned",
          "link": "https://old.reddit.com/r/n8n/comments/1l1f6n8/i_analysed_2000_n8n_workflows_and_this_is_what_i/",
          "author": "Majestic-Fix-3857",
          "time": "2025-06-02T11:05:59+00:00",
          "body_text": "So I downloaded 2,050 public n8n workflows and then used claude opus 4 to help me vibe code my way through a detailed analysis. I used cursor as my code running tool, ran the claude scripts over the 2,000 JSON files, created a report, and then summarised into the below actionable doc Here is a video walkthrough of me visually going over the insights + also exploring the recommendations on the n8n canvas: https://youtu.be/BvBa_npD4Og Or if you just wanna read, here is the claude actionable report (hope you legends enjoy and find useful) -- n8n Workflow Best Practices Guide Learnings from Analyzing 2,000+ Production Workflows This guide is based on insights gathered from analyzing 2,050 production n8n workflows containing 29,363 nodes. It highlights common patterns, critical issues, and best practices for building robust, secure, and maintainable automation workflows. üìä Executive Summary Our analysis revealed critical gaps in error handling (97% of workflows lack it), security vulnerabilities (320 public webhooks without auth), and efficiency issues (7% contain unused nodes). This guide provides actionable recommendations to address these issues and build better workflows. Key Statistics: 2,050 workflows analyzed 29,363 total nodes 14.3 average nodes per workflow 97% lack error handling 472 security vulnerabilities found 34.7% are AI/ML workflows üö® Critical Issue #1: Error Handling (97% Gap) The Problem Only 62 out of 2,050 workflows (3%) have any error handling mechanism. This means when things fail, workflows silently break without notification or recovery. Best Practices 1. Always Use Error Triggers // Add an Error Trigger node at the beginning of every workflow\n// Connect it to a notification system (Email, Slack, etc.)\nError Trigger ‚Üí Format Error Message ‚Üí Send Notification 2. Implement Node-Level Error Handling For critical nodes (HTTP requests, database operations, API calls): Enable \"Continue On Fail\" for non-critical operations Add retry logic with exponential backoff Set appropriate timeout values 3. Error Handling Template Start ‚Üí Error Trigger ‚Üí Error Handler\n  ‚Üì\nMain Workflow Logic\n  ‚Üì\nCritical Operation (with retry: 3, delay: 1000ms)\n  ‚Üì\nSuccess Path / Error Path 4. Monitoring Pattern Log all errors to a centralized system Include workflow name, node name, error message, and timestamp Set up alerts for repeated failures üîí Critical Issue #2: Security Vulnerabilities The Problems 320 public webhooks without authentication 152 unsecure HTTP calls 3 workflows with hardcoded secrets Security Best Practices 1. Webhook Security // Always enable authentication on webhooks\nWebhook Settings:\n  - Authentication: Header Auth / Basic Auth\n  - Use HTTPS only\n  - Implement IP whitelisting where possible\n  - Add rate limiting 2. Secure API Communications Never use HTTP - always use HTTPS Store credentials in n8n's credential system, never hardcode Use OAuth2 when available (694 workflows do this correctly) Implement API key rotation policies 3. Authentication Methods (from most to least secure) OAuth2 - Use for major integrations API Keys - Store securely, rotate regularly Basic Auth - Only when necessary, always over HTTPS No Auth - Never for public endpoints 4. Secret Management Checklist [ ] No hardcoded API keys in Code/Function nodes [ ] All credentials stored in n8n credential manager [ ] Regular credential audit and rotation [ ] Environment-specific credentials (dev/staging/prod) üéØ Critical Issue #3: Workflow Efficiency The Problems 144 workflows with unused nodes (264 total unused nodes) 133 workflows with API calls inside loops 175 workflows with redundant transformations Efficiency Best Practices 1. Clean Architecture Input ‚Üí Validate ‚Üí Transform ‚Üí Process ‚Üí Output\n         ‚Üì (fail)\n      Error Handler 2. Avoid Common Anti-Patterns ‚ùå Bad: API in Loop Loop ‚Üí HTTP Request ‚Üí Process Each ‚úÖ Good: Batch Processing Collect Items ‚Üí Single HTTP Request (batch) ‚Üí Process Results 3. Node Optimization Remove unused nodes (7% of workflows have them) Combine multiple Set nodes into one Use Code node for complex transformations instead of chaining Set nodes Cache API responses when possible 4. Performance Guidelines Average workflow should complete in < 10 seconds Use Split In Batches for large datasets Implement parallel processing where possible (only 4.8% currently do) Add progress logging for long-running workflows ü§ñ AI/ML Workflow Best Practices (34.7% of workflows) Common Patterns Observed 346 agent-based workflows 267 multi-model workflows 201 with memory systems 0 with vector databases (RAG pattern opportunity) AI Workflow Best Practices 1. Prompt Engineering // Structure prompts with clear sections\nconst prompt = `\nSystem: ${systemContext}\nContext: ${relevantData}\nTask: ${specificTask}\nFormat: ${outputFormat}\n`; 2. Cost Optimization Use GPT-3.5 for simple tasks, GPT-4 for complex reasoning Implement caching for repeated queries Batch similar requests Monitor token usage 3. Agent Workflow Pattern Trigger ‚Üí Context Builder ‚Üí Agent (with tools) ‚Üí Output Parser ‚Üí Response\n                                ‚Üì\n                          Memory System 4. Error Handling for AI Handle rate limits gracefully Implement fallback models Validate AI outputs Log prompts and responses for debugging üìã Workflow Organization Best Practices The Problem 74.7% of workflows categorized as \"general\" Poor documentation and organization Organization Best Practices 1. Naming Conventions [Category]_[Function]_[Version]\nExamples:\n- Sales_LeadScoring_v2\n- HR_OnboardingAutomation_v1\n- DataSync_Salesforce_Daily_v3 2. Tagging Strategy Essential tags to use: Environment: prod, staging, dev Category: sales, hr, finance, it-ops Frequency: real-time, hourly, daily, weekly Status: active, testing, deprecated 3. Documentation with Sticky Notes The #1 most used node (7,024 times) - use it well: Document complex logic Explain business rules Note dependencies Include contact information 4. Workflow Structure üìù Sticky Note: Workflow Overview\n    ‚Üì\n‚öôÔ∏è Configuration & Setup\n    ‚Üì\nüîÑ Main Process Logic\n    ‚Üì\n‚úÖ Success Handling | ‚ùå Error Handling\n    ‚Üì\nüìä Logging & Monitoring üîÑ Common Node Sequences (Best Patterns) Based on the most frequent node connections: 1. Data Transformation Pattern Set ‚Üí HTTP Request (379 occurrences) Best for: Preparing data before API calls 2. Chained API Pattern HTTP Request ‚Üí HTTP Request (350 occurrences) Best for: Sequential API operations (auth ‚Üí action) 3. Conditional Processing If ‚Üí Set (267 occurrences)\nSwitch ‚Üí Set (245 occurrences) Best for: Data routing based on conditions 4. Data Aggregation Set ‚Üí Merge (229 occurrences) Best for: Combining multiple data sources üõ°Ô∏è Security Checklist for Every Workflow Before Deployment [ ] No hardcoded credentials [ ] All webhooks have authentication [ ] All external calls use HTTPS [ ] Sensitive data is encrypted [ ] Access controls are implemented [ ] Error messages don't expose sensitive info Regular Audits [ ] Review webhook authentication monthly [ ] Rotate API keys quarterly [ ] Check for unused credentials [ ] Verify HTTPS usage [ ] Review access logs üìà Optimization Opportunities 1. For Complex Workflows (17.5%) Break into sub-workflows Use Execute Workflow node Implement proper error boundaries Add performance monitoring 2. For Slow Workflows Identify bottlenecks (usually API calls) Implement caching Use batch operations Add parallel processing 3. For Maintenance Remove unused nodes (found in 7% of workflows) Consolidate redundant operations Update deprecated node versions Document business logic üéØ Top 10 Actionable Recommendations Implement Error Handling - Add Error Trigger to all production workflows Secure Webhooks - Enable authentication on all 320 public webhooks Use HTTPS - Migrate 152 HTTP calls to HTTPS Clean Workflows - Remove 264 unused nodes Batch API Calls - Refactor 133 workflows with APIs in loops Add Monitoring - Implement centralized logging Document Workflows - Use Sticky Notes effectively Categorize Properly - Move from 74.7% \"general\" to specific categories Implement Retry Logic - Add to all critical operations Regular Audits - Monthly security and performance reviews üöÄ Quick Start Templates 1. Error-Handled Webhook Workflow Webhook (with auth) ‚Üí Validate Input ‚Üí Process ‚Üí Success Response\n         ‚Üì                    ‚Üì (error)\n   Error Trigger ‚Üê Error Formatter ‚Üê Error Response 2. Secure API Integration Schedule Trigger ‚Üí Get Credentials ‚Üí HTTPS Request (with retry) ‚Üí Process Data\n                                            ‚Üì (fail)\n                                     Error Handler ‚Üí Notification 3. AI Workflow with Error Handling Trigger ‚Üí Build Context ‚Üí AI Agent ‚Üí Validate Output ‚Üí Use Result\n    ‚Üì            ‚Üì             ‚Üì            ‚Üì\nError Handler ‚Üê Rate Limit ‚Üê Timeout ‚Üê Invalid Output üìö Resources and Next Steps Create Workflow Templates - Build standard templates with error handling Security Audit Tool - Scan all workflows for vulnerabilities Performance Dashboard - Monitor execution times and failures Training Program - Educate team on best practices Governance Policy - Establish workflow development standards üéâ Success Metrics After implementing these practices, aim for: < 5% workflows without error handling 0 public webhooks without authentication 0 HTTP calls (all HTTPS) < 3% workflows with unused nodes > 90% properly categorized workflows < 10s average execution time This guide is based on real-world analysis of 2,050 production workflows. Implement these practices to build more reliable, secure, and maintainable n8n automations.",
          "extracted_links": [
            "https://youtu.be/BvBa_npD4Og"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m0o40i/i_built_an_ai_workflow_that_analyzes_longform/",
          "author": "dudeson55",
          "time": "2025-07-15T17:31:23+00:00",
          "body_text": "Clipping youtube videos and twitch VODs into tiktoks/reels/shorts is a super common practice for content creators and major brands where they take their long form video content like podcasts and video streams then turn it into many different video clips that later get posted and shared on TikTok + IG Reels. Since I don‚Äôt have an entire team of editors to work on creating these video clips for me, I decided to build an automation that does the heavy lifting for me. This is what I was able to come up with: Here's how the automation works 1. Workflow Trigger / Inputs The workflow starts with a simple form trigger that accepts a YouTube video URL. In your system, you could automate this further by setting up an RSS feed for your youtube channel or podcast. 2. Initial Video Processing Request Once the URL is submitted, the workflow makes an HTTP POST request to the Vizard API to start processing the video: The request includes the YouTube video URL and processing parameters like max_clip_number - IMO the defaults actually work pretty well here so I‚Äôd leave most alone to let their system analyze for the most viral moments in the video By default, it will also add in captions. If you want to customize the style of the video / keep captions consistent with your brand you can also specify a template id in your request The API returns a project ID and initial status code that we'll use to poll for results after the video analysis completes 3. Polling Loop for Processing Status Since video processing can take significant time (especially for longer videos), the workflow uses a simple polling system which will loop over: A simple Wait node pauses execution for 10 seconds between status checks (analyzing long form videos will take a fair bit of time so this will check many times) An HTTP GET request checks the processing status using the project ID from the initial request If the status code is 1000 (still processing), the workflow loops back to wait and check again When the status reaches 2000 (completed), the workflow continues to the next section 4. Filtering and Processing Results Once the video analysis/processing is complete, I get all the video clip results back in the response and I‚Äôm able to continue with further processing. The response I get back from this include a virality score of 1/10 based on the clips potential. Clips are filtered based on virality score - I only keep clips with a score of 9 or higher In my testing, this reduces a lot of the noise / worthless clips from the output After those videos get filtered, I then share a summary message in slack with the title, virality score, and download link for each clip You can also take this further and auto-generate a social media caption + pickout ideal hashtags to use based on the content of the video and where you plan to post it. If you want to auto-post, you would use another tool like blotato to publish to each social media platform you need I personally really like using slack to review all the clips because it centralizes all clips into a single spot for me to review before posting. Costs I‚Äôm currently just on the ‚ÄúCreator‚Äù plan for Vizard which costs $29 / month for 600 upload minutes (of source YouTube material). This fits my needs for the content that I create but if you are running a larger scale clipping operation or working with multiple brands that cost is going to scale up linearly for the minutes of source material you use. Workflow Link + Other Resources YouTube video that walks through this workflow node-by-node: https://www.youtube.com/watch?v=Yb-mZmvHh-I The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json",
            "https://www.youtube.com/watch?v=Yb-mZmvHh-I"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lunl3h/stealing_someone_elses_project_and_claiming_it_as/",
          "author": "hncvj",
          "time": "2025-07-08T12:43:31+00:00",
          "body_text": "Saw a post here today about an \"I built an AI that runs a viral ASMR channel hands-free.\" Bold claim. Except‚Ä¶ it's not their work. The entire idea, workflow, and execution is taken straight from Nate Herk' YouTube channel and his free S kool community, without a single credit or change. Original here: https://youtu.be/RvuPRX-b7Lc?si=a2iHUF3pqPWDdhAN What's worse? They didn't even bother innovating or acknowledging. Just slapped a \"Workflow - Code Not Included\" banner and called it theirs. This trend of stealing real creators' work for fake clout needs to stop. It's not \"inspired,\" it's not \"repurposed\", it's a direct plagiarism, plain and simple. It's my humble request, respect the creators. Don't build your karma on someone else's grind. What do you guys think?",
          "extracted_links": [
            "https://youtu.be/RvuPRX-b7Lc"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "My n8n Workflows Site ( update )- Find Quality Automations Easily!",
          "link": "https://old.reddit.com/r/n8n/comments/1kgxgo4/i_made_a_docker_compose_for_n8n_queue_mode_with/",
          "author": "conor_is_my_name",
          "time": "2025-05-07T13:32:53+00:00",
          "body_text": "UPDATE: Check the 2nd branch if you want to use cloudflared. TLDR:  Put simply, this is the pro level install that you have been looking for, even if you aren't a power user (yet). I can't be the only one who has struggled with queue mode (the documentation is terrible), but I finally nailed it.  Please take this code and use it so no one else has to suffer through what I did building it.  This version is better in every way than the regular install.  Just leave me a GitHub star. https://github.com/conor-is-my-name/n8n-autoscaling First off, who is this for? Anyone who wants to run n8n either locally or on a single server of any size (ram should be 2gb+, but I'd recommend  8gb+ if using with the other containers linked at the bottom, the scrapers are ram hogs) You want simple setup Desire higher parallel throughput (it won't make single jobs faster) Why is queue mode great? No execution limit bottlenecks scales up and scales down based on load if a worker fails, the jobs gets reassigned Whats inside: A Docker-based autoscaling solution for n8n workflow automation platform. Dynamically scales worker containers based on Redis queue length. No need to deal with k8s or any other container scaling provider, a simple script runs it all and is easily configurable. Includes Puppeteer and Chrome built-in for pro level scraping directly from the n8n code node.  It makes it so much easier to do advanced scraping compared to using the community nodes.  Just paste your puppeteer script in a regular code node and you are rolling.  Use this in conjunction with my Headful Chrome Docker that is linked at the bottom for great results on tricky websites. Everything installs and configures automatically, only prerequisite is having docker installed.  Works on all platforms, but the puppeteer install requires some dependency tweaks if you are using a ARM cpu.  (an AI will know what to do for the dependency changes) Install instructions: Windows or Mac: Install the docker desktop app. Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d Linux: Follow the instructions for the Docker Convenience Script . Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d That's it.  (But remember to change the passwords) Default settings are for 50 simultaneous workflow executions.  See GitHub page for instructions on changing the worker count and concurrency. A tip for those who are in the process of leveling up their n8n game: move away from google sheets and airtable - they are slow and unstable embrace Postgres - with AI its really easy, just ask it what to do and how to set up the tables Tested on a Netcup 8 core 16gb Root VPS - RS 2000 G11.  Easily ran hundreds of simultaneous executions.  Lower end hardware should work fine too, but you might want to limit the number of worker instances to something that makes sense for your own hardware. If this post inspires you to get a server, use this link .  Or don't, just run this locally for free. I do n8n consulting, send me a message if you need help on a project. check out my other n8n specific GitHub repos: Extremely fast google maps scraper - this one is a masterpiece web scraper server using crawlee for deep scraping - I've scraped millions of pages using this Headful Chrome Docker with Puppeteer for precise web scraping and persistent sessions - for tricky websites and those requiring logins",
          "extracted_links": [
            "https://github.com/conor-is-my-name/n8n-autoscaling"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that analyzes long-form YouTube videos and generates short form clips optimized for TikTok / IG Reels / YT Shorts",
          "link": "https://old.reddit.com/r/n8n/comments/1lkbx2b/i_built_this_ai_automation_that_generates_viral/",
          "author": "dudeson55",
          "time": "2025-06-25T17:28:56+00:00",
          "body_text": "There‚Äôs been a huge trend of Bigfoot / Yeti vlog videos exploding across IG and TikTok all created with Veo 3 and I wanted to see if I could replicate and automate the full process of: Taking a simple idea as input Generate an entire story around that simple idea Turn that into a Veo 3 prompt Finally generate those videos inside n8n using FAL. Had a lot of fun building this and am pretty happy with final output . Here‚Äôs the workflow breakdown. 1. Input / Trigger The input and trigger for this workflow is a simple Form Trigger that has a single text field. What goes into here is a simple idea for for what bigfoot will be doing that will later get turned into a fully fleshed-out story. It doesn‚Äôt need any crazy detail, but just needs something the story can be anchored around. Here‚Äôs an example of one of the ones I used earlier to give you a better idea: jsx\nBigfoot discovers a world war 2 plane crash while on a hike through the deep forest that he hasn't explored yet 2. The Narrative Writer Prompt The next main node of this automation is what I call the ‚Äúnarrative writer‚Äù. Its function is very similar to a storyboard artist where it will accept the basic ideas as input and will generate an outline for each clip that needs to be generated for the story. Since Veo 3 has a hard limit of 8 seconds per video generation, that was a constraint I had to define here. So after this runs, I get an outline that splits up the story into 8 distinct clips that are each 8 seconds long. I also added in extra constraints here like what I want bigfoots personality to be like on camera to help guide the dialog and I also specified that I want the first out of the 8 clips to always be an introduction to the video. Here‚Äôs the full prompt I am using: ```jsx Role: You are a creative director specializing in short-form, character-driven video content. Goal: Generate a storyboard outline for a short vlog based on a user-provided concept. The output must strictly adhere to the Persona, Creative Mandate, and Output Specification defined below. [Persona: Bigfoot the Vlogger] Identity: A gentle giant named \"Sam,\" who is an endlessly curious and optimistic explorer. His vibe is that of a friendly, slightly clumsy, outdoorsy influencer discovering the human world for the first time. Voice & Tone: Consistently jolly, heartwarming, and filled with childlike wonder. He is easily impressed and finds joy in small details. His language is simple, and he might gently misuse human slang. PG-rated, but occasional mild exasperation like \"geez\" or \"oh, nuts\" is authentic. His dialog and lines MUST be based around the \"Outdoor Boys\" YouTube channel and he must speak like the main character from that Channel. Avoid super generic language. Physicality: An 8-foot male with shaggy, cedar-brown fur ( #6d6048 ) and faint moss specks. His silhouette is soft and \"huggable\" due to fluffy fur on his cheeks and shoulders. Features soft, medium-amber eyes, rounded cheeks, a broad nose, and short, blunt lower canines visible when he smiles. He holds a simple selfie stick at all times. [Creative Mandate] Visual Style: All scenes are shot 16:9 from a selfie-stick perspective held by Bigfoot. The style must feel like authentic, slightly shaky \"found footage.\" The camera is always on him, not his POV. Narrative Goal: The primary objective is to create audience affection. Each scene must showcase Bigfoot's charm through his gentle humor, endearing discoveries, or moments of vulnerability. The 8-scene arc must have a satisfying and heartwarming payoff. [Output Specification] Structure: Provide a storyboard with exactly 8 sequential scenes, formatted as shown below. Introduction Rule: Scene 1 must be a direct-to-camera introduction. In it, Bigfoot should enthusiastically greet his viewers (e.g., \"Hey everybody!\" or \"Hi friends!\") and briefly state the goal or adventure for the vlog, based on the user's concept. Duration: Each scene represents 8 seconds of footage. Content per Scene: For each scene, provide a single, descriptive paragraph. This paragraph must seamlessly weave together the visual action, Bigfoot's expressions, and his spoken dialogue. Each scene you create should be part of a cohesive story. Example Formats: SCENE 1 (0:00-0:08): Sam the Bigfoot grins warmly into his selfie stick, the background slightly out of focus. He waves a large, furry hand and says, \"Hey friends, Sam here! Today, we're going to try and build a brand new shelter out of... well, whatever we can find! Wish me luck!\" SCENE 3 (0:32-0:40): Sam holds up a rusty, bent bicycle wheel, peering at it curiously with his head tilted. He tries to spin it, a look of concentration on his face. \"Hmm. This is a weird-looking rock. Very... holey. Not good for a wall, I don't think.\" Task Using the rules above, create the storyboard outline for the following concept: {{ $json['Bigfoot Video Idea'] }} ``` And here‚Äôs what the output looks like after this prompt runs: ```jsx\nTitle: Finding History in the Deep Woods - WWII Plane Discovery!\nConcept: Bigfoot discovers a world war 2 plane crash while on a hike through the deep forest Scene 1 | 0:00-0:08\nSam the Bigfoot grins warmly into his selfie stick, towering evergreens framing his fuzzy shoulders. He adjusts his grip on the stick with both hands and beams at the camera. \"Hey everybody, Sam here! Today we're heading way deep into the back country for some serious exploring. You never know what kind of treasures you might stumble across out here!\" Scene 2 | 0:08-0:16\nSam trudges through dense undergrowth, his selfie stick bouncing slightly as he navigates around massive fir trees. Moss hangs like curtains around him, and his amber eyes dart curiously from side to side. \"Man, this forest just keeps getting thicker and thicker. Perfect day for it though - nice and cool, birds are singing. This is what I call the good life, friends!\" Scene 3 | 0:16-0:24\nSam suddenly stops mid-stride, his eyes widening as he stares off-camera. The selfie stick trembles slightly in his grip, showing his surprised expression clearly. \"Whoa, hold on a second here...\" He tilts his shaggy head to one side, his mouth forming a perfect 'O' of amazement. \"Guys, I think I'm seeing something pretty incredible through these trees.\" Scene 4 | 0:24-0:32\nSam approaches cautiously, pushing aside hanging branches with his free hand while keeping the camera steady. His expression shifts from wonder to respectful awe as he gets closer to his discovery. \"Oh my goodness... friends, this is... this is an old airplane. Like, really old. Look at the size of this thing!\" His voice drops to a whisper filled with reverence. Scene 5 | 0:32-0:40\nSam extends the selfie stick to show himself standing next to the moss-covered wreckage of a WWII fighter plane, its metal frame twisted but still recognizable. His expression is one of deep respect and fascination. \"This has got to be from way back in the day - World War Two maybe? The forest has just been taking care of it all these years. Nature's got its own way of honoring history, doesn't it?\" Scene 6 | 0:40-0:48\nSam crouches down carefully, his camera capturing his gentle examination of some scattered debris. He doesn't touch anything, just observes with his hands clasped respectfully. \"You know what, guys? Someone's story ended right here, and that's... that's something worth remembering. This pilot was probably somebody's son, maybe somebody's dad.\" His usual cheerfulness is tempered with genuine thoughtfulness. Scene 7 | 0:48-0:56\nSam stands and takes a step back, his expression shifting from contemplation to gentle resolve. He looks directly into the camera with his characteristic warmth, but there's a new depth in his amber eyes. \"I think the right thing to do here is let the proper folks know about this. Some family out there might still be wondering what happened to their loved one.\" Scene 8 | 0:56-1:04\nSam gives the camera one final, heartfelt look as he begins to back away from the site, leaving it undisturbed. His trademark smile returns, but it's softer now, more meaningful. \"Sometimes the best adventures aren't about what you take with you - they're about what you leave behind and who you help along the way. Thanks for exploring with me today, friends. Until next time, this is Sam, reminding you to always respect the stories the forest shares with us.\"\n``` 3. The Scene Director Prompt The next step is to take this story outline and turn it into a real prompt that can get passed into Veo 3. If we just took the output from the outline and tried to create a video, we‚Äôd get all sorts of issues where the character would not be consistent across scenes, his voice would change, the camera used would change, and things like that. So the next step of this process is to build out a highly detailed script with all technical details necessary to give us a cohesive video across all 8 clips / scenes we need to generate. The prompt here is very large so I won‚Äôt include it here (it is included inside the workflow) but I will share the desired output we are going for. For every single 8 second clip we generate, we are creating something exactly like that will cover: Scene overview Scene description Technical specs like duration, aspect ratio, camera lens Details of the main subject (Bigfoot) Camera motion Lighting Atmosphere Sound FX Audio Bigfoot dialog Really the main goal here is to be as specific as possible so we can get consistent results across each and every scene we generate. ```jsx SCENE 4  ‚ñ∏  ‚ÄúTrail to the Lake‚Äù  ‚ñ∏  0 ‚Äì 8 s Selfie-stick POV. Bigfoot strolls through dense cedar woods toward a sun-sparkled lake in the distance. No spoken dialogue in this beat‚Äîjust ambient forest sound and foot-fall crunches. Keeps reference camera-shake, color grade, and the plush, lovable design. SCENE DESCRIPTION POV selfie-stick vlog: Bigfoot walks along a pine-needle path, ferns brushing both\nsides. Sunbeams flicker through the canopy. At the 6-second mark the shimmering\nsurface of a lake appears through the trees; Bigfoot subtly tilts the stick to\nhint at the destination. TECHNICAL SPECS ‚Ä¢ Duration‚ÄÉ8 s‚ÄÉ‚Ä¢ 29.97 fps‚ÄÉ‚Ä¢ 4 K UHD‚ÄÉ‚Ä¢ 16 : 9 horizontal ‚Ä¢ Lens‚ÄÉ24 mm eq, ∆í/2.8‚ÄÉ‚Ä¢ Shutter‚ÄÉ1/60 s (subtle motion-blur) ‚Ä¢ Hand-held wobble amplitude cloned from reference clip (small ¬±2¬∞ yaw/roll). SUBJECT DETAILS  (LOCK ACROSS ALL CUTS) ‚Ä¢ 8-ft male Bigfoot, cedar-brown shaggy fur #6d6048 with faint moss specks. ‚Ä¢ Fluffier cheek & shoulder fur ‚Üí plush, huggable silhouette. ‚Ä¢ Eyes: soft medium-amber, natural catch-lights only ‚Äî no glow or excess brightness. ‚Ä¢ Face: rounded cheeks, gentle smile crease; broad flat nose; short blunt lower canines. ‚Ä¢ Hands: dark leathery palms, 4-inch black claws; right paw grips 12-inch carbon selfie stick. ‚Ä¢ Friendly, lovable, gentle vibe. CAMERA MOTION 0 ‚Äì 2 s‚ÄÉStick angled toward Bigfoot‚Äôs chest/face as he steps onto path. 2 ‚Äì 6 s‚ÄÉSmooth forward walk; slight vertical bob; ferns brush lens edges. 6 ‚Äì 8 s‚ÄÉStick tilts ~20¬∞ left, revealing glinting lake through trees; light breeze ripples fur. LIGHTING & GRADE Late-morning sun stripes across trail; teal-olive mid-tones, warm highlights,\ngentle film grain, faint right-edge lens smudge (clone reference look). ATMOSPHERE FX ‚Ä¢ Dust motes / pollen drifting in sunbeams. ‚Ä¢ Occasional leaf flutter from breeze. AUDIO BED  (NO SPOKEN VOICE) Continuous forest ambience: songbirds, light wind, distant woodpecker;\nsoft foot-crunch on pine needles; faint lake-lap audible after 6 s. END FRAME Freeze at 7.8 s with lake shimmering through trees; insert one-frame white-noise\npop to preserve the series‚Äô hard-cut rhythm.\n``` 3. Human in the loop approval The middle section of this workflow is a human in the loop process where we send the details of the script to a slack channel we have setup and wait for a human to approve or deny it before we continue with the video generation. Because generation videos this way is so expensive ($6 per 8 seconds of video), we want to review this before before potentially being left with a bad video. 4. Generate the video with FAL API The final section of this automation is where actually take the scripts generated from before, iterate over each, and call in to FAL‚Äôs Veo 3 endpoint to queue up the video generation request and wait for it to generate. I have a simple polling loop setup to check its status every 10 seconds which will loop until the video is completely rendered. After that is done, the loop will move onto the next clip/scene it needs to generate until all 8 video clips are rendered. Each clip get‚Äôs uploaded to a Google Drive I have configured so my editor can jump in and stitch them together into a full video. If you wanted to extend this even further, you could likely use the json2video API to do that stitching yourself, but that ultimately depends on how far or not you want to automate. Notes on keeping costs down Like I mentioned above, the full cost of running this is currently very expensive. Through the FAL API it costs $6 for 8 seconds of video so this probably doesn‚Äôt make sense for everyone‚Äôs use case. If you want to keep costs down, you can still use this exact same workflow and drop the 3rd section that uses the FAL API. Each of the prompts that get generated for the full script can simply be copied and pasted into Gemini or Flow to generate a video of the same quality but it will be much cheaper to do so. Workflow Link + Other Resources Github n8n workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=C65c8itWvf4 Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json",
            "https://www.youtube.com/watch?v=C65c8itWvf4"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI that's smarter than most real estate agents. Here's the n8n blueprint.",
          "link": "https://old.reddit.com/r/n8n/comments/1kml8st/i_made_a_google_maps_scraper_designed/",
          "author": "conor_is_my_name",
          "time": "2025-05-14T17:33:42+00:00",
          "body_text": "Hey everyone! Today I am sharing my custom built google maps scraper.  It's extremely fast compared to most other maps scraping services and produces more reliable results as well. I've spent thousands of dollars over the years on scraping using APIFY, phantom buster, and other services.  They were ok but I also got many formatting issues which required significant data cleanup. Finally went ahead and just coded my own.  Here's the link to the GitHub repo, just give me a star: https://github.com/conor-is-my-name/google-maps-scraper It includes example json for n8n workflows to get started in the n8n nodes folder.  Also included the Postgres code you need to get basic tables up and running in your database. These scrapers are designed to be used in conjunction with my n8n build linked below.  They will work with any n8n install, but you will need to update the IP address rather than just using the container name like in the example. https://github.com/conor-is-my-name/n8n-autoscaling If using the 2 together, make sure that you set up the external docker network as described in the instructions.  Doing so makes it much easier to get the networking working. Why use this scraper? Best in class speed and reliability You can scale up with multiple containers on multiple computers/servers, just change the IP. A word of warning: Google will rate limit you if you just blast this a million times.  Slow and steady wins the race.  I'd recommend starting at no more than 1 per minute per IP address.  There are 1440 minutes in a day x 100 results per search = 144,000 results per day. https://preview.redd.it/ozwqpz31is0f1.png?width=644&format=png&auto=webp&s=a0b472a70594a4b3fbd72ffc7e429e7ae7acbb11 Example Search: Query = Hotels in 98392 (you can put anything here) language = en limit results = 1 (any number) headless = true [\n  {\n    \"name\": \"Comfort Inn On The Bay\",\n    \"place_id\": \"0x549037bf4a7fd889:0x7091242f04ffff4f\",\n    \"coordinates\": {\n      \"latitude\": 47.543005199999996,\n      \"longitude\": -122.6300069\n    },\n    \"address\": \"1121 Bay St, Port Orchard, WA 98366\",\n    \"rating\": 4,\n    \"reviews_count\": 735,\n    \"categories\": [\n      \"Hotel\"\n    ],\n    \"website\": \"https://www.choicehotels.com/washington/port-orchard/comfort-inn-hotels/wa167\",\n    \"phone\": \"3603294051\",\n    \"link\": \"https://www.google.com/maps/place/Comfort+Inn+On+The+Bay/data=!4m10!3m9!1s0x549037bf4a7fd889:0x7091242f04ffff4f!5m2!4m1!1i2!8m2!3d47.5430052!4d-122.6300069!16s%2Fg%2F1tfz9wzs!19sChIJidh_Sr83kFQRT___BC8kkXA?authuser=0&hl=en&rclk=1\"\n  },",
          "extracted_links": [
            "https://github.com/conor-is-my-name/google-maps-scraper",
            "https://github.com/conor-is-my-name/n8n-autoscaling"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1l0nj7m/i_built_a_workflow_that_generates_longform_blog/",
          "author": "LargePay1357",
          "time": "2025-06-01T12:16:04+00:00",
          "body_text": "First, an AI agent goes back and forth with you to finalize the title and headings. Then, it calls the next workflow to write the blog post. Code for both workflows: https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json Video tutorial: https://www.youtube.com/watch?v=6AgfcKUMkVY",
          "extracted_links": [
            "https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json",
            "https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json",
            "https://www.youtube.com/watch?v=6AgfcKUMkVY"
          ],
          "link_count": 3
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Looking for Review!",
          "link": "https://old.reddit.com/r/n8n/comments/1lineqi/how_you_can_setup_and_use_n8n_as_your_backend_for/",
          "author": "dudeson55",
          "time": "2025-06-23T17:52:36+00:00",
          "body_text": "I wanted to put together a quick guide and walk through on how you can use n8n to be the backend that powers your mobile apps / web apps / internal tools. I‚Äôve been using Lovable a lot lately and thought this would be the perfect opportunity to put together this tutorial and showcase this setup working end to end. The Goal - Clone the main app functionality Cal AI I thought a fun challenge for this would be cloning the core feature of the Cal AI mobile app which is an AI calorie tracker that let‚Äôs you snap a picture of your meal and get a breakdown of all nutritional info in the meal. I suspected this all could be done with a well written prompt + an API call into Open AI‚Äôs vision API (and it turns out I was right). 1. Setting up a basic API call between lovable and n8n Before building the whole frontend, the first thing I wanted to do was make sure I could get data flowing back and forth between a lovable app and a n8n workflow. So instead of building the full app UI in lovable, I made a very simple lovable project with 3 main components: Text input that accepts a webhook url (which will be our n8n API endpoint) File uploader that let‚Äôs me upload an image file for our meal we want scanned Submit button to make the HTTP request to n8n When I click the button, I want to see the request actually work from lovable ‚Üí n8n and then view the response data that actually comes back (just like a real API call). Here‚Äôs the prompt I used: jsx\nPlease build me a simple web app that contains three components. Number one, a text input that allows me to enter a URL. Number two, a file upload component that lets me upload an image of a meal. And number three, a button that will submit an HTTP request to the URL that was provided in the text input from before. Once that response is received from the HTTP request, I want you to print out JSON of the full details of the successful response. If there's any validation errors or any errors that come up during this process, please display that in an info box above. Here‚Äôs the lovable project if you would like to see the prompts / fork for your own testing: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 2. Setting up the n8n workflow for our backend Next up we need to setup the n8n workflow that will be our ‚Äúbackend‚Äù for the app. This step is actually pretty simple to get n8n working as your backend, all you need is the following: A Webhook Trigger on your workflow Some sort of data processing in the middle (like loading results from your database or making an LLM-chain call into an LLM like GPT) A Respond To Webhook node at the very end of the workflow to return the data that was processed On your initial Webhook Trigger it is very important that you change the Respond option set to Using ‚ÄòRespond To Webhook‚Äô Node. If you don‚Äôt have this option set, the webhook is going to return data immediately instead of waiting for any of your custom logic to process such as loading data from your database or calling into a LLM with a prompt. In the middle processing nodes, I ended up using Open AI‚Äôs vision API to upload the meal image that will be passed in through the API call from lovable and ran a prompt over it to extract the nutritional information from the image itself. Once that prompt finished running, I used another LLM-chain call with an extraction prompt to get the final analysis results into a structured JSON object that will be used for the final result. I found that using the Auto-fixing output parser helped a lot here to make this process more reliable and avoided errors during my testing. Meal image analysis prompt: ```jsx\n<identity>\nYou are a world-class AI Nutrition Analyst.\n</identity> <mission>\nYour mission is to perform a detailed nutritional analysis of a meal from a single image. You will identify the food, estimate portion sizes, calculate nutritional values, and provide a holistic health assessment.\n</mission> Analysis Protocol\n1. Identify: Scrutinize the image to identify the meal and all its distinct components. Use visual cues and any visible text or branding for accurate identification.\n2. Estimate: For each component, estimate the portion size in grams or standard units (e.g., 1 cup, 1 filet). This is critical for accuracy.\n3. Calculate: Based on the identification and portion estimates, calculate the total nutritional information for the entire meal.\n4. Assess & Justify: Evaluate the meal's overall healthiness and your confidence in the analysis. Justify your assessments based on the provided rubrics. Output Instructions\nYour final output MUST be a single, valid JSON object and nothing else. Do not include json markers or any text before or after the object. Error Handling\nIf the image does not contain food or is too ambiguous to analyze, return a JSON object where confidenceScore is 0.0 , mealName is \"Unidentifiable\", and all other numeric fields are 0 . OUTPUT_SCHEMA json\n{\n  \"mealName\": \"string\",\n  \"calories\": \"integer\",\n  \"protein\": \"integer\",\n  \"carbs\": \"integer\",\n  \"fat\": \"integer\",\n  \"fiber\": \"integer\",\n  \"sugar\": \"integer\",\n  \"sodium\": \"integer\",\n  \"confidenceScore\": \"float\",\n  \"healthScore\": \"integer\",\n  \"rationale\": \"string\"\n} Field Definitions\n*   ** mealName : A concise name for the meal (e.g., \"Chicken Caesar Salad\", \"Starbucks Grande Latte with Whole Milk\"). If multiple items of food are present in the image, include that in the name like \"2 Big Macs\".\n*   ** calories : Total estimated kilocalories.\n*   ** protein : Total estimated grams of protein.\n*   ** carbs : Total estimated grams of carbohydrates.\n*   ** fat : Total estimated grams of fat.\n*   ** fiber : Total estimated grams of fiber.\n*   ** sugar : Total estimated grams of sugar (a subset of carbohydrates).\n*   ** sodium : Total estimated milligrams (mg) of sodium.\n*   ** confidenceScore : A float from 0.0 to 1.0 indicating your certainty. Base this on:\n    *   Image clarity and quality.\n    *   How easily the food and its components are identified.\n    *   Ambiguity in portion size or hidden ingredients (e.g., sauces, oils).\n*   ** healthScore : An integer from 0 (extremely unhealthy) to 10 (highly nutritious and balanced). Base this on a holistic view of:\n    *   Level of processing (whole foods vs. ultra-processed).\n    *   Macronutrient balance.\n    *   Sugar and sodium content.\n    *   Estimated micronutrient density.\n*   ** rationale **: A brief (1-2 sentence) explanation justifying the healthScore and confidenceScore . State key assumptions made (e.g., \"Assumed dressing was a standard caesar\" or \"Portion size for rice was difficult to estimate\").\n``` On the final Respond To Webhook node it is also important to node that this is the spot where we will be cleaning up the final data setting the response Body for the HTTP request / API call. For my use-case where we are wanting to send back nutritional info for the provided image, I ended up formatting my response as JSON to look like this: jsx\n{\n  \"mealName\": \"Grilled Salmon with Roasted Potatoes and Kale Salad\",\n  \"calories\": 550,\n  \"protein\": 38,\n  \"carbs\": 32,\n  \"fat\": 30,\n  \"fiber\": 7,\n  \"sugar\": 4,\n  \"sodium\": 520,\n  \"confidenceScore\": 0.9,\n  \"healthScore\": 4\n} 3. Building the final lovable UI and connecting it to n8n With the full n8n backend now in place, it is time to spin up a new Lovable project and build the full functionality we want and style it to look exactly how we would like. You should expect this to be a pretty iterative process. I was not able to get a fully working app in 1-shot and had to chat back and forth in lovable to get the functionality working as expected. Here‚Äôs some of the key points in the prompt / conversation that had a large impact on the final result: Initial create app prompt: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8pekjpfeyrs52bdf1m1dm7 Style app to more closely match Cal AI: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rbd2wfvkrxxy7pc022n0e Setting up iphone mockup container: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rs1b8e7btc03gak9q4rbc Wiring up the app to make an API call to our n8n webhook: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxajea31e2xvtwbr1kytdxbb Updating app functionality to use real API response data instead of mocked dummy data (important - you may have to do something similar): https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxapb65ree5a18q99fsvdege If I was doing this again from the start, I think it would actually be much easier to get the lovable functionality working with default styles to start with and then finish up development by styling everything you need to change at the very end. The more styles, animations, other visual elements that get added in the beginning, the more complex it is to change as you get deeper into prompting. Lovable project with all prompts used: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 4. Extending this for more complex cases + security considerations This example is a very simple case and is not a complete app by any means. If you were to extend this functionality, you would likely need to add in many more endpoints to take care of other app logic + features like saving your history of scanned meals, loading up your history of scanned meals, other analysis features that can surface trends. So this tutorial is really meant to show you a bit of what is possible between lovable + n8n. The other really important thing I need to mention here is the security aspect of a workflow like this. When following my instructions above, your webhook url will not be secure. This means that if your webhook url leaks, it is completely possible for someone to make API requests into your backend and eat up your entire quota for n8n executions and run up your Open AI bill. In order to get around this for a production use-case, you will need to implement some form of authentication to protect your webhook url from malicious actors. This can be something as simple as basic auth where web apps that consume your API need to have a username / password or you could build out a more advanced auth system to protect your endpoints. My main point here is, make sure you know what you are doing before you publically rollout a n8n workflow like this or else you could be hit with a nasty bill or users of your app could be accessing things they should not have access to. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json Lovable project links: Simple n8n API request: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 Full Cal AI cloned project: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=4c-kYOiksFg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json",
            "https://www.youtube.com/watch?v=4c-kYOiksFg"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How you can setup and use n8n as your backend for a Lovable.dev app (I cloned the mobile app Cal AI)",
          "link": "https://old.reddit.com/r/n8n/comments/1lvlb8n/i_built_a_content_repurposing_system_that_turns/",
          "author": "dudeson55",
          "time": "2025-07-09T15:12:29+00:00",
          "body_text": "I built a content repurposing system that I have been using for the past several weeks that my YouTube video as input ‚Üí scrapes the transcript ‚Üí repurposes it into a post that is optimized for engagement on the platform I am posting to (right now just Twitter and LinkedIn but can be extended to many more). My social accounts are still pretty young so I don‚Äôt have great before/after stats to share, but I‚Äôm confident that the output quality here is on-par with what other creators are making and going viral with. My goal with this is to share a basic setup that you can take an run with in your own business to be customized for your niche / industry and add additional target platforms that you want to repurpose to. You could even change the main input to a long form blog post as your starting point instead of a youtube video. Here's a full breakdown of the automation 1. Workflow Trigger / Input The workflow starts with a simple form trigger that accepts a YouTube video URL as input. This is specific to our business since we always start with creating YouTube content first and then repurpose it into other formats. Form trigger accepts YouTube video URL as required text input If your content workflow starts with blog posts or other formats, you'll need to modify this trigger accordingly The URL gets passed through to the scraping operation (If your company and or your client‚Äôs company starts with a blog post first, I‚Äôd suggested simply using a tool to scrape that web page to load of that text content) 2. Scrape YouTube Video This is where we extract the video metadata and full transcript using a YouTube Scraper on Apify. Starts by using the streamers/youtube-scraper actor from the apify store (Costs $5 per 1,000 videos you scrape) Makes an HTTP request to the /run-sync-get-dataset-items endpoint to start scraping / get results back I like using this endpoint when consuming apify actors as it returns data back in the same http request we make. No need to setup polling or extra n8n nodes to use The scraper extracts title, metadata, and most importantly the full transcript in SRT format (timestamps w/ the text that was said in the video) 3. Generate Twitter Post The Twitter repurposing path follows a structured approach using a few examples I want to replicate + a detailed prompt. Set Twitter Examples : Simple ‚ÄúSet Field‚Äù node where I curated and put in 8 high-performing tweet examples that define the style and structure I want to replicate Build Master Prompt : Another Set Field node where I build a prompt that will tell the LLM to: Analyze the source YouTube transcript material Study the Twitter examples for structure and tone Generate 3 unique viral tweet options based on the content LLM Chain Call : Pass the complete prompt to Claude Sonnet Format and Share : Clean up the output and share the best 3 tweet options to Slack for me to review ```jsx ROLE: You are a world-class social media copywriter and viral growth hacker. Your expertise is in the AI, automation, and no-code space on Twitter/X. You are a master at deconstructing viral content and applying its core principles to generate new, successful posts. OBJECTIVE: Your mission is to generate three distinct, high-potential viral tweets . This tweet will promote a specific n8n automation, with the ultimate goal of getting people to follow my profile, retweet the post, and comment a specific keyword to receive the n8n workflow template via DM. STEP 1: ANALYZE SOURCE MATERIAL First, meticulously analyze the provided YouTube video transcript below. Do not summarize it. Instead, your goal is to extract the following key elements:\n1. The Core Pain Point: What is the single most frustrating, time-consuming, or tedious manual task that this automation eliminates?\n2. The \"Magic\" Solution: What is the most impressive or \"wow\" moment of the automation? What does it enable the user to do that felt impossible or difficult before?\n3. The Quantifiable Outcome: Identify any specific metrics of success mentioned (e.g., \"saves 10 hours a week,\" \"processes 100 leads a day,\" \"automates 90% of the workflow\"). If none are mentioned, create a powerful and believable one. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES Next, study the structure, tone, and psychological hooks of the following successful tweets. These examples are your primary source for determining the structure of the tweets you will generate. <twitter_tweet_examples>\n{{ $('set_twitter_examples').item.json.twitter_examples }}\n</twitter_tweet_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE TWEETS Now you will generate the 3 unique, viral tweet options. Your primary task is to act as a structural analyst: analyze the provided examples, identify the most effective structures, and then apply those structures to the content from Step 1. Your process: 1. Identify Core Structures: Analyze the <twitter_tweet_examples> . Identify the different underlying formats. For instance, is there a \"Problem ‚Üí Solution\" structure? A \"Shocking Result ‚Üí How-to\" structure? A \"Controversial Statement ‚Üí Justification\" structure? Identify the 3 most distinct and powerful structures present.\n2. Map Content to Structures: For each of the 3 structures you identified, map the \"Pain Point,\" \"Magic Solution,\" and \"Outcome\" from Step 1 into that framework.\n3. Craft the Tweets: Generate one tweet for each of the 3 structures you've chosen. The structure of each tweet (the hook, the flow, the tone) should directly mirror the style of the example it is based on. Essential Components: While you choose the overall structure, ensure each tweet you craft contains these four key elements, integrated naturally within the chosen format:\n- A Powerful Hook: The opening line that grabs attention.\n- A Clear Value Proposition: The \"what's in it for me\" for the reader.\n- An Irresistible Offer: The free n8n workflow template.\n- A High-Engagement Call to Action (CTA): The final call to action must include elements the ask for a follow, a retweet, and a comment of the \"[KEYWORD]\". CONSTRAINTS: -   Vary light use of emojis to add personality and break up the text. Not all Tweets you write should have emojis.\n-   Keep the tone energetic, confident, and educational, mirroring the tone found in the examples.\n-   Ensure the chosen [KEYWORD] is simple, relevant, and in all caps. Now, generate the 3 distinct tweet options, clearly labeled as Tweet Option 1 , Tweet Option 2 , and Tweet Option 3 . For each option, briefly state which example structure you are applying. (e.g., \"Tweet Option 1: Applying the 'Problem ‚Üí Solution' structure from Example 2.\").\n``` 4. Generate LinkedIn Post The LinkedIn path follows a similar but platform-specific approach (better grammar and different call to action): Set LinkedIn Examples : Curated examples of high-performing LinkedIn posts with different formatting and professional tone Build LinkedIn-Specific Prompt : Modified prompt that positions the LLM as a \"B2B content strategist and LinkedIn growth expert\" rather than a viral Twitter copywriter Generate Multiple Options : Creates 3 different LinkedIn post variations optimized for professional engagement Review Process : Posts all options to Slack for me to review The key difference is tone and structure - LinkedIn posts are longer, more professional, minimize emoji usage, and focus on business value rather than viral hooks. It is important to know your audience here and have a deep understanding of the types of posts that will do well. ```jsx ROLE: You are a world-class B2B content strategist and LinkedIn growth expert. Your expertise lies in creating compelling professional content around AI, automation, and no-code solutions. You are a master of professional storytelling, turning technical case studies into insightful, engaging posts that drive meaningful connections and establish thought leadership. OBJECTIVE: Your mission is to generate three distinct, high-potential LinkedIn posts . Each post will promote a specific n8n automation, framing it as a professional case study. The ultimate goals are to:\n1.  Grow my LinkedIn professional network (followers).\n2.  Establish my profile as a go-to resource for AI and automation.\n3.  Drive awareness and interest in my YouTube channel.\n4.  Get users to comment for a lead magnet (the n8n workflow). STEP 1: ANALYZE SOURCE MATERIAL (THE BUSINESS CASE) First, meticulously analyze the provided YouTube video transcript. Do not summarize it. Instead, extract the following key business-oriented elements:\n1. The Business Pain Point: What common, frustrating, or inefficient business process does this automation solve? Frame it in terms of lost time, potential for human error, or missed opportunities.\n2. The Strategic Solution: How does the n8n automation provide a smart, strategic solution? What is the core \"insight\" or \"lever\" it uses to create value?\n3. The Quantifiable Business Impact: What is the measurable outcome? Frame it in business terms (e.g., \"reclaimed 10+ hours for strategic work,\" \"achieved 99% accuracy in data processing,\" \"reduced new client onboarding time by 50%\"). If not explicitly mentioned, create a powerful and believable metric. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES (LINKEDIN POSTS) Next, study the structure, tone, and especially the Call to Action (CTA) of the following successful LinkedIn posts. These examples are your primary source for determining the structure of the posts you will generate. Pay close attention to the length of the examples as they \"feel\" right in length. <linkedin_post_examples>\n{{ $('set_linked_in_examples').item.json.linked_in_examples }}\n</linkedin_post_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE POSTS Now you will generate 3 unique LinkedIn post options. Your primary task is to act as a content strategist: analyze the provided LinkedIn examples, identify the most effective post structures, and then apply those structures to the business case from Step 1. Your process: 1. Identify Core Structures: Analyze the <linkedin_post_examples> . Identify 3 distinct formats (e.g., \"Problem/Agitate/Solve,\" \"Personal Story ‚Üí Business Lesson,\" \"Contrarian Take ‚Üí Justification\").\n2. Map Content to Structures: For each structure, weave the \"Business Pain Point,\" \"Strategic Solution,\" and \"Business Impact\" into a compelling narrative.\n3. Craft the Posts: Generate one post for each chosen structure. The post should be highly readable, using short paragraphs and ample white space. Essential Components for each LinkedIn Post: - An Intriguing Hook: A first line that stops the scroll and speaks to a professional ambition or frustration.\n- A Relatable Story/Problem: Briefly set the scene using the \"Business Pain Point.\"\n- The Insightful Solution: Explain the \"Strategic Solution\" as the turning point.\n- A Dynamic, High-Engagement Call to Action (CTA): This is critical. Instead of a fixed format, you will craft the most effective CTA by analyzing the examples provided. Your CTA must accomplish two things:\n    1.  Clearly state how to get the free n8n workflow template by commenting with a specific [KEYWORD] .\n    2.  Naturally encourage following my profile and sharing the post. Draw inspiration for the wording and style directly from the successful CTAs in the examples. If it fits the narrative, you can subtly mention that more deep dives are on my YouTube. CONSTRAINTS: -   Use emojis sparingly and professionally (e.g., ‚úÖ, üí°, üöÄ) to enhance readability.\n-   The tone must be professional, insightful, and helpful.\n-   The [KEYWORD] should be a professional, single word in all caps (e.g., BLUEPRINT, WORKFLOW, SYSTEM). FINAL OUTPUT FORMAT: You MUST format your entire response as a single, valid JSON object. The root of the object should be a key named \"post_options\", which contains an array of three post objects. Adhere strictly to the following structure for each object:\n{\n  \"analysis\": \"<string: Explain which LinkedIn example structure was applied>\",\n  \"post_text\": \"<string: The full text of the LinkedIn post, with line breaks>\"\n}\nDo not include any text or explanations outside of the JSON object.\n``` 5. Final Output Review Both paths conclude by sharing the generated content to Slack channels for human review. This gives me 3 Twitter options and 3 LinkedIn options to choose from, each optimized for best engagement. All I have to do is copy and paste the one I like the most into my social media scheduling tool then I‚Äôm done. Extending the System The best part about this is it is very easy to extend this system for any type of repurposing you need to do. LinkedIn / Twitter is only the starting point, it can be taken much further. Instagram carousel posts - Take the transcript ‚Üí pull out a few quotes ‚Üí generate an image using either Canva an AI Image generator Newsletter sections - Take the transcript + video url ‚Üí build a prompt that will write a mini-promo section for your video to be included in your newsletter Blog post / tutorial post - Take the transcript ‚Üí write a prompt that will turn it into a text-based tutorial to be published on your blog. Each new path would follow the same pattern: curate platform-specific examples, build targeted prompts, and generate multiple options for review. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=u9gwOtjiYnI The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json",
            "https://www.youtube.com/watch?v=u9gwOtjiYnI"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "MVP Generator",
          "link": "https://old.reddit.com/r/n8n/comments/1ktun9q/you_guys_liked_my_custom_n8n_autoscaling_build/",
          "author": "conor_is_my_name",
          "time": "2025-05-23T21:08:42+00:00",
          "body_text": "I've shared a number of custom n8n builds and custom scrapers: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/crawlee-server What do you want next? Things I'm considering: a real enterprise grade chatbot using Chatwoot and AI agents Scraper deep dive using crawlee Workflow I used to update 50k product listings AI content creation for wordpress that doesn't seem like AI spam.  Actual scholarly articles. anything else? I'll probably do all 4 on the list at some point. Bonus points if you send me a real database of stuff to connect to, or a specific site you want data from.  I can demo a whole integration then.",
          "extracted_links": [
            "https://github.com/conor-is-my-name/n8n-autoscaling",
            "https://github.com/conor-is-my-name/google-maps-scraper",
            "https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer",
            "https://github.com/conor-is-my-name/crawlee-server"
          ],
          "link_count": 4
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1kkhh1m/i_built_an_aipowered_automation_that_creates/",
          "author": "hashpanak",
          "time": "2025-05-12T02:12:37+00:00",
          "body_text": "I automated the creation of long-form, faceless YouTube videos using AI and n8n. This system churns out engaging videos and uploads them to Google Drive and YouTube, all in a no-code workflow. It‚Äôs perfect for anyone wanting to run a YouTube channel on autopilot without breaking the bank. Use It For: Creating faceless YouTube channels for motivational, educational, or storytelling content Generating video ideas and scripts for niches like history, self-improvement, or book summaries or any other use case. Automating daily, hourlyvideo uploads to grow your audience I explain the setup here -> https://youtu.be/8sIJdMwifSE I automated from ideation to publishing using n8n, OpenAI, and Flux AI (Fal.ai or GPT 4o image generation), with no third-party video assembly APIs - no JSON2VIDEO or CREATOMATE. Here‚Äôs the blueprint. How It Works This workflow turns a niche idea into a polished YouTube video in 5 stages: Video Ideation AI-Powered Ideas: Input a niche (e.g., ‚Äúmotivational‚Äù or ‚Äúhistory‚Äù) and topics (e.g., ‚Äúphilosophy‚Äù), and an AI agent (GPT-4.1 mini) generates JSON-formatted video ideas with category, story, duration, and tone. Google Sheets: Ideas are stored in Google Sheets, ready for manual tweaks or approval. Script and Audio Generation Story Agent: A second AI agent crafts a script based on the selected idea, tailored to the duration. Text-to-Speech: OpenAI‚Äôs TTS (or 11 Labs) converts the script into audio, with word-by-word transcription for precise timing. Scene Cutting and Image Generation Scene Editor: An AI agent splits the transcript into 2-5 second scenes, ensuring dynamic pacing. achieves this by transcript granularity set to words for whisper model and using an ai agent to cut at the end of a word Visuals via Flux  (Fal.ai): Another agent generates cinematic image prompts for each scene, rendered by Flux  Schnell model ($0.003/megapixel) - one of the cheapest. Video Assembly FFmpeg Magic: A custom 700-line code node uses FFmpeg to combine audio, images, and captions, cutting scenes to match spoken words with zoom-pan effects. No Third-Party APIs: Unlike JSON2Video or Creatomate, this runs locally or via a hosted API you control, or with a Code node. Publishing Google Drive Upload: The final video lands in a designated Drive folder. YouTube Push: The workflow creates a YouTube upload link and streams the video, setting title, description, and privacy status. Why This Works Save 10+ Hours/Week: Automates scripting, editing, and uploading - no manual work, except approvals. Grow your audience on autopilot Engaging Content: Dynamic scene cuts and AI-generated visuals keep viewers hooked. Budget-Friendly: NO CREATOMATE OR JSON2VIDEO - no manual editing I explain the setup here -> https://youtu.be/8sIJdMwifSE https://preview.redd.it/rja5cl4my80f1.png?width=2604&format=png&auto=webp&s=912478508f8f565686611262d7af5afc3a7f91ae https://preview.redd.it/wa7x7c7ny80f1.png?width=1236&format=png&auto=webp&s=63b19e6d209a23ebd8a5ec478992b2d4eeb29c54 Example: https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4/view?usp=drive_link",
          "extracted_links": [
            "https://youtu.be/8sIJdMwifSE",
            "https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I Built an AI-Powered Automation That Creates Long-Form YouTube Videos From Scratch ‚Äî Steal My Workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1l1mk0e/i_built_an_ai_workflow_that_monitors_twitter_x/",
          "author": "dudeson55",
          "time": "2025-06-02T16:30:13+00:00",
          "body_text": "Now before I get started, I know this automation may be a bit controversial as there's a lot of spam already on Twitter, but I truly believe it is possible to build a Twitter / X reply bot that is useful to people if you get your messaging down and do a good job of filtering out irrelevant messages that don't make much sense to reply to. I currently run an AI Tools directory and we noticed that each day, there are a bunch of Tweets that get posted that ask for advice on choosing the best AI Tool for a specific task or job such as \"What is the best AI Tool for writing blog posts?\" or \"What is the best AI Tool for clipping short form videos?\" Tweets like this are perfect opportunity for us to jump in, and share a link to a category page or list of tools on our directory to help them find and explore exactly what they are looking for. The problem with this is it just would take forever to do this manually as I'd have to be in front of the screen all day watching Twitter instead of doing 'real work'. So, we decided to build an AI automation that completely automates this. At a high level, we use Mention.com to monitor and alert for AI Tool questions getting asked on twitter -> use a prompt to evaluate each of these tweets individually to see if it is a good and relevant question -> fetch a list of category pages from our own website -> write a helpful reply that mentions we have a page specifically for the type of tools they are looking for. Each reply we share here doesn't amount to a ton of impressions or traffic, but ultimately this is something we believe will compound over time as it lets us have this marketing motion turned on that wasn't feasible before. Here's a full breakdown of the automation 1. Trigger / Inputs The entry point into this whole automation starts with Mention.com , we setup a new keyword alert that monitors for phrases like \"Is there any AI Tool\" or \"How can I use AI to\", etc. This setup is really important as you need to filter out a bunch of the noise that doesn't make sense to reply to. It is also important that your alert that you have setup is going to be your target customer or persona you are trying to get in front of. After the alert is configured, we used the Mention.com <> Slack integration to post the feed of all alerts into a dedicated slack channel setup just for this. 2. Initial Filtering & Validation The next couple of nodes are responsible for further filtering out ineligible Tweets that we don't want to respond too. This includes checking if the Tweet from the alert is a Retweet or if the Tweet from the alert actually was from the account we want to with (avoid our own reply causing an infinite execution loop) 3. Evaluation Prompt + LLM Call The first LLM call we make here is a simple prompt that checks the text content of the Tweet from the alert and makes a decision if we want to proceed with creating a reply or if we should exit early out of the workflow. If you are taking this workflow and extending it for your own use-case, it will be important that you change this for your own goals. In this prompt, I found it most effective to include examples of Tweets that we did want to reply to and Tweets that we wanted to skip over. 4. Build Context for Tweet Reply This step is also going to be very specific to your own goals and how you want to modify this workflow. In our case, we are making an HTTP request to our own API in order to get back a JSON list of all category pages on our website. We then take that JSON and format it nicely into more LLM-friendly text We finally take that text and will include it in our next prompt to actually write the Tweet reply If you are going to use this workflow / automation, this step must be changed and customized for the kind of reply you are trying to create. If you are trying to share helpful resources with potential leads and customers, it would be a good idea to retrieve and build up that context at this step. 5. Write The Tweet Reply In this step we take all of the context created from before and use Claude to write a Tweet reply. For our reply, we like to keep it short + include a link to one of the category pages on the AI Tools website. Since our goal is to share these pages with people asking for AI Tool suggestions, we found it most effective to include Tweet input + good examples of a reply Tweet that we would personally write if we were doing this manually. 6. Posting The Reply + Notifying In Slack The final step here was actually using the X / Twiter node in n8n to post the reply to the original Tweet we got an alert for. All that is needed here is to pass in the initial Tweet Id we need to reply to and the output of our LLM call to claude which wrote the Tweet. After that, we have a couple of Slack nodes hooked up that leave a checkmark reaction and will share the Tweet output that claude decided to go with so we can easily monitor and make changes to the prompt if we found that the reply was not quite what we were looking for. Most of the work here comes from iterating on the prompt so its important to have a good feedback loop in place so you can see what is happening as the automation runs over more and more Tweets. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Q_b5uPndsLY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json",
            "https://www.youtube.com/watch?v=Q_b5uPndsLY"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an AI workflow that monitors Twitter (X) for relevant keywords and posts a reply to promote my business (Mention.com + X API)",
          "link": "https://old.reddit.com/r/n8n/comments/1kc5kin/efficient_serp_analysis_export_results_to_google/",
          "author": "Marvomatic",
          "time": "2025-05-01T10:37:34+00:00",
          "body_text": "Hey everyone, I wanted to share something I‚Äôve been using in my own workflow that‚Äôs saved me a ton of time: a set of free n8n templates for automating SERP analysis. I built these mainly to speed up keyword research and competitor analysis for content creation, and thought they might be useful for others here too. What these workflows do: Basically, you enter a focus keyword and a target country, and the workflow fetches organic search results, related searches, and FAQs from Google (using either SerpAPI or Serper). It grabs the top results for both mobile and desktop, crawls the content of those pages (using either Crawl4AI or Firecrawl), and then runs some analysis on the content with an LLM (I‚Äôm using GPT-4o-mini, but you can swap in any LLM you prefer). How it works: You start by filling out a simple form in n8n with your keyword and country. The workflow pulls SERP data (organic results, related searches, FAQs) for both device types. It then crawls the top 3 results (you can adjust this) and analyzes the content by using an LLM. The analysis includes article summaries, potential focus keywords, long-tail keyword ideas, and even n-gram analysis if there‚Äôs enough content. All the data gets saved to Google Sheets, so you can easily review or use it for further research. What the output looks like: At the end, you get a Google Soreadsheet with: The top organic results (URLs, titles, snippets) Summaries of each top result Extracted FAQs and related searches Lists of suggested keywords and long-tail variations N-gram breakdowns for deeper content analysis Why Three Templates? I included three templates to give you flexibility based on your preferred tools, budget, and how quickly you want to get started. Each template uses a different combination of SERP data providers (SerpApi or Serper) and content crawlers (Crawl4AI or Firecrawl). This way, you can choose the setup that best fits your needs‚Äîwhether you want the most cost-effective option, the fastest setup, or a balance of both. Personally, I‚Äôm using the version with Serper and Crawl4AI, which is pretty cost-effective (though you do need to set up Crawl4AI). If you want to get started even faster, there‚Äôs also a version that uses Firecrawl instead. You can find the templates on my GitHub profile https://github.com/Marvomatic/n8n-templates . Each template has it's own set up instructions in a sticky node. If anyone‚Äôs interested, I‚Äôm happy to answer questions. Would love to hear any feedback or suggestions for improvement!",
          "extracted_links": [
            "https://github.com/Marvomatic/n8n-templates"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "So far this is what I have learnt in last 6 months in n8n, what else do I need to do next so as to be able to say... work for an enterprise client needing AI automation ...may be using n8n.",
          "link": "https://old.reddit.com/r/n8n/comments/1lfcnar/from_reddit_problem_to_business_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-19T14:46:50+00:00",
          "body_text": "The best way to make money is by finding and solving real-world problems that people are actively discussing. This guide provides a step-by-step framework for building a no-code AI agent that automatically finds these business problems on Reddit and generates potential money-making solutions, helping you uncover profitable niches you might have otherwise missed. This workflow automates the entire discovery process. Here are the actionable tips to build this idea factory yourself: Step 1: Find the Problems (Reddit Node) Start your n8n workflow with the \"Reddit\" node. Configure it to search specific subreddits, like r/smallbusiness , for posts containing keywords that indicate a need, such as \"looking for a solution\" or \"recommendation\". Step 2: Filter the Noise (IF Node) Add an \"IF\" node to filter the results for quality. You can set conditions to only process posts with real engagement, like having at least two upvotes, and ensure the idea is recent by only including posts from the last six months. Step 3: AI Problem Identification Connect an AI Agent to the filtered posts. Its only job is to read the post and determine if it describes a business-related problem, then output a simple \"yes\" or \"no\". Step 4: AI Solution Generation For every post that the AI identifies as a problem, send it to another AI tool, like the OpenAI node. Prompt this AI to suggest a business idea or service that could solve the problem described in the post. Step 5: Store Your Goldmine (Google Sheets) Finally, take all the organized data‚Äîthe original post, the problem summary, and the new business idea‚Äîand save it into a Google Sheet for easy review and analysis.\nIf you can do this, you will have an automated system that finds profitable niches and delivers validated business ideas straight to a spreadsheet. Made a full video to explain everything:- https://youtu.be/Aovh878cJc4 What subreddits would you monitor with a system like this? Let me know in the comments!",
          "extracted_links": [
            "https://youtu.be/Aovh878cJc4"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How To SelfHost n8n On Railway, Setup Community Nodes, and Integrate FireCrawl MCP Server without touching a Single Line Of Code !",
          "link": "https://old.reddit.com/r/n8n/comments/1lyd7pe/four_types_of_ai_systems_that_actually_sell_and/",
          "author": "ChaseAI",
          "time": "2025-07-12T22:41:32+00:00",
          "body_text": "There was a great post on r/AI_Agents the other week that went through 6 months of Upwork data to get some insight on what sorts of AI systems companies are no shit paying money for. Believe it or not, it wasn't ASMR glass fruit cutting videos. Here is the quote: The Common Pain Points I Saw Repeated Over and Over: ‚ÄúI‚Äôm drowning in lead gen, I need this to run on autopilot‚Äù\n‚ÄúI get too many junk messages on WhatsApp / LinkedIn ‚Äî need something to filter and qualify leads‚Äù\n‚ÄúI have 10,000 rows of customer data and no time to sort through it manually‚Äù\n‚ÄúI want to turn YouTube videos into blog posts, tweets, summaries‚Ä¶ automatically‚Äù\n‚ÄúCan someone just connect GPT to my CRM and make it smart?‚Äù Exact Automations Clients Paid For: WhatsApp ‚Üí GPT lead qualification ‚Üí Google Sheets CRM\nAuto-reply bots for DMs that qualify and tag leads\nBrowser automations for LinkedIn scraping & DM follow-ups\nn8n flows that monitor RSS feeds and creates a custom news aggregator for finance companies For anyone whose been working with real clients in this space for any amount of time, this should come as no surprise. I think you can divide this up into roughly four categories: lead generation automations, lead qualification automations, content creation agents, and CRM integrated RAG agents. Those four alone cover probably 80% of the solutions I've sold since I started my agency so it makes sense to see that echoed in the data. I think people who want to get into the ai agency business get extremely overwhelmed with the breadth of \"viral\" n8n workflows shoved in their face on social media so I thought it would be helpful to not only call out the ones mentioned in this post but also run through an example for each. I'll link my YouTube video that goes over each of them in depth. The workflows for each are also freely available in the respective video description. ----- 1) Lead Generation https://www.youtube.com/watch?v=bFxWRkWAFzs User inputs the leads he wants, research is conducted on the sourced leads via Apollo + Tavily, research is used to create custom messaging, all info is then sent to Instantly.ai. I like this one since it all gets rolled into Instantly which handles all the cold email BS (email warm up, etc) and you won't have to bother with creating some fancy dashboard from scratch. 2) Lead Qualification https://www.youtube.com/watch?v=Vmgmva3dL44 This is a very simple foundation you could take a build upon to qualify leads. This uses gmail, but that could obviously be swapped for a true CRM input. Lead's info comes in, AI sorts / classifies / qualifies, and then you go down whatever path is appropriate from there. Really easy to customize and implement for a client. 3) Content Creation https://www.youtube.com/watch?v=mzxdtDCZbNY https://www.youtube.com/watch?v=Qp3h7WLYpH0 Obviously content comes in a billion different forms but it doesn't need to be complicated. Honestly I've found that clients are more pressed when it comes to just consistently posting the content across multiple platforms than they are about creating it but almost everyone wants some mega-simple LinkedIn post generator. 4) RAG https://www.youtube.com/watch?v=nwR5519zTC8 This form of RAG is as simple as it gets but I'm telling you most client's \"RAG Agent\" is a glorified FAQ chatbot that is able to locate and link internal documents to lazy employees. Understanding this very basic form of RAG will get you 90% of the way there. ----- The big takeaway is that the technical bar you must cross in order to make money selling AI solutions is not nearly as high as you think it is (the real hurdle is sourcing clients but that's an entirely different problem). The main issue is people get pulled into a million different directions trying to copy these over-engineered and flashy n8n workflows that are usually completely worthless instead of just mastering a handful of tried and true value generators. Hope this helps.",
          "extracted_links": [
            "https://www.youtube.com/watch?v=bFxWRkWAFzs",
            "https://www.youtube.com/watch?v=Vmgmva3dL44",
            "https://www.youtube.com/watch?v=mzxdtDCZbNY",
            "https://www.youtube.com/watch?v=Qp3h7WLYpH0",
            "https://www.youtube.com/watch?v=nwR5519zTC8"
          ],
          "link_count": 5
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Included",
          "link": "https://old.reddit.com/r/n8n/comments/1lt91sn/i_built_an_n8n_ai_agent_that_produces_educational/",
          "author": "Silent-Willow-7543",
          "time": "2025-07-06T18:59:32+00:00",
          "body_text": "In this step-by-step tutorial, I show you how to build a fully automated workflow using AI Agents and n8n to generate geography explainer videos on autopilot ‚Äî perfect for teachers, creators, or anyone looking to share knowledge at scale. What you‚Äôll learn:\n‚úÖ Generate video scripts with AI Agents\n‚úÖ Automatically source images, videos, and maps\n‚úÖ Assemble and edit videos hands-free\n‚úÖ Publish educational content consistently This workflow saves hours and helps you teach the world smarter without coding or manual editing. Tools used: LLM model (Gemini)\nGoogle APIs (Image generation)\nVideo generation: Runway ML\nVoice over (Elevenlabs)\nCaptions (Elevenlabs)\nFFmeg for combining the audio, video and captions into one MP4 file Sample video - Life in the Amazon: Uncontacted Tribes https://youtu.be/EZySnlX_yyQ",
          "extracted_links": [
            "https://youtu.be/EZySnlX_yyQ"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Tutorial",
          "link": "https://old.reddit.com/r/n8n/comments/1j7vk1j/ai_news_reporter_ai_video_ai_audio_ai_music_ai/",
          "author": "gochapachi1",
          "time": "2025-03-10T10:28:56+00:00",
          "body_text": "Workflow Do give an upvote you guys, Discover how to create a professional AI news reporter video using an automated n8n workflow! In this video, we demonstrate an end-to-end process that integrates various AI tools and automated video editing techniques to produce a fully polished news video. Here's what you'll learn: AI Video Model Generation: Automatically generate realistic video models using AI. AI Audio Creation: Generate high-quality AI audio for the model with perfect lipsync. AI Music Generation: Create custom background music using AI to add the perfect vibe to your video. Automated Editing & Transitions: Utilize advanced video editing techniques and seamless transitions with ffmpeg integrated into the n8n workflow. Complete End-to-End Automation: Watch as the entire process‚Äîfrom content creation to final editing‚Äîis fully automated, saving time and effort. Whether you're a content creator, media professional, or just curious about the power of automation and AI, this workflow offers a glimpse into the future of video production. Workflow:- https://github.com/gochapachi/AI-news-Reporter Youtube :- https://youtu.be/Km2u6193pDU If you enjoyed this video, please like, comment, and subscribe for more content on AI-driven automation and innovative video production techniques. Let's revolutionize content creation with AI and automation! üëâ Follow Us on Social Media for More Updates: üß† Reddit: https://www.reddit.com/user/gochapachi1/ üìò Facebook: https://facebook.com/gochapachi/ üì∏ Instagram: https://www.instagram.com/gochapachi/ üé• YouTube: https://www.youtube.com/@gochapachi üíº LinkedIn: https://www.linkedin.com/in/gochapachi/ üìû whatsapp: +91-8400210108 üì© Email:¬†[ sanjeevcs0034@gmail.com ](mailto: sanjeevcs0034@gmail.com )",
          "extracted_links": [
            "https://github.com/gochapachi/AI-news-Reporter",
            "https://youtu.be/Km2u6193pDU"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "How I turned n8n automation into software with Supabase and Windsurf",
          "link": "https://old.reddit.com/r/n8n/comments/1lvynu2/a_fully_modular_ai_agentfree_code_with_mcps_for/",
          "author": "croos-sime",
          "time": "2025-07-10T00:12:36+00:00",
          "body_text": "This is how you can empower your AI Agents using MCPs (Model Context Protocols). MCPs are a great way to extract and reuse logic between different agents. If you're reading this and you're a developer, a quick comparison would be with a microservice or an API: you encapsulate specific logic within an MCP. If you're not, it's a very cool way to save code (rare to say this in a no-code tool) and reuse it. It's recommended that each MCP does one thing well, that it be a service focused on a single functionality. For example, one MCP specializes in Reddit-related actions, another for Calendar, and another for Supabase. A very powerful approach to starting to implement MCPs in your agents (especially since it's so easy to integrate with N8n) is to leverage this tool as the primary means of communication with the agents. It's the best way to do it because each MCP can clearly explain what it needs, and if you follow a pattern like the one I use (where each MCP has multiple actions, and each action requires a specific JSON), implementation becomes much simpler. I've been developing software for over eight years, and in my experience, this clear, modular structure allows you to maintain scalability and order as your system grows. It's a much more organized alternative than connecting 10 different nodes for each functionality within the agent. You'll simplify the structure, reduce friction in prompts, and make the agent's job easier. This is an agent I use daily because I need to search for content on Reddit, read comments, keep my database up to date, and always know if I have an upcoming meeting or class. It's tailored to my real problems and needs. But you can create as many agents as you want, and the most interesting thing is that MCPs can be reused across different agents, which increases their value. The approach I use for each MCP is that it calls a node in N8n that executes another workflow. All MCPs call the same base workflow, but they are passed a value called an action. This action determines which path the workflow will take when processing the request. Each workflow may require a different JSON, and when writing the description of the MCP (and its actions) for the agent, that content serves as a reference to know, based on the user's request, what action to use, how to use it, and what JSON to send to the MCP. This allows for flexible and precise interaction, without the need to code rigid paths for each feature. All code here: https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md In the link you have a youtube video to dive deep on it",
          "extracted_links": [
            "https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1m4tjkj/share_a_wordpress_autopublishing_workflow/",
          "author": "gnosjah",
          "time": "2025-07-20T16:41:56+00:00",
          "body_text": "full workflow My customer recently asked me to build a fully automated content publishing system with WordPress and n8n. Naturally, I went looking for inspiration‚Äîbut to my surprise, there wasn‚Äôt a single public or usable WordPress + n8n basic workflow out there. So I rolled up my sleeves and built one from scratch. üíÄ It took more time than I expected, but the result was worth it: This workflow now runs daily, hands-free, and has already brought my client real traffic and positive ROI. They even started recommending it to others. That‚Äôs why I‚Äôve decided to share it with the community‚Äîso you can use it, improve it, or even build on it to do something better. I have posted the code here: https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json ‚∏ª Happy to give back to this amazing community üôå",
          "extracted_links": [
            "https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Why is reading 2 inputs so difficult in a code node?",
          "link": "https://old.reddit.com/r/n8n/comments/1m55gyt/built_an_ai_agent_in_n8n_for_youtube_market/",
          "author": "kyle4real",
          "time": "2025-07-21T01:05:56+00:00",
          "body_text": "https://preview.redd.it/3gav577mn4ef1.png?width=1920&format=png&auto=webp&s=dd788d8ebfe15a113d6bcd973cba8162c95fcfc3 Hey r/n8n !! If you're into AI automation and content creation, I just dropped a tutorial on building an AI agent in n8n that analyzes YouTube channels on the fly. You chat with it about any channel, and it scrapes video data (views, likes, duration, etc.), saves it to a Google Sheet, and answers smart market analysis questions to help brainstorm better content strategies. Why it's cool for creators/businesses: Ask stuff like: \"What are this channel‚Äôs top 5 videos?\" \"Which ones went viral?\" \"Average engagement rate?\" \"Videos with unusual view-to-like ratios?\" or \"Which underperformed?\" It pulls insights you can't get from just browsing a channel's popular vids ‚Äì perfect for niche research or competitor spying. Handles multiple channels for comparisons, with optimizations to avoid redundant API calls. Video structure: Quick demo (00:53) High-level overview (02:13) Step-by-step build: Get Channel Info (04:18), Get Channel Data (09:13), AI Agent setup (19:16), System Prompt (22:40) Testing & thoughts (24:43) What you'll learn in n8n: Creating a chat-based AI agent (using OpenAI) Setting up multi-agent workflows Capturing YouTube data via APIs/HTTP nodes Reading/updating Google Sheets Connecting credentials for YouTube, Sheets, and OpenAI It's a great example of chaining n8n nodes for real-world AI tools ‚Äì from HTTP requests and code parsing to tool calling and prompts. Watch here: https://www.youtube.com/watch?v=h_oPOkDLw28 Download template/code here: https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI?usp=sharing If you build this, share your results or mods! Anyone used n8n for similar YouTube/API projects? Tips on scaling for big channels or adding more features? Let's chat ‚Äì open to suggestions for future vids too. Thanks for the support! üöÄ (Music creds in vid desc)",
          "extracted_links": [
            "https://www.youtube.com/watch?v=h_oPOkDLw28",
            "https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "N8n workflow for graphics/infographics with templates",
          "link": "https://old.reddit.com/r/n8n/comments/1lyb3nc/reddit_curation_automated/",
          "author": "kenmiranda",
          "time": "2025-07-12T21:04:34+00:00",
          "body_text": "Snippet of the workflow I noticed an uptick in content here and in some of the other subreddits I follow. Like a lot of people, I kept thinking ‚ÄúI just want to see more of X instead of Y\". So I built an n8n workflow that pulls posts from your favorite subreddits, runs them through a filter using upvote ratio and AI Agents, and then sends the best ones to a Slack channel. It also summarizes longer posts and checks for duplicates using Supabase. I included the SQL DDL as this will be required for this setup. It's currently setup to use supabase as the database, Openai as the LLM, and slack as the communication app. Feel free to tear it apart and make it your own. Have fun! Here is the GitHub repo: https://github.com/kenandrewmiranda/reddit-content-filter-n8n Here is just the JSON: https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json Example of an output: https://preview.redd.it/q8kd44w9cicf1.png?width=1906&format=png&auto=webp&s=9735474a772c45980318a1ea3844b9f5d2a07534",
          "extracted_links": [
            "https://github.com/kenandrewmiranda/reddit-content-filter-n8n",
            "https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Novice question about changing a step in workflow",
          "link": "https://old.reddit.com/r/n8n/comments/1lrkam5/minitutorial_how_to_easily_scrape_data_from/",
          "author": "dudeson55",
          "time": "2025-07-04T14:36:43+00:00",
          "body_text": "I‚Äôve gotten a bunch of questions from a previous post I made about how I go about scraping Twitter / X data to generate my AI newsletter so I figured I‚Äôd put together and share a mini-tutorial on how we do it. Here's a full breakdown of the workflow / approaches to scrape Twitter data This workflow handles three core scraping scenarios using Apify's tweet scraper actor ( Tweet Scraper V2 ) and saves the result in a single Google Sheet (in a production workflow you should likely use a different method to persist the tweets you scrape) 1. Scraping Tweets by Username Pass in a Twitter username and number of tweets you want to retrieve The workflow makes an HTTP POST request to Apify's API using their \"run actor synchronously and get dataset items\" endpoint I like using this when working with Apify because it returns results in the response of the initial http request. Otherwise you need to setup a polling loop and this just keeps things simple. Request body includes maxItems for the limit and twitterHandles as an array containing the usernames Results come back with full tweet text, engagement stats (likes, retweets, replies), and metadata All scraped data gets appended to a Google Sheet for easy access ‚Äî This is for example only in the workflow above, so be sure to replace this with your own persistence layer such as S3 bucket, Supabase DB, Google Drive, etc Since twitterHandles is an array, this can be easily extended if you want to build your own list of accounts to scrape. 2. Scraping Tweets by Search Query This is a very useful and flexible approach to scraping tweets for a given topic you want to follow. You can really customize and drill into a good output by using twitter‚Äôs search operations. Documentation link here: https://developer.x.com/en/docs/x-api/v1/rules-and-filtering/search-operators Input any search term just like you would use on Twitter's search function Uses the same Apify API endpoint (but with different parameters in the JSON body) Key difference is using searchTerms array instead of twitterHandles I set onlyTwitterBlue: true and onlyVerifiedUsers: true to filter out spam and low-quality posts The sort parameter lets you choose between \"Top\" or \"Latest\" just like Twitter's search interface This approach gives us much higher signal-to-noise ratio for curating content around a specific topic like ‚ÄúAI research‚Äù 3. Scraping Tweets from Twitter Lists This is my favorite approach and is personally the main one we use to capture and save Tweet data to write our AI Newsletter - It allows us to first curate a list on twitter of all of the accounts we want to be included. We then pass the url of that twitter list into the request body that get‚Äôs sent to apify and we get back a list of all tweets from users who are on that list. We‚Äôve found this to be very effective when filtering out a lot of the noise on twitter and keeping costs down for number of tweets we have to process. Takes a Twitter list URL as input (we use our manually curated list of 400 AI news accounts) Uses the startUrls parameter in the API request instead of usernames or search terms Returns tweets from all list members in a single result stream Cost Breakdown and Business Impact Using this actor costs 40 cents per 1,000 tweets versus Twitter's $200 for 15,000 tweets a month. We scrape close to 100 stories daily across multiple feeds and the cost is negligible compared to what we'd have to pay Twitter directly. Tips for Implementation and working with Apify Use Apify's manual interface first to test your parameters before building the n8n workflow. You can configure your scraping settings in their UI, switch to JSON mode, and copy the exact request structure into your HTTP node. The \"run actor synchronously and get dataset items\" endpoint is much simpler than setting up polling mechanisms. You make one request and get all results back in a single response. For search queries, you can use Twitter's advanced search syntax to build more targeted queries. Check Apify's documentation for the full list of supported operators. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=otK0ILpn4GQ The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json",
          "extracted_links": [
            "https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json",
            "https://www.youtube.com/watch?v=otK0ILpn4GQ"
          ],
          "link_count": 2
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Learn Faster: 8 Common Mistakes I Correct with Every Student I Mentor",
          "link": "https://old.reddit.com/r/n8n/comments/1kr506d/i_made_a_docker_chrome_with_full_vnc_access_for/",
          "author": "conor_is_my_name",
          "time": "2025-05-20T13:34:10+00:00",
          "body_text": "TLDR: This Docker container gives you full visual control of Chrome with VNC access‚Äîperfect for scraping tricky sites, testing, or logged-in sessions. If you are new to web scraping this makes a lot of things easier! üîó GitHub Link: https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer Who is this for? Scrapers battling sites requiring logins, CAPTCHAs, or dynamic content. Developers who need to debug visually or automate complex interactions. Anyone who has wasted hours trying to make Puppeteer/Playwright work headlessly when a real browser would‚Äôve taken 5 minutes. (this is me) Stealth mode users who want the most realistic browser usage with minimal chance of detection. I made this because I wanted to do analysis on long form journalism articles.  All of my sources required logins to read the articles, and had pretty strong subscription and login checking protocols.  Even though I actually do pay for these subscriptions and have valid credentials, it was tricky to get the logins to work in headless mode. Basically, you can connect to a full GUI chrome running on a server, raspberry pi, even your own local machine, and then control it programmatically.  In my case, I remote into the GUI, log into the website as needed in a fully normal chrome browser instance, and then run my scripts. Use page.close() instead of browser.close() to end your scripts.  This will keep the browser open and ready for a new command. You will need to restart the container if you pass a browser.close() command. Why this beats headless mode: Full Chrome GUI in a container‚Äîjust like your local browser, but remote-controlled. VNC access (with audio support if needed). Pre-loaded with Puppeteer for scripting inside or outside the container. Persistent sessions (no more re-logging in every scrape). Downsides: S low R esource Heavy (but sometimes it doesn't matter: skipping login scripting and captchas can more than make up for a slow scraper) What‚Äôs inside? Chrome Stable (+ all dependencies). VNC for multiple remote access options. Puppeteer/Playwright-compatible ‚Äîuse your existing scripts. Easy volume mounts to save profiles/sessions. n8n json starter Install in 2 commands: git clone https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\n\ndocker compose up -d Then connect via VNC (default password: password ) Example n8n nodes are included: U pdate the IP address, everything else will be automatic. Use Code Node for your scripts.  This allows way more customization than using the community nodes. Tested on: 4GB+ RAM recommended for smooth VNC. Raspberry Pi 4 (ARM support included!). Here's the link to the ARM version Pair this with my n8n autoscaling setup for next-level workflow execution speed . Need n8n consulting? DM me for help with your projects.",
          "extracted_links": [
            "https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Mini-Tutorial: How to easily scrape data from Twitter / X using Apify",
          "link": "https://old.reddit.com/r/n8n/comments/1leypz5/ai_lofi_girl_youtube_channel_automation/",
          "author": "Silent-Willow-7543",
          "time": "2025-06-19T01:45:36+00:00",
          "body_text": "This workflow is designed to fully automate your AI-powered LoFi Girl YouTube channel, enabling you to scale content creation and grow your channel with minimal ongoing effort. It handles everything‚Äîfrom generating visuals and animations to composing music and posting directly to YouTube‚Äîso you can focus on strategy, branding, or other creative pursuits. The workflow leverages multiple AI tools and APIs to streamline each part of the content creation pipeline. First, it uses Google image APIs to generate unique background images based on prompt inputs or trending aesthetic themes. These images serve as the visual foundation for each video. Next, Runway ML‚Äôs API is integrated to animate the LoFi Girl and other scene elements, bringing your visuals to life with smooth, cinematic motion. This part of the workflow adds a professional polish to your videos and helps differentiate your content in a crowded niche. Note that while Runway ML offers a powerful animation engine, it is the only paid tool in this stack. For music, the workflow connects to Segmind, a free AI tool that generates LoFi-style music tracks. These tracks are unique, royalty-free, and tuned to the vibe you specify, making it easy to build a consistent sonic brand across your uploads. Finally, the entire asset bundle‚Äîimage, animation, and audio‚Äîis compiled into a video. Using YouTube‚Äôs Data API, the workflow can auto-upload the video with relevant metadata, thumbnails, and scheduling options. You can even predefine tags and titles for SEO optimization. Here‚Äôs an example output from the workflow, showing how all these tools work together to produce a high-quality LoFi video‚Äîautomatically.\n - https://youtu.be/ISNGL9IvQik",
          "extracted_links": [
            "https://youtu.be/ISNGL9IvQik"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "I built an agentic recruiting pipeline with n8n that analyzes resumes against job requirements - here's how",
          "link": "https://old.reddit.com/r/n8n/comments/1luuwkp/how_i_built_a_100_free_aipowered_faceless_video/",
          "author": "Valuable-Range-6785",
          "time": "2025-07-08T17:36:20+00:00",
          "body_text": "Hi everyone, I‚Äôve been automating my content creation and distribution workflow lately, and I thought I‚Äôd share something that might help those of you building with AI + no-code tools. A few days ago I created a system that: Generates faceless, illustrated AI videos automatically Schedules & posts them to all major social platforms (YouTube Shorts, TikTok, Instagram Reels, LinkedIn) Does 100% for free using open-source and free-tier tools Powered by n8n , with triggers, GPT prompts, video-generation, and posting all set up in a workflow I go through: How to set up your n8n environment (no server, no subscription) How to generate the visuals, script, and voice from text How to stitch the video together and post automatically Customizations: branding, posting cadence, scheduling logic For anyone looking to build a hands-free content pipeline or learn how to combine AI + no-code, this could be a helpful reference. The setup runs entirely on the free tier of tools! Watch the full tutorial here: üëâ https://youtu.be/TMGsnqit6o4?si=Y7sxXSV7y4yZ0D0p",
          "extracted_links": [
            "https://youtu.be/TMGsnqit6o4"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1m1oqa9/i_built_a_workflow_that_creates_edit_comments_for/",
          "author": "ComfortableArm121",
          "time": "2025-07-16T21:12:34+00:00",
          "body_text": "I got tired of editing my essays on chatgpt because it was annoying to pinpoint the corrections. Now, I just get a comment of what could be better, and I can choose to accept or deny the edit. I generated this with https://alpha.osly.ai/ and I've attached the code here https://github.com/Osly-AI/google-doc-editor .",
          "extracted_links": [
            "https://github.com/Osly-AI/google-doc-editor"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1lvht0i/i_built_an_automation_to_autoorganize_my_inbox/",
          "author": "Milan_SmoothWorkAI",
          "time": "2025-07-09T12:42:44+00:00",
          "body_text": "Discover how to easily extend an AI-generated video from a single image using KAI.AI , VEO3, and automation tools like NCA Toolkit and FFmpeg. Learn how to automate video sequences, extract key frames, and create longer, more dynamic videos ‚Äî perfect for creative projects or content creation! Video: https://www.youtube.com/watch?v=qc00jJ7bCBA Workflow: https://pub-7710cf36e9f24295acffe6938f40f147.r2.dev/Extend%20Video%20with%20NCA%20Toolkit.json",
          "extracted_links": [
            "https://www.youtube.com/watch?v=qc00jJ7bCBA"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lebpkb/from_reddit_complaint_to_saas_idea_in_5_minutes/",
          "author": "Legitimate_Fee_8449",
          "time": "2025-06-18T08:18:32+00:00",
          "body_text": "I automated a full AI-powered video pipeline that creates faceless vertical stories like the miniature worker / tiny character genre you‚Äôre seeing blow up right now. Why? Because writing, designing, animating, soundtracking, and publishing even one of these manually is a multi-hour grind - and I wanted to publish daily without touching CapCut. So I built an end-to-end workflow in n8n that does it all, 100% automated. Perfect for: AI-generated short stories & mini animations YouTube Shorts / Instagram Reels / TikTok Faceless vertical video channels (well you can adapt it for 16:9 as well - change the aspect ratio) Clients who want storytelling content with AI flair Follow along this tutorial here. Explanation here : https://youtu.be/Z1n6nU9O0BA Example video with the lowest settings https://youtube.com/shorts/LN46liFamoY üß† How It Works The system turns a scheduled trigger into a fully animated AI-generated story , complete with voiceover, visuals, and sound FX, and background music. Here‚Äôs the full 8-step pipeline: 1. Story Idea Generation GPT-4 creates original video ideas e.g., ‚ÄúTiny workers making a large pizza‚Äù 2. Scene Breakdown + Visual Planning Each story is split into multiple scenes with structured prompts and reference descriptions. 3. Reference + Scene Images via FAL / FreePik FAL‚Äôs image generation API creates high-quality scene visuals - one per segment, based on the story and setting. In the workflow you have option for using fal/general for using lora's and controlnets if you wish. Freepik integration is also there - Started with this, kept in the workflow (good images, but slow generation) - absolutely free image generation - with rate limiting of course. 4. Scene Animation via Kling  - or Minimax (ran out of minimax's $50 dollar credit so Swtiched to Kling via Fal) Each image is passed to an animation engine that gives movement, transitions, and subtle effects (like camera pan or zoom) for that short-form storytelling vibe. 5. Bacground music + SFX from ElevenLabs AI-generated voiceover narrates the story, matched with ambient music and sound effects, all created using ElevenLabs. 6. Video Compilation via FFmpeg A custom command stitches the scenes, SFX, and music together, handles crossfades, syncs timing, and applies volume balancing. No JSON2Video or Creatomate - they can become expensive. 7. YouTube Upload The finished video is uploaded directly to YouTube Shorts via resumable upload. Upload to reels and Tiktok (coming soon) 8. Logged in Google Sheets Each published video is logged for tracking, repurposing, or reuse. üîß Tools Used Function Tool Idea & Script OpenAI GPT-4 Images & Scenes FAL (Flux) with lora support Animation Hailuo API / Kling Voice + SFX ElevenLabs Stitching FFmpeg Automation n8n Publishing YouTube API Tracking Google Sheets ‚öôÔ∏è Key Features No video editing required. Fully modular, works scene-by-scene. Customize prompt style for any genre (horror, fantasy, sci-fi, slice of life). Built in n8n , no-code setup. Ideal for scaling faceless content at volume. AI is not perfect always, it will make mistakes üöÄ Why This Works Saves 10 - 15 hours/week Produces daily, high-quality, AI-first content Scalable for multiple channels or client brands Zero editing tools or manual uploads needed Great for storytelling niches, explainer content, or abstract visual art Explanation here: https://youtu.be/Z1n6nU9O0BA",
          "extracted_links": [
            "https://youtu.be/Z1n6nU9O0BA"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Create Powerful AI Videos with Veo 3 Fast in n8n ‚Äì No KYC or Org Verification Needed!",
          "link": "https://old.reddit.com/r/n8n/comments/1m1octi/create_powerful_ai_videos_with_veo_3_fast_in_n8n/",
          "author": "automayweather",
          "time": "2025-07-16T20:57:43+00:00",
          "body_text": "Just built a seamless automation using n8n and the Veo 3 Fast API from Ulazai.com. Automatically generates AI videos in seconds Cheapest pricing available for Veo 3 Fast (just $1 per video!) No KYC, instant API access Supports the new vertical (9:16) aspect ratio Bonus: use OpenAI‚Äôs image generation without organizational verification‚Äîespecially helpful now after OpenAI‚Äôs recent model update! ulazai.com - VEO 3 Fast Ideal for automation enthusiasts, marketers, and developers looking to leverage cutting-edge AI content creation without complex setup. üëâ Quick-start guide & API docs + Workflow**:** https://ulazai.com download the workflow: https://gist.github.com/smrht/82c072241a679ae827421a1c61fd35b9",
          "extracted_links": [],
          "link_count": 0
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Built my first advanced flow! Help me simplify it?",
          "link": "https://old.reddit.com/r/n8n/comments/1i66chs/i_used_ai_and_n8n_to_automate_youtube_shorts/",
          "author": "Horizon-Dev",
          "time": "2025-01-21T00:54:02+00:00",
          "body_text": "Hey everyone! I've been experimenting with n8n and AI automation to streamline the process of creating YouTube Shorts , and I wanted to share my results. In this video, I walk through how I: ‚úÖ Used n8n to automate the entire workflow ‚úÖ Integrated AI tools to generate content dynamically ‚úÖ Automated script creation ‚úÖ Grabbed Videos to clip together ‚úÖ Generated voice, and grabbed music ‚úÖ Optimized the system to make the process scalable If you guys have questions, or would do something differently let me know! üîó Watch the full breakdown here: https://youtu.be/g3XsT_55xTA",
          "extracted_links": [
            "https://youtu.be/g3XsT_55xTA"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Is AI about to become the most groundbreaking productivity tool in human history, outpacing the wheel and the internet?",
          "link": "https://old.reddit.com/r/n8n/comments/1lq3glp/showoff_i_automated_trustpilot_outreach_review/",
          "author": "automayweather",
          "time": "2025-07-02T18:28:50+00:00",
          "body_text": "I just built an automation that turns Trustpilot reviews into actual lead gen opportunities ‚Äî especially useful for freelancers, outreach agencies, and DFY reputation management services. üîß What it does (fully automated via n8n): Scans any Trustpilot profile (even unclaimed ones) Detects if the business already responded to reviews Uses Google Gemini (via OpenRouter) to write realistic, helpful replies Sends a clean HTML report via Gmail to the business Logs everything in Google Sheets with timestamps & outreach status Works with Dutch üá≥üá± and English üá¨üáß reviews (auto-detect) üí° Why it matters: Most ecommerce stores don‚Äôt reply to reviews at all. That‚Äôs a huge reputational gap ‚Äî and a perfect entry point for value-based outreach. Instead of spamming \"hey, want SEO?\", you send: üì¨ ‚ÄúHere‚Äôs what your customers said. Here‚Äôs how you could reply. Want to automate it?‚Äù ‚Üí Value first. Pitch second. üí• Built with: n8n (open source Zapier alternative) OpenRouter (Google Gemini model) Gmail Google Sheets üìΩÔ∏è Loom walkthrough + full workflow template available. https://reddit.com/link/1lq3glp/video/0tte6vqi7iaf1/player {\n  \"name\": \"Review Responder & Content Trustpilot V2 Samautomation.work\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"operation\": \"appendOrUpdate\",\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"columns\": {\n          \"mappingMode\": \"defineBelow\",\n          \"value\": {\n            \"TrustPilotUrL\": \"={{ $('Limit').item.json.TrustPilotUrL }}\",\n            \"Claimed\": \"={{ $('Limit').item.json.Claimed }}\",\n            \"Contacted\": \"={{ $now }}\",\n            \"Email\": \"={{ $('Limit').item.json.Email }}\"\n          },\n          \"matchingColumns\": [\n            \"TrustPilotUrL\"\n          ],\n          \"schema\": [\n            {\n              \"id\": \"TrustPilotUrL\",\n              \"displayName\": \"TrustPilotUrL\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Claimed\",\n              \"displayName\": \"Claimed\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Added\",\n              \"displayName\": \"Added\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Contacted\",\n              \"displayName\": \"Contacted\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Email\",\n              \"displayName\": \"Email\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            }\n          ],\n          \"attemptToConvertTypes\": false,\n          \"convertFieldsToString\": false\n        },\n        \"options\": {\n          \"useAppend\": true\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        2520,\n        -180\n      ],\n      \"id\": \"d0c3ea12-2a69-47ba-822a-b79f5eb4a44c\",\n      \"name\": \"Update Row Geplaatst\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"extractHtmlContent\",\n        \"extractionValues\": {\n          \"values\": [\n            {\n              \"key\": \"article_html\",\n              \"cssSelector\": \".styles_reviewCard__Qwhpy\",\n              \"returnValue\": \"html\",\n              \"returnArray\": true\n            },\n            {\n              \"key\": \"email_html\",\n              \"cssSelector\": \"a[href^=\\\"mailto:\\\"]\"\n            },\n            {\n              \"key\": \"unclaimed_profile\",\n              \"cssSelector\": \"div[class*=\\\"styles_label\\\"]\",\n              \"returnArray\": true\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"8ecff1e8-b4ee-49b4-9441-7e8d83f1c676\",\n      \"name\": \"Extract Article URLs\",\n      \"type\": \"n8n-nodes-base.html\",\n      \"position\": [\n        2000,\n        -540\n      ],\n      \"typeVersion\": 1.2\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"data\"\n            }\n          ]\n        },\n        \"options\": {\n          \"mergeLists\": true\n        }\n      },\n      \"id\": \"6c617d6b-1651-4d62-b77b-b18bf86698e4\",\n      \"name\": \"Combine Articles\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        2420,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"html\": \"={{ $json.article_html }}\",\n        \"options\": {}\n      },\n      \"id\": \"88aa1a9b-ebd0-4e87-afd6-b5ae6007e426\",\n      \"name\": \"Markdown\",\n      \"type\": \"n8n-nodes-base.markdown\",\n      \"position\": [\n        2220,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.UpdatedTrustpilotURL }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        1780,\n        -540\n      ],\n      \"id\": \"0c289d7a-b90b-4a3d-974e-6f20babd9384\",\n      \"name\": \"HTTP Request1\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Je reageert als een echt persoon, niet als AI. Gebruik NOOIT volgende corporate/AI-zinnen in welke taal dan ook.\\n\\nINHOUD REVIEW:\\n{{ $json['data[0]'] }}\\n\\nInstructies:\\n- Reageer namens de webshop\\n- Gebruik dezelfde taal als de review\\n- Ga concreet in op de aangehaalde punten\\n- Houd het professioneel √©n persoonlijk\\n- Lengte: 2-4 zinnen\\n- Gebruik geen typische AI-woorden!\\n\\nGeef alleen de tekst van de reactie terug, zonder verdere opmaak.\\n\",\n        \"options\": {\n          \"systemMessage\": \"=Je bent een warme, persoonlijke klantenservicemedewerker voor Nederlandse webwinkels. Schrijf korte, oprechte reacties op klantbeoordelingen in vloeiend Nederlands.\\n\\n## EERSTE STAP - VERPLICHTE CONTROLE\\n\\n**DENK EERST NA:** Analyseer de ontvangen data grondig voordat je begint met schrijven.\\n\\n**CONTROLEER ALTIJD EERST:** Is er al een reactie van het bedrijf aanwezig in de data? \\n- Zoek naar termen zoals: \\\"reactie van\\\", \\\"antwoord van\\\", \\\"bedrijf reageerde\\\", \\\"webshop antwoordt\\\", of vergelijkbare tekst die duidt op een bestaande bedrijfsreactie\\n- Zoek naar gestructureerde reacties die niet van de klant afkomstig zijn\\n- Let op tekst die duidelijk een professioneel antwoord van de webshop is\\n\\n**ALS ER AL EEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Stop onmiddellijk met verder analyseren\\n- Schrijf geen nieuwe reactie\\n- GEEF ALLEEN DEZE OUTPUT: \\\"{bedrijfsnaam} heeft al gereageerd.\\\"\\n\\n**ALS ER GEEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Ga verder met het schrijven van een nieuwe reactie volgens onderstaande regels\\n\\n## BELANGRIJKE REGELS\\n\\n- Gebruik nooit een em-dash (‚Äî); gebruik een gewoon koppelteken (-)\\n- Laat elke reactie natuurlijk klinken ‚Äì nooit als AI\\n- Wissel begroetingen en afsluitingen af - nooit 2x dezelfde achter elkaar\\n\\n**Negatieve reviews:**\\n- Als {{ $('Limit').item.json.Email }} een waarde heeft ‚Üí eindig de reactie met: \\\"Mail ons op {{ $('Limit').item.json.Email }} zodat we je direct verder kunnen helpen.\\\"\\n- Heeft het veld g√©√©n waarde ‚Üí noem geen e-mail\\n\\n## SCHRIJFSTIJL\\n\\n- Begin met \\\"Beste [Naam]\\\". Geen naam? Gebruik \\\"Beste klant\\\"\\n- Verwerk concrete details uit de review (product, datum, levering, enz.)\\n- Toon oprechte empathie, vermijd corporate jargon\\n- Gebruik dezelfde aanspreekvorm (je/jij of u) als de klant\\n- Sluit af met een uitnodiging of hulpaanbod - wissel dit af\\n\\n## STRUCTUUR (2-4 zinnen)\\n\\n1. **Begroet & bedank:** \\\"Beste [Naam], dank voor ‚Ä¶\\\" - Wees creatief en varieer\\n2. **Reageer specifiek:** Ga in op de concrete punten uit de review\\n3. **Toon waardering/empathie:** \\n   - Positief ‚Üí benadruk blijdschap\\n   - Negatief ‚Üí erken probleem, bied oplossing (+ e-mail indien beschikbaar)\\n4. **Nodig uit tot vervolg:** Varieer je afsluitingen\\n\\n## RESULTAAT\\n\\nLever alleen de tekst van de reactie, zonder verdere opmaak; 2-4 zinnen, warm, menselijk en in helder Nederlands.\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        820,\n        -180\n      ],\n      \"id\": \"466305a5-312e-4080-95ad-a2785fe953e2\",\n      \"name\": \"Review Responder\",\n      \"executeOnce\": false\n    },\n    {\n      \"parameters\": {\n        \"fieldToSplitOut\": \"data[0]\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitOut\",\n      \"typeVersion\": 1,\n      \"position\": [\n        2620,\n        -540\n      ],\n      \"id\": \"a8cd687e-db79-46c9-a2dc-e11e6ad10504\",\n      \"name\": \"Split Out\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Haal de originele URL en het gewenste paginanummer op\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\nconst pageNumber = 1;\\n\\n// Controleer of de originele URL al een paginaquery bevat\\nlet updatedUrl;\\nif (originalUrl.includes('?page=')) {\\n  // Vervang bestaande pagina-parameter\\n  updatedUrl = originalUrl.replace(/(\\\\?page=)(\\\\d+)/, `$1${pageNumber}`);\\n} else if (originalUrl.includes('?')) {\\n  // Voeg toe als er al andere queryparameters zijn\\n  updatedUrl = `${originalUrl}&page=${pageNumber}`;\\n} else {\\n  // Voeg toe als er nog geen queryparameters zijn\\n  updatedUrl = `${originalUrl}?page=${pageNumber}`;\\n}\\n\\n// Geef de nieuwe URL terug in de output\\nreturn [\\n  {\\n    json: {\\n      ...$json,\\n      UpdatedTrustpilotURL: updatedUrl\\n    }\\n  }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1580,\n        -540\n      ],\n      \"id\": \"ed03ac93-fc67-4e54-b1b7-07cfcac5104d\",\n      \"name\": \"Add Page to URL\"\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"output\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1180,\n        -180\n      ],\n      \"id\": \"5b9793b6-a45b-496f-aa4a-797450ea6f52\",\n      \"name\": \"Aggregate\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üìà {{ $json.bedrijfsnaam }} klantrespons rapport - Gratis\",\n        \"message\": \"={{ $('HTML parser').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -20\n      ],\n      \"id\": \"2021c9b1-c1db-4580-a05e-ef71a7a27d7e\",\n      \"name\": \"Send Review Respsonse\",\n      \"webhookId\": \"32b66b21-c68d-4386-b1db-3a11f8e9e2ea\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"rule\": {\n          \"interval\": [\n            {\n              \"field\": \"minutes\",\n              \"minutesInterval\": 15\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        800,\n        -540\n      ],\n      \"id\": \"1f2df98b-7a80-425d-9831-cdb2d9825b97\",\n      \"name\": \"Schedule Trigger\"\n    },\n    {\n      \"parameters\": {\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"filtersUI\": {\n          \"values\": [\n            {\n              \"lookupColumn\": \"Claimed\",\n              \"lookupValue\": \"Geclaimd profiel\"\n            },\n            {\n              \"lookupColumn\": \"Contacted\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        1000,\n        -540\n      ],\n      \"id\": \"69b6abc7-6df7-4f7a-a01d-1dab75c550b0\",\n      \"name\": \"Google Sheets\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1360,\n        60\n      ],\n      \"id\": \"4fc10a76-c8a3-4651-8e4e-733db12b624e\",\n      \"name\": \"OpenRouter Chat Model2\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"={{ $('Customer Response Aggregatio').item.json.output }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Nederlandse Klantrespons Analyse Agent (JSON Output)\\n\\n\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1960,\n        -180\n      ],\n      \"id\": \"74977264-1c23-4b4a-ad54-9bd0894df187\",\n      \"name\": \"International Customer Response Analysis Agent\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Reviews: {{ $json.output }}\\nExtract van de trustpilot URL de bedrijfsnaam: {{ $('Limit').first().json.TrustPilotUrL }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Klantrespons-Aggregatie-Specialist met Gepersonaliseerde Ice Breaker\\n\\n## Rol: \\nn```\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1380,\n        -180\n      ],\n      \"id\": \"adac1c81-9ccf-4725-883a-f351d8ee7b83\",\n      \"name\": \"Customer Response Aggregatio\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"d83b559f-1dfb-414c-9365-69e7193b77af\",\n              \"leftValue\": \"={{ $json.Email }}\",\n              \"rightValue\": \"NO EMAIL\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"notEquals\"\n              }\n            }\n          ],\n          \"combinator\": \"and\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        1180,\n        -540\n      ],\n      \"id\": \"1b53040d-986f-4e2f-8d42-788481c65a8b\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {},\n      \"type\": \"n8n-nodes-base.limit\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1400,\n        -660\n      ],\n      \"id\": \"24401a8f-8657-411b-9723-a369ecb56efb\",\n      \"name\": \"Limit\"\n    },\n    {\n      \"parameters\": {\n        \"content\": \"## Create Subworkflow\\n\\nStuur de NO EMAIL naar de subworkflow.\"\n      },\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1360,\n        -900\n      ],\n      \"typeVersion\": 1,\n      \"id\": \"22618369-e14f-4818-898f-89eea60d889a\",\n      \"name\": \"Sticky Note\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {\n          \"frequencyPenalty\": 0.5,\n          \"presencePenalty\": 0.7,\n          \"temperature\": 0.9\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        800,\n        40\n      ],\n      \"id\": \"e266310b-6362-47b0-b399-e5845a28f859\",\n      \"name\": \"OpenRouter Chat Model\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -300\n      ],\n      \"id\": \"d1de90ed-0b98-4534-8d0b-a08de0dbeac4\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -20\n      ],\n      \"id\": \"63fb494e-6878-4fed-b70d-37075f81cb3c\",\n      \"name\": \"Code1\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üîç Dit zeggen klanten over {{ $json.bedrijfsnaam }}\",\n        \"message\": \"={{ $('Code2').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -300\n      ],\n      \"id\": \"e5491ac2-0fc7-4f4e-9b52-2ccc26de78ff\",\n      \"name\": \"Review Analyse Rapport\",\n      \"webhookId\": \"b70f3f19-ee55-4572-9445-abe53056511c\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"description\": \"Use the tool to think about and analyze the review data before responding. It will not obtain new information or change the database, but just append your analysis to the log.\\nUse it specifically for:\\n\\nReviewing data structure: Analyze the incoming review data to identify all components (reviewer name, content, rating, date, etc.)\\nDetecting existing responses: Check if there's already a business response present in the data before writing a new one\\nLanguage analysis: Determine the language and tone of the review to match it appropriately\\nContent analysis: Identify key points mentioned (products, service aspects, specific complaints/compliments)\\nStrategy planning: Decide on the appropriate response approach (positive reinforcement, problem resolution, etc.)\\nQuality checking: Review your planned response before finalizing to ensure it meets the guidelines\\n\\nAlways think first before writing any review response to ensure proper analysis and avoid duplicate responses.\"\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.toolThink\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1000,\n        40\n      ],\n      \"id\": \"f3a6ce09-2bea-4701-99e4-8b403a0f6957\",\n      \"name\": \"Think\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1940,\n        40\n      ],\n      \"id\": \"11a5d333-abb6-45a2-96b9-1b5efcdc7dc0\",\n      \"name\": \"OpenRouter Chat Model3\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"//timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1740,\n        -180\n      ],\n      \"id\": \"064c4a24-fe58-4944-b3fc-38b7d1750f6f\",\n      \"name\": \"HTML parser\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// \\n        timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2320,\n        -180\n      ],\n      \"id\": \"14b30bdd-c774-45b0-ad98-e9c3a14da488\",\n      \"name\": \"Code2\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Update Row Geplaatst\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Code1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Article URLs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Markdown\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Markdown\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Combine Articles\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTTP Request1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Extract Article URLs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Combine Articles\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Out\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Out\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Responder\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Add Page to URL\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Schedule Trigger\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Sheets\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Sheets\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model2\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"International Customer Response Analysis Agent\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code2\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Customer Response Aggregatio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTML parser\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Limit\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        []\n      ]\n    },\n    \"Limit\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Add Page to URL\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Analyse Rapport\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Review Respsonse\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Analyse Rapport\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"Think\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model3\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTML parser\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code2\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Update Row Geplaatst\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": true,\n  \"settings\": {\n    \"executionOrder\": \"v1\",\n    \"callerPolicy\": \"workflowsFromSameOwner\",\n    \"errorWorkflow\": \"SGT2yyLX0uoBgIBI\"\n  },\n  \"versionId\": \"895363e2-bac3-46f0-b678-f31d7b874eea\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"190e1b81e9845aeb2f58812b2e7f9c8a86cb13b1e8883f6c2bfaa8050710ae79\"\n  },\n  \"id\": \"kuYaiJDh24Ta0NuG\",\n  \"tags\": []\n}",
          "extracted_links": [
            "https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Question",
          "link": "https://old.reddit.com/r/n8n/comments/1luvgkn/create_an_aipowered_upwork_job_scorer_and/",
          "author": "paperjamai",
          "time": "2025-07-08T17:57:02+00:00",
          "body_text": "Workflow code: https://jsonblob.com/1392186102303088640 YouTube video walkthrough: https://youtu.be/fVegeKYAzz8 Disclaimer: In order to use this workflow you'll have to subscribe to Upwork's Freelancer Plus plan, which is $20/mo and includes job alert email notifications (among other things). I assume most people who frequently work on Upwork are already subscribed, but don't want any surprises. --- When applying for freelance jobs on Upwork, minutes matter. The first quality application is more often than not the one that‚Äôs ultimately selected. Upwork's job alert emails are helpful, but filters are very limited. As a result, you often receive several dozen emails a day and it takes a lot of time to manually go through each email and determine if each job fits your criteria. I built a workflow that scans your Gmail every few minutes, finds all Upwork job alerts, scores them based on your profile/preferences, and sends a Slack channel message for jobs that are strong potential matches. I use this workflow myself and find that I've been applying to (and winning) more jobs than before, despite the fact that it filters out most alerts. The ones that come through are the right ones for me, and I have more confidence that I'm acting quickly and not wasting my time. Hopefully you find it easy to both get up and running and customize to your liking. Very open to feedback/questions!",
          "extracted_links": [
            "https://youtu.be/fVegeKYAzz8"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "From Star Trek to N8N: How I turned my 20 years of Corporate Ops experience into real-world automations",
          "link": "https://old.reddit.com/r/n8n/comments/1lkwqz7/multilevel_rag_ai_agent_guide/",
          "author": "Aggravating-Put-9464",
          "time": "2025-06-26T10:32:52+00:00",
          "body_text": "Hi! I had a client with >70 files in knowledge base - docs, presentation, tables and videos, who was needed to build RAG agent for internal use. The problem is that files have different overlapping data that doesn't suit for each request - we decided to add additional step for data retrieval to have more focused results. I built a two-step retrieval pipeline using Supabase + n8n, where you first filter files by their description/metadata, then only run vector search within those relevant files. The main problem that there is no way in original Supabase nodes in n8n to setup metadata filtration via array of values - there is hardcoded \"equal\" condition. To solve it I made custom tools with code and special Supabase functions. Video guide: https://youtu.be/asXVOHg89hs How it works: 1. When chat message received Starts the workflow whenever a user sends a question to the chatbot. 2. Find Files Takes the user‚Äôs query Generates an embedding (OpenAI) Runs a semantic search (via Supabase match_files function) Returns a list of files (with their IDs & similarity) that are the best match, based on the file description/metadata‚Äînot full file content yet 3. Filter Similarity Filters file matches so only those above a certain similarity score are kept Controls ‚Äúhow relevant‚Äù is relevant enough Useful for not passing irrelevant/weak matches 4. Aggregate Files Collects all matching file IDs into a list/array This is your shortlist of files to actually search in the next step 5. Database Retrieval (Knowledge Base) Takes the user‚Äôs original query Uses OpenAI to generate embedding again Runs Supabase vector search (match_documents) but filters by the file IDs from the previous step Returns only chunks from the shortlisted files, sorted by similarity, with file names and URLs 6. OpenAI (LLM Completion) Feeds the filtered, relevant chunks to the LLM to answer the user‚Äôs question Always returns the file URL as reference 7. Set Output Prepares and formats the final output for the user/chatbot",
          "extracted_links": [
            "https://youtu.be/asXVOHg89hs"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Discussion",
          "link": "https://old.reddit.com/r/n8n/comments/1lxxzcw/showcase_smarter_chatbots_with_n8n_agent_node/",
          "author": "Maamriya",
          "time": "2025-07-12T11:15:52+00:00",
          "body_text": "Smarter Chatbots with n8n Agent Node & OpenAI: Text + Voice, Step-by-Step I want to share a practical structure for building next-level chatbots and assistants by combining the n8n Agent node with OpenAI‚Äîhandling both text and voice messages in Telegram, all with an AI agent. What‚Äôs unique about this approach is how seamlessly n8n acts as the bridge: taking a message from Telegram, sending it to OpenAI via the API, receiving the AI-generated answer, and then passing it back to the user‚Äîall within your workflow. This transforms n8n into a true orchestrator of conversations, letting the Agent node manage message routing, AI logic, and response delivery in one automated loop. Here‚Äôs a high-level view: A user sends a message (text or voice) to your Telegram bot. n8n captures that message and, if needed, transcribes voice to text. n8n passes the user‚Äôs message to OpenAI via API (through the Agent node). OpenAI generates a reply ‚Äîas smart or as specific as you want, guided by your system prompt. The answer is returned to n8n , which then handles sending the reply straight back to your Telegram user‚Äîcompleting the loop. This lets you build real conversational AI bots with no code, using just nodes, flows, and your own creativity. üîπ The Core Idea AI everywhere: The Agent node lets you plug advanced LLMs (OpenAI, Claude, Grok, etc.) right into n8n. Not just text: My workflow also handles Telegram voice notes‚Äîthese get transcribed, then processed by the AI agent. Unified logic: Whether the user types or talks, the agent understands and replies‚Äîinstantly. üõ†Ô∏è How I Built It: Step-By-Step (Technical Outline) Here‚Äôs the practical structure so you can recreate (or adapt) it: Trigger (Telegram node): Set up a bot in Telegram and connect it to n8n. The trigger is ‚ÄúOn Message,‚Äù capturing every message (text/voice). Switch node (Type Check): Branch workflow: Is the message text or voice? Use a Switch node to check if message.text exists (text) or message.voice exists (voice). Text Path: If it‚Äôs text, pass the message content directly toward the Agent node. Voice Path: If it‚Äôs a voice note: Get File: Use Telegram‚Äôs ‚ÄúGet File‚Äù node to download the voice message using its File ID. Transcribe: Add the OpenAI ‚ÄúTranscribe Audio‚Äù node (Whisper) to convert voice to text. Output: You now have clean text, ready for the AI agent. Agent Node (The AI Core): Add the n8n Agent node after both paths (merge/join if needed). Select your model (e.g., OpenAI Chat). Configure a System Prompt to guide the AI agent‚Äôs tone/behavior (e.g., ‚ÄúYou are a helpful assistant. Answer every question clearly and professionally.‚Äù). Pass in the user message (original text or transcribed text). Behind the scenes: The Agent node sends the message to OpenAI‚Äôs API, gets the answer, and hands it back to your workflow. Reply (Telegram ‚ÄúSend Message‚Äù node): Take the output from the Agent node (AI reply). Send it back to the user in Telegram via their Chat ID. (Optional) : Log chats, add extra steps (e.g., Sheets, Notion), or expand the flow based on use case. No code needed‚Äîjust node configuration and logical connections! üí° What Makes This Special? Handles both text and voice in one clean flow. Supports multiple LLMs‚Äîswap OpenAI for Claude, Grok, Mistral, etc. System prompt makes it easy to customize your AI agent‚Äôs ‚Äúpersonality.‚Äù Reusable for other platforms: WhatsApp, Discord, web forms, and more. üöÄ What You Can Build 24/7 smart Telegram/WhatsApp bots. Voice-based Q&A or help desk agents. Multichannel support workflows, all using the same logic. üé• Full Video Tutorial Want the full step-by-step tutorial with screen sharing and live build ? https://www.youtube.com/watch?v=EYxBm42ja0k",
          "extracted_links": [
            "https://www.youtube.com/watch?v=EYxBm42ja0k"
          ],
          "link_count": 1
        }
      },
      {
        "json": {
          "keyword": "content creation workflow code included",
          "header": "Workflow - Code Not Included",
          "link": "https://old.reddit.com/r/n8n/comments/1loejcq/how_i_save_time_with_n8n_as_a_content_creator/",
          "author": "prometheus_one",
          "time": "2025-06-30T18:02:14+00:00",
          "body_text": "I was spending 4+ hours per video just reformatting the same content for different platforms. YouTube descriptions, Twitter threads, LinkedIn posts, Instagram captions - pure manual hell. So I built an AI workflow that does it all automatically. The magic: Paste transcript ‚Üí Hit \"start\" ‚Üí Get optimized content for 5+ platforms in 2 minutes. My N8N + Airtable + Gemini AI system generates everything: SEO titles, platform-specific captions, hashtag strategies, even creates organized Google Drive folders. Manual content creation in 2025 = digital manual labor. Work smarter, not harder. Want the complete workflow? I'm sharing the N8N JSON, Airtable templates, and setup guide for free: https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk/ Next i will build the content validation and autoposting with postiz. i found with ai you typically have to challenge the initial outcome to get a better result.",
          "extracted_links": [
            "https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk"
          ],
          "link_count": 1
        }
      }
    ],
    "Airtable Storage": [
      {
        "json": {
          "id": "recAHyPTPsxCQ9uyC",
          "createdTime": "2025-07-24T10:42:07.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "I run a daily AI Newsletter called The Recap and a huge chunk of work we do each day is scraping the web for interesting news stories happening in the AI space. In order to avoid spending hours scrolling, we decided to automate this process by building this scraping pipeline that can hook into Google News feeds, blog pages from AI companies, and almost any other \"feed\" you can find on the internet. Once we have the scraping results saved for the day, we load the markdown for each story into another automation that prompts against this data and helps us pick out the best stories for the day. Here's how it works 1. Trigger / Inputs The workflow is build with multiple scheduled triggers that run on varying intervals depending on the news source. For instance, we may only want to check  feed for Open AI's research blog every few hours while we want to trigger our check more frequently for the 2. Sourcing Data For every news source we want to integrate with, we setup a new feed for that source inside rss.app. Their platform makes it super easy to plug in a url like the blog page of a company's website or give it a url that has articles filtered on Google News. Once we have each of those sources configured in rss.app, we connect it to our scheduled trigger and make a simple HTTP request to the url rss.app gives us to get a list of news story urls back. 3. Scraping Data For each url that is passed in from the rss.app feed, we then make an API request to the the Firecrawl /scrape endpoint to get back the content of the news article formatted completely in markdown. Firecrawl's API allows you to specify a paramter called onlyMainContent but we found this didn't work great in our testing. We'd often get junk back in the final markdown like copy from the sidebar or extra call to action copy in the final result. In order to get around this, we opted to actually to use their LLM extract feature and passed in our own prompt to get the main content markdown we needed (prompt is included in the n8n workflow download). 4. Persisting Scraped Data Once the API request to Firecrawl is finished, we simply write that output to a .md file and push it into the Google Drive folder we have configured. Extending this workflow With this workflow + rss.app approach to sourcing news data, you can hook-in as many data feeds as you would like and run it through a central scraping node. I also think for production use-cases it would be a good idea to set a unique identifier on each news article scraped from the web so you can first check if it was already saved to Google Drive. If you have any overlap in news stories from your feed(s), you are going to end up getting re-scraping the same articles over and over. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=2uwV4aUyGIg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "Now Hiring or Looking for Cofounder",
            "link": "https://old.reddit.com/r/n8n/comments/1kzaysv/i_built_a_workflow_to_scrape_virtually_any_news/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-workflows/blob/main/ai_scraping_pipeline.json\", \"https://www.youtube.com/watch?v=2uwV4aUyGIg\"]"
          }
        }
      },
      {
        "json": {
          "id": "recXKeSBqe4EM6z3Y",
          "createdTime": "2025-07-24T10:37:57.000Z",
          "fields": {
            "Name": "hashpanak",
            "body_text": "I automated social posting to 6 different platforms. Why? I wanted to build authority on my socials in the AI space, and I was putting it off for a while simply because I haven't got the time or the energy to do social media. But now I have leverage. and don't have to sign up for expensive tools. Use it for: - inbound lead-gen - growing authority on socials - set up for your clients I explain the setup here -> https://youtu.be/D1iKOayS270 I automated everything ‚Äîfrom research to posting‚Äîusing n8n ,  here's the blueprint, also see the images. This system generates viral-ready content (of course you can customise the style in the prompt), lets you approve it from your phone, and posts across LinkedIn, Instagram, X, Threads, YouTube, and more. How It Works This workflow turns raw topics into polished posts in 4 stages: AI-Powered Research & Content Creation Topic Deep Dive : Input a keyword (e.g., ‚ÄúSEO‚Äù), and AI scours Google, extracts trending subtopics, and builds a research doc. (goes through people also search for and sub topics) Multi-Platform Content : Generates LinkedIn posts, Twitter threads, Instagram reels scripts, and YouTube shorts scripts‚Äîall hyper-tailored to each platform‚Äôs style. Give examples to train in your style in the Agent prompts) Dynamic Scripts depending on the research found : For reels/shorts, get ready-to-record scripts like: ‚ÄúI wish someone told me this SEO secret sooner. Google doesn‚Äôt care about your website. Here‚Äôs why‚Ä¶‚Äù On-the-Go Review via Telegram Review and approve from your phone : Get notified on Telegram to review drafts. Approve, request edits, or upload video links directly from your phone ( So you can be productive even if you are out and about ) . Automated Publishing & Scheduling Independent Schedules in one System : Set different schedules for different platforms, add more platforms because of the modular nature of the workflow.  Go live at optimal times (e.g., LinkedIn every Tuesday at 2 PM). Reels and Shorts (TikTok upload is manual) : Upload Instagram reels/YouTube shorts by pasting a Google Drive link‚ÄîThe agent downloads and posts the video to shorts and reels, TikTok is not yet done, (if you know how to do it, let me know, there does not seem to be an API) Cross-Platform Sync One System, All Apps : Posts auto-sync to LinkedIn, X, Facebook, Threads, YouTube, and Instagram. Key Features - No-Code Setup : Built entirely in n8n ‚Äîno coding skills needed. - Free Tools : Uses Serper (2.5k free searches), and platform APIs. - Video Ready : Downloads filmed videos from Google Drive and posts as Instagram reels, and Youtube shorts (This filming bit is manual  but the agent will send you the script to your phone, and if it is easy enough you can record it then - currently looking at hooking in my Faceless video automation into this as well - but that is work in progress) - Customizable : Tweak prompts, schedules, or platforms in minutes. Why This Works Save 10+ Hours/Week : Saves a ton of time in research, and coming up with content ideas, just tweak or it will give you a lot of drafts, a single topic can produce up to 200 + pieces of content. Scale Your Brand : Post daily on 6+ platforms without lifting a finger. PS : This setup replaces tools like Hootsuite ($50+/mo) or Later ($30+/mo) with a free AI-powered machine. üî• Turn your social media into a 24/7 inbound lead-gen engine. üî• I explain the setup here -> https://youtu.be/D1iKOayS270 publishing schedule The whole system for social media review system short form video review system topic research sample content generated for linked in. Of course the tone and everything else is customisable. Topic cold email outreach X/Twitter and Threads",
            "header": "Introducing My AI Automation Workflow with n8n for Social Media Posting!",
            "link": "https://old.reddit.com/r/n8n/comments/1jv9ikl/i_built_an_aipowered_social_media_machine_that/",
            "Extracted Links": "[\"https://youtu.be/D1iKOayS270\"]"
          }
        }
      },
      {
        "json": {
          "id": "recgT1LSRMv3WhzVQ",
          "createdTime": "2025-07-24T10:37:44.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "So I built an AI newsletter that isn‚Äôt written by me ‚Äî it‚Äôs completely written by an n8n workflow that I built. Each day, the system scrapes close to 100 AI news stories off the internet ‚Üí saves the stories in a data lake as markdown file ‚Üí and then runs those through this n8n workflow to generate a final newsletter that gets sent out to the subscribers. I‚Äôve been iterating on the main prompts used in this workflow over the past 5 months and have got it to the point where it is handling 95% of the process for writing each edition of the newsletter. It currently automatically handles: Scraping news stories sourced all over the internet from Twitter / Reddit / HackerNews / AI Blogs / Google News Feeds Loading all of those stories up and having an \"AI Editor\" pick the top 3-4 we want to feature in the newsletter Taking the source material and actually writing each core newsletter segment Writing all of the supplementary sections like the intro + a \"Shortlist\" section that includes other AI story links Formatting all of that output as markdown so it is easy to copy into Beehiiv and schedule with a few clicks What started as an interesting pet project AI newsletter now has several thousand subscribers and has an open rate above 20% Data Ingestion Workflow Breakdown This is the foundation of the newsletter system as I wanted complete control of where the stories are getting sourced from and need the content of each story in an easy to consume format like markdown so I can easily prompt against it. I wrote a bit more about this automation on this reddit post but will cover the key parts again here: The approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each one: This is done by calling into a scrape_url sub-workflow that I built out. This uses the Firecrawl API /scrape endpoint to scrape the contents of the news story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. Newsletter Generator Workflow Breakdown This workflow is the big one that actually loads up all scraped news content, picks the top stories, and writes the full newsletter. 1. Trigger / Inputs I use an n8n form trigger that simply let‚Äôs me pick the date I want to generate the newsletter for I can optionally pass in the previous day‚Äôs newsletter text content which gets loaded into the prompts I build to write the story so I can avoid duplicated stories on back to back days. 2. Loading Scraped News Stories from the Data Lake Once the workflow is started, the first two sections are going to load up all of the news stories that were scraped over the course of the day. I do this by: Running a simple search operation on our S3 bucket prefixed by the date like: 2025-06-10/ (gives me all stories scraped on June 10th) Filtering these results to only give me back the markdown files that end in an .md extension (needed because I am also scraping and saving the raw HTML as well) Finally read each of these files and load the text content of each file and format it nicely so I can include that text in each prompt to later generate the newsletter. 3. AI Editor Prompt With all of that text content in hand, I move on to the AI Editor section of the automation responsible for picking out the top 3-4 stories for the day relevant to the audience. This prompt is very specific to what I‚Äôm going for with this specific content, so if you want to build something similar you should expect a lot of trial and error to get this to do what you want to. It's pretty beefy. Once the top stories are selected, that selection is shared in a slack channel using a \"Human in the loop\" approach where it will wait for me to approve the selected stories or provide feedback. For example, I may disagree with the top selected story on that day and I can type out in plain english to \"Look for another story in the top spot, I don't like it for XYZ reason\". The workflow will either look for my approval or take my feedback into consideration and try selecting the top stories again before continuing on. 4. Subject Line Prompt Once the top stories are approved, the automation moves on to a very similar step for writing the subject line. It will give me its top selected option and 3-5 alternatives for me to review. Once again this get's shared to slack, and I can approve the selected subject line or tell it to use a different one in plain english. 5. Write ‚ÄúCore‚Äù Newsletter Segments Next up, I move on to the part of the automation that is responsible for writing the \"core\" content of the newsletter. There's quite a bit going on here: The action inside this section of the workflow is to split out each of the stop news stories from before and start looping over them. This allows me to write each section one by one instead of needing a prompt to one-shot the entire thing. In my testing, I found this to follow my instructions / constraints in the prompt much better. For each top story selected, I have a list of \"content identifiers\" attached to it which corresponds to a file stored in the S3 bucket. Before I start writing, I go back to our S3 bucket and download each of these markdown files so the system is only looking at and passing in the relevant context when it comes time to prompt. The number of tokens used on the API calls to LLMs get very big when passing in all news stories to a prompt so this should be as focused as possible. With all of this context in hand, I then make the LLM call and run a mega-prompt that is setup to generate a single core newsletter section. The core newsletter sections follow a very structured format so this was relatively easier to prompt against (compared to picking out the top stories). If that is not the case for you, you may need to get a bit creative to vary the structure / final output. This process repeats until I have a newsletter section written out for each of the top selected stories for the day. You may have also noticed there is a branch here that goes off and will conditionally try to scrape more URLs. We do this to try and scrape more ‚Äúprimary source‚Äù materials from any news story we have loaded into context. Say Open AI releases a new model and the story we scraped was from Tech Crunch. It‚Äôs unlikely that tech crunch is going to give me all details necessary to really write something really good about the new model so I look to see if there‚Äôs a url/link included on the scraped page back to the Open AI blog or some other announcement post. In short, I just want to get as many primary sources as possible here and build up better context for the main prompt that writes the newsletter section. 6. Final Touches (Final Nodes / Sections) I have a prompt to generate an intro section for the newsletter based off all of the previously generated content I then have a prompt to generate a newsletter section called \"The Shortlist\" which creates a list of other AI stories that were interesting but didn't quite make the cut for top selected stories Lastly, I take the output from all previous node, format it as markdown, and then post it into an internal slack channel so I can copy this final output and paste it into the Beehiiv editor and schedule to send for the next morning. Workflow Link + Other Resources Github workflow links: AI News Story / Data Ingestion Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json Firecrawl Scrape Url Sub-Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json AI Newsletter Generator Workflow: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Nv5_LU0q1IY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "From news headline to Instagram post in 2 minutes, all automated! (CODE INCLUDED)",
            "link": "https://old.reddit.com/r/n8n/comments/1l9pff8/i_built_an_ai_system_that_scrapes_stories_off_the/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_news_data_ingestion.json\", \"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/firecrawl_scrape_url.json\", \"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/ai_newsletter_generator.json\", \"https://www.youtube.com/watch?v=Nv5_LU0q1IY\"]"
          }
        }
      },
      {
        "json": {
          "id": "recYFSH2fF9YT54fG",
          "createdTime": "2025-07-24T10:37:44.000Z",
          "fields": {
            "Name": "Altruistic-Brother37",
            "body_text": "I've created and given up on multiple website and revenue potentials. Creating a site on WordPress is easy and fun, but writing the content is boring and tiresome. Since I don‚Äôt have a team of writers or editors to help churn out daily blog posts, I built an n8n automation system that posts 10 content a day. It scrapes fresh topics, generates long form blog posts from RSS feeds, repurpose them using AI, and publishes directly to WordPress. Here‚Äôs how the system works: 1. Daily Trigger & Topic Ideation The workflow starts with a daily schedule trigger. It pulls fresh news content from multiple sources (mostly big tech news sites) and uses embeddings to group them into potential topics. For example, if multiple articles are trending around ‚ÄúAI music tools,‚Äù the workflow merges those into one blog worthy topic. 2. AI Powered Blog Generation Once the topic is selected, the workflow sends a custom GPT prompt to generate a 1000 - 1500 word blog post. It uses summary + original content merging, plus markdown formatting for readability. You can configure tone, length, call to actions, or SEO keywords depending on your niche. 3. WordPress Publishing The final content is cleaned, formatted, and then sent to WordPress via the API. It handles: Categories and tags Featured image (auto generated using Leonardo AI) Author and post status Slug and permalink structure This means new blog posts go live without you touching a thing. Costs The workflow runs entirely on free or open source tools. . Your only costs would be Open AI API calls + your WordPress server. Component Daily Usage Monthly Cost Estimate OpenAI 10 posts per day ~$6 Leonardo AI 10 images per day (15 credits each) ~$14 (9 base + 5 top-up) MongoDB Free up to 512 MB $0 WordPress Hosting and domain ~$1.25 Total ~$21/month Alternatively, you could use open / cheaper models via local API or and image generation via other models on Hugging Face Limitations I noticed that sometimes the word count is below 1,500. Usually around 700 to 800 words. Its not bad, but not ideal I've received a few feedback from people that sometimes the source RSS websites change the slug URL of their site and it causes errors Further Improvement (I'm working on) I want this flow to be able to generate images and tables within the blog Will be cool to repurpose the content for other sites like Twritter and Linkedin I think the current setup is too linear, passing information from one AI agent to another. Perhaps, a flow where there are 3 agents in a loop. The task master who creates the task, sections and reading journey The content writer who writes the content, drafts the tables, generates the images The Quality Checker who validates the SEO score and overall consistency. Provides feedback and passes the article if it meets certain quality thresholds Currently, it only works for one category per workflow. Right now, i hard coded AI, but you can change it to whatever you want. I notice that most blogs use 3 to 5 content pillars. Will add more topics so readers dont get bored GitHub repo with the full n8n workflow code: https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress Hopefully its useful and inspire you to light up that WP blog u abandoned.",
            "header": "Workflow - Code Included",
            "link": "https://old.reddit.com/r/n8n/comments/1m5qnxn/i_built_a_content_generator_ai_workflow_using_n8n/",
            "Extracted Links": "[\"https://github.com/Jharilela/n8n-workflows/tree/main/AI-Powered%20Blog%20Automation%20for%20WordPress\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec5rf0sE3DfGDfzu",
          "createdTime": "2025-07-24T10:37:45.000Z",
          "fields": {
            "Name": "conor_is_my_name",
            "body_text": "Hello Everyone! Today I'm sharing my latest n8n tool - a very performant dockerized version of the crawlee web scraping package. https://github.com/conor-is-my-name/crawlee-server Who is this for: Want to scrape every page on a website customize the fields & objects that you scrape you already have a database setup - default is postgres Scaled scraping - can run multiple containers for parallelism Who this is not for: you don't have a database - the scraper is too fast to return results to google sheets or n8n I've used this to scrape millions of web pages, and this setup is the baseline that I use for my competitor analysis and content generation work.  This template is all you need to get good at web scraping.  If you can learn how to modify the selectors in the code of this package, you can scrape 99% of websites. Simply run this docker container & update the IP address and Port number in the workflow - example n8n http node is already included. http://100.XX.XX.XX:####/start-crawl?url=https://paulgraham.com&maxResults=10 Parameters to pass from n8n: url & max results (don't pass max results if you want full site scraped) The baseline code that I'm sharing is configured as a generic web scraper most suitable for blogs and news articles.  You can modify what you want returned in the results.js file. sitehomepage, article_url, title, bodyText, datePublished, \narticlecategories, tags, keywords, author, featuredImage, comments I have also included an example for scraping a e-commerce site that runs on Woo Commerce in the n8n-nodes folder.  You can use that as a template to adjust to just about any site by changing the selectors used in the routes.js file. If you don't know how to do this, I highly recommend using Roo Code in VS Code.  It's as simple as copying the HTML from the page and asking Roo Code to pick the specific selectors you want. It will make the adjustments in the routes.js file for you.  But note that you will have to make sure your database also has all of the matching fields you want scraped. Example SQL is also included for initial database setup.  I recommend using this in conjunction with my n8n-autoscaling build which already comes with postgres installed. Instructions: Clone the repository Update passwords in the .env file to match your setup docker compose up -d update the IP address and port number in the n8n workflow to match the running containers Optional: The docker compose file has a Deploy section that comes commented out by default.  If you want to run multiple instances of this container you can make your adjustments here. You can modify scraper concurrency in the .env file.  I'd advise you to stay in the 3-5 range unless you know the site doesn't have rate limiting. As always, be sure to check out my other n8n specific GitHub repositories: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM I do expert n8n consulting, send me a message if you need help on a project.",
            "header": "I Built an AI-Powered Social Media Machine That Writes, Reviews, & Posts Content 24/7 ‚Äî Steal My Workflow",
            "link": "https://old.reddit.com/r/n8n/comments/1l1i5mp/i_made_a_crawlee_server_built_specifically_for/",
            "Extracted Links": "[\"https://github.com/conor-is-my-name/crawlee-server\", \"https://github.com/conor-is-my-name/n8n-autoscaling\", \"https://github.com/conor-is-my-name/google-maps-scraper\", \"https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\", \"https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer-ARM\"]"
          }
        }
      },
      {
        "json": {
          "id": "recyhT5rgNibDhjv4",
          "createdTime": "2025-07-24T10:42:11.000Z",
          "fields": {
            "Name": "piothur",
            "body_text": "Hey everyone! üëã\n\nI've been working on a FREE project that solves a common challenge many of us face with n8n: tracking long-running and asynchronous tasks. I'm excited to share the n8n Task Manager - a complete orchestration solution built entirely with n8n workflows!\n\nüéØ What Problem Does It Solve?\n\nIf you've ever needed to:\n- Track ML model training jobs that take hours\n- Monitor video rendering or time consuming processing tasks\n- Manage API calls to services that work asynchronously (Kling, ElevenLabs, etc.)\n- Keep tabs on data pipeline executions\n- Handle webhook callbacks from external services\n\nThen this Task Manager is for you!\n\nüöÄ Key Features:\n\n- 100% n8n workflows - No external code needed\n- Automatic polling - Checks task status every 2 minutes\n- Real-time monitoring - React frontend with live updates\n- Database backed - Uses Supabase (free tier works!)\n- Slack alerts - Get notified when tasks fail\n- API endpoints - Create, update, and query tasks via webhooks\n- Batch processing - Handles multiple tasks efficiently\n\nüì¶ What You Get:\n\n1. 4 Core n8n Workflows:\n¬† ¬†- Task Creation (POST webhook)\n¬† ¬†- Task Monitor (Scheduled polling)\n¬† ¬†- Status Query (GET endpoint)\n¬† ¬†- Task Update (Callback handler)\n\n2. React Monitoring Dashboard:\n¬† ¬†- Real-time task status\n¬† ¬†- Media preview (images, videos, audio)\n¬† ¬†- Running time tracking\n\n3. 5 Demo Workflows - Complete AI creative automation:\n¬† ¬†- OpenAI image generation\n¬† ¬†- Kling video animation\n¬† ¬†- ElevenLabs text-to-speech\n¬† ¬†- FAL Tavus lipsync\n¬† ¬†- Full orchestration example\n\nüõ†Ô∏è How to Get Started:\n\n1. Clone the repo: https://github.com/lvalics/Task_Manager_N8N\n2. Set up Supabase (5 minutes, free account)\n3. Import n8n workflows (drag & drop JSON files)\n4. Configure credentials (Supabase connection)\n5. Start tracking tasks!\n\nüí° Real-World Use Cases:\n\n- AI Content Pipeline: Generate image ‚Üí animate ‚Üí add voice ‚Üí create lipsync\n- Data Processing: Track ETL jobs, report generation, batch processing\n- Media Processing: Monitor video encoding, image optimization, audio transcription\n- API Orchestration: Manage multi-step API workflows with different services\n\nüì∫ See It In Action:\n\nI've created a full tutorial video showing the system in action: [\\[YouTube Link\\]](\nhttps://www.youtube.com/watch?v=PckWZW2fhwQ\n)\n\nü§ù Contributing:\n\nThis is open source! I'd love to see:\n- New task type implementations\n- Additional monitoring features\n- Integration examples\n- Bug reports and improvements\n\nGitHub: https://github.com/lvalics/Task_Manager_N8N\n\nüôè Feedback Welcome!\n\nI built this to solve my own problems with async task management, but I'm sure many of you have similar challenges. What features would you like to see? How are you currently handling long-running tasks in n8n?\n\nDrop a comment here or open an issue on GitHub. Let's make n8n task management better together!",
            "header": "Workflow - Code Not Included",
            "link": "https://old.reddit.com/r/n8n/comments/1lrlbdi/i_automated_my_content_creation_from_client/",
            "Extracted Links": "[\"https://github.com/lvalics/Task_Manager_N8N\", \"https://www.youtube.com/watch?v=PckWZW2fhwQ\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec7yaNKZdQtfpPc6",
          "createdTime": "2025-07-24T10:37:47.000Z",
          "fields": {
            "Name": "Silly_Chapter6498",
            "body_text": "I wanted to share a workflow I've been refining. I was tired of manually finding content for a niche site I'm running, so I built a bot with N8N to do it for me. It automatically fetches news articles on a specific topic and posts them to my Ghost blog. The end result is a site that stays fresh with relevant content on autopilot. Figured some of you might find this useful for your own projects. Here's the stack: Data Source: LumenFeed API (Full disclosure, this is my project. The free tier gives 10k requests/month which is plenty for this). Automation: N8N (self-hosted) De-duplication: Redis (to make sure I don't post the same article twice) CMS: Ghost (but works with WordPress or any CMS with an API) The Step-by-Step Workflow: Here‚Äôs the basic logic, node by node. (1) Setup the API Key: First, grab a free API key from LumenFeed. In N8N, create a new \"Header Auth\" credential. Name: X-API-Key Value: [Your_LumenFeed_API_Key] (2) HTTP Request Node (Get the News): This node calls the API. URL: https://client.postgoo.com/api/v1/articles Authentication: Use the Header Auth credential you just made. Query Parameters: This is where you define what you want. For example, to get 10 articles with \"crypto\" in the title: q:¬†crypto query_by:¬†title language:¬†en per_page:¬†10 (3) Code Node (Clean up the Data): The API returns articles in a¬†data¬†array. This simple JS snippet pulls that array out for easier handling. return $node[\"HTTP Request\"].json[\"data\"]; (4) Redis \"Get\" Node (Check for Duplicates): Before we do anything else, we check if we've seen this article's URL before. Operation: Get Key: {{ $json.source_link }} (5) IF Node (Is it a New Article?): This node checks the output of the Redis node. If the¬†value¬†is empty, it's a new article and we continue. If not, we stop. Condition: {{ $node[\"Redis\"].json.value }}¬†->¬†Is Empty (6) Publishing to Ghost/WordPress: If the article is new, we send it to our CMS. In your Ghost/WordPress node, you map the fields: Title: {{ $json.title }} Content: {{ $json.content_excerpt }} Featured Image: {{ $json.image_url }} (7) Redis \"Set\" Node (Save the New Article): This is the final step for each¬†new¬†article. We add its URL to Redis so it won't get processed again. Operation: Set Key: {{ $json.source_link }} Value: true That's the core of it! You just set the Schedule Trigger to run every few hours and you're good to go. Happy to answer any questions about the setup in the comments! For those who prefer video or a more detailed write-up with all the screenshots: Full Video Guide: https://youtu.be/BNkxt48plFg Detailed Blog Post: https://medium.com/@contact_34329/how-to-build-an-automated-news-aggregator-with-n8n-and-lumenfeed-api-step-by-step-guide-c63b61d4c0ce",
            "header": "Automated Shorts and Reels Creation with N8N ‚Äì Full Workflow & Video Guide!",
            "link": "https://old.reddit.com/r/n8n/comments/1l0k5dh/opensource_task_manager_for_n8n_track_longrunning/",
            "Extracted Links": "[\"https://youtu.be/BNkxt48plFg\"]"
          }
        }
      },
      {
        "json": {
          "id": "rech1Tqm0pzamaUj7",
          "createdTime": "2025-07-24T10:42:12.000Z",
          "fields": {
            "Name": "hashpanak",
            "body_text": "I automated keyword research and clustering for SEO and content planning. Why? I was tired of spending hours manually digging through keywords and organizing them into clusters, only to end up with a chaotic mess. Now, I‚Äôve got a system that does it all‚Äîresearches keywords, builds clusters, and prioritizes opportunities‚Äîusing n8n and AI. It‚Äôs a game-changer for anyone looking to streamline their SEO strategy without breaking the bank. Use it for: Creating topic clusters and using it as a lead magnet for acquiring new clients. Using generated pillars and pages for social media content ideas from what people are actually searching for. Automating keyword research and clustering Saving time while uncovering high-impact topics I explain the setup here -> https://youtu.be/twFg_L9_07E I automated everything‚Äîfrom keyword discovery to clustering‚Äîusing n8n. Here‚Äôs the blueprint. This system pulls keyword data, organizes it into pillar pages and subpages, and even stores it neatly in Google Sheets, all while running on free or low-cost tools. How It Works This workflow turns a single seed keyword into a full SEO strategy in 4 stages: Automated Keyword Research (with human in the loop) Seed Keyword Start: Drop in a keyword (e.g., ‚Äún8n‚Äù or ‚ÄúSEO‚Äù), and the system uses the Google Ads API (or Data for SEO) to fetch related keywords, search volumes, competition, and trends. Replace it with other APIs if you so wish like \"DataForSEO\" Iterative Discovery: It keeps digging, researching related keywords to build a massive, relevant list‚Äîthink hundreds of terms in minutes. AI-Powered Clustering Smart Grouping: An AI model (like GPT-4.1 or Gemini) analyzes the keywords based on user intent (informational, transactional, navigational), search trends, and competition. Pillar & Subpage Structure: It creates thematic clusters‚Äîpillar pages for broad topics and subpages for specifics‚Äîthen ranks them by growth potential and ranking ease. Google Sheets Integration Central Hub: All keywords and clusters land in Google Sheets, ready for you to review or tweak. Manual Control: Spot a dud keyword? Delete it. Need to adjust a cluster? It‚Äôs all right there. n8n Automation No-Code Workflow: Built in n8n, this system runs on autopilot with scheduled triggers or manual kicks. Scalable Processing: Looping mechanisms handle huge keyword sets without breaking a sweat. Key Features No-Code Setup: Built entirely in n8n‚Äîno coding skills needed. Free/Low-Cost Tools: Uses Google Ads API (free tier) or Data for SEO ($1 signup credit and cheap alternative to Ahrefs or SEMRush). Scalable: Tackles thousands of keywords with batch processing. Customizable: Tweak AI prompts or schedules to match your needs. Why This Works Save 10+ Hours/Week: No more manual keyword grinding‚Äîautomation does the heavy lifting. Smarter SEO: AI prioritizes clusters with real data, targeting topics that rank and convert. Scale Effortlessly: Perfect for big content strategies without the overwhelm. Example: Feed it ‚Äún8n,‚Äù and it pulls keywords like ‚Äún8n automation,‚Äù ‚Äún8n workflows,‚Äù and ‚Äún8n integrations.‚Äù The AI then builds clusters like ‚Äún8n Automation Tutorials‚Äù (pillar) with subpages like ‚ÄúConnecting n8n to Salesforce‚Äù‚Äîcomplete with primary/secondary keywords and ranking priorities. PS: This replaces tools like Ahrefs ($99+/mo) or SEMrush ($129+/mo) with a free AI-powered system. üî• Turn your SEO strategy into a 24/7 content-planning engine. üî• I explain the setup here -> https://youtu.be/twFg_L9_07E https://preview.redd.it/bxjl5j430qxe1.png?width=1704&format=png&auto=webp&s=53e7bc67c1f76f46a3fbdc76b2acdef1bbc4ac51 https://preview.redd.it/xr50mgn40qxe1.png?width=2344&format=png&auto=webp&s=04768c89f7bc0947eac2fe1a1976c30634273a38 https://preview.redd.it/sobyfvw50qxe1.png?width=2716&format=png&auto=webp&s=bb7994483791ff33a21ec3e44aa766e2d6560388 https://preview.redd.it/r8zawm170qxe1.png?width=2886&format=png&auto=webp&s=a3f5803b70120d2c34de4d8547b264a2e84ca1ae https://reddit.com/link/1kahpiu/video/zz4xrkxg0qxe1/player",
            "header": "I Built an AI-Powered Keyword Research and Clustering Machine That Automates SEO Content Strategy ‚Äî Steal My Workflow",
            "link": "https://old.reddit.com/r/n8n/comments/1kahpiu/i_built_an_aipowered_keyword_research_and/",
            "Extracted Links": "[\"https://youtu.be/twFg_L9_07E\"]"
          }
        }
      },
      {
        "json": {
          "id": "recLCsv3z8iODqXFM",
          "createdTime": "2025-07-24T10:37:49.000Z",
          "fields": {
            "Name": "teezworkspace",
            "body_text": "Hey everyone, I built out this workflow in n8n to help me intake the highest quality AI content in the most digestible format for myself; audio. In short, the RSS Feed scrapes three (could be more if you want) of the most reputable sources in the AI space, goes through a Code node for scoring (looks for the highest quality content: whitepapers, research papers, etc) and calls AutoContentAPI (NOT free, but a NotebookLM alternative nonetheless) via HTTP Request and generates podcasts on the respective material and sends it to me via Telegram and Gmail, and updates my Google Drive as well. Provided below is a screenshot and the downloadable JSON in case anyone would like to try it. Feel free to DM me if you have any questions. I'm also not too familiar with how to share files on Reddit so the option I settled on was placing the JSON in this code block, hopefully that works? Again, feel free to DM me if you'd like to try it and I should be able to share it to you directly as downloadable JSON for you to import into n8n. {\n  \"name\": \"AI Podcast Generation (AutoContentAPI)\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"triggerTimes\": {\n          \"item\": [\n            {}\n          ]\n        }\n      },\n      \"name\": \"Schedule: Weekly Learning Run\",\n      \"type\": \"n8n-nodes-base.cron\",\n      \"typeVersion\": 1,\n      \"position\": [\n        -1820,\n        -200\n      ],\n      \"id\": \"7a78b92e-d75b-4cab-bf0c-6a9fd41c5683\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.url }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.rssFeedRead\",\n      \"typeVersion\": 1.1,\n      \"position\": [\n        -920,\n        -180\n      ],\n      \"id\": \"2a012472-2e03-451c-80d7-202d159c3959\",\n      \"name\": \"RSS Read\",\n      \"onError\": \"continueRegularOutput\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"return [\\n  { json: { url: \\\"https://huggingface.co/blog/feed\\\" } },\\n  { json: { url: \\\"https://machinelearningmastery.com/blog/feed/\\\" } },\\n  { json: { url: \\\"https://blog.tensorflow.org/feeds/posts/default\\\" } }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -1620,\n        -200\n      ],\n      \"id\": \"758b3629-43b5-4330-a1a0-2c1aabdfdf1e\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"const keywords = [\\n  \\\"whitepaper\\\", \\\"research\\\", \\\"study\\\", \\\"publication\\\", \\\"paper\\\", \\\"preprint\\\", \\\"abstract\\\",\\n  \\\"benchmark\\\", \\\"evaluation\\\", \\\"methodology\\\", \\\"experiment\\\", \\\"analysis\\\", \\\"dataset\\\",\\n  \\\"LLM\\\", \\\"GPT\\\", \\\"transformer\\\", \\\"language model\\\", \\\"fine-tuning\\\", \\\"pretraining\\\"\\n];\\n\\nconst now = new Date();\\nconst weekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000);\\nconst monthStart = new Date(now.getFullYear(), now.getMonth(), 1);\\nconst seenLinks = new Set();\\n\\n// Domains not supported by AutoContentAPI on free tier\\nconst blockedDomains = [\\n  \\\"arxiv.org\\\",\\n  \\\"ieeexplore.ieee.org\\\",\\n  \\\"springer.com\\\",\\n  \\\"sciencedirect.com\\\",\\n  \\\"dl.acm.org\\\"\\n];\\n\\n// Score and parse\\nlet scored = items.map(item => {\\n  const title = (item.json.title || \\\"\\\").toLowerCase();\\n  const description = (item.json.description || item.json.contentSnippet || item.json.content || \\\"\\\").toLowerCase();\\n  const link = item.json.link || item.json.url || \\\"\\\";\\n  const pubDateStr = item.json.pubDate || item.json.date || item.json.isoDate || \\\"\\\";\\n  const pubDate = pubDateStr && !isNaN(Date.parse(pubDateStr)) ? new Date(pubDateStr) : null;\\n\\n  let score = 0;\\n  keywords.forEach(keyword => {\\n    if (title.includes(keyword)) score += 2;\\n    if (description.includes(keyword)) score += 1;\\n  });\\n\\n  return {\\n    json: {\\n      title: item.json.title,\\n      link,\\n      pubDate: pubDateStr,\\n      pubDateObject: pubDate,\\n      content: item.json.content || item.json.contentSnippet || \\\"\\\",\\n      score\\n    }\\n  };\\n});\\n\\n// Filter: only allow whitelisted, non-duplicate, recent items\\nlet filtered = scored.filter(item =>\\n  item.json.score >= 2 &&\\n  item.json.pubDateObject instanceof Date &&\\n  !isNaN(item.json.pubDateObject) &&\\n  item.json.link &&\\n  !seenLinks.has(item.json.link) &&\\n  !blockedDomains.some(domain => item.json.link.includes(domain)) &&\\n  seenLinks.add(item.json.link)\\n);\\n\\n// Prioritize items from the last 7 days\\nlet pastWeek = filtered.filter(item => item.json.pubDateObject >= weekAgo);\\n\\n// If none found, fall back to items from this calendar month\\nif (pastWeek.length === 0) {\\n  pastWeek = filtered.filter(item =>\\n    item.json.pubDateObject >= monthStart && item.json.pubDateObject <= now\\n  );\\n}\\n\\n// Sort by score descending\\npastWeek.sort((a, b) => b.json.score - a.json.score);\\n\\n// Return top 3\\nreturn pastWeek.slice(0, 3);\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        -700,\n        -180\n      ],\n      \"id\": \"3ffafffd-f20a-4197-a09c-b08dca6099a6\",\n      \"name\": \"Whitepaper Filter\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"0e2fb51a-8995-4b8d-bb41-ea78cf5c1904\",\n              \"name\": \"url\",\n              \"value\": \"={{ $json.url }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -1120,\n        -180\n      ],\n      \"id\": \"d0115844-b5fb-489c-83fe-4d2fbd11b7b9\",\n      \"name\": \"Edit Fields\"\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"ca3acbb3-9375-4335-b8b2-a951e72dff76\",\n              \"name\": \"request_id\",\n              \"value\": \"={{ $json.request_id }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        120,\n        -160\n      ],\n      \"id\": \"06ef9efc-88b3-470a-b7dd-b615e7700d09\",\n      \"name\": \"Extract Request ID\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"=https://api.autocontentapi.com/content/status/{{$json[\\\"request_id\\\"]}}\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer 5b62e1aa-54d0-4319-81e8-93320d9a58ef\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        320,\n        -160\n      ],\n      \"id\": \"50db4ed9-e412-48bd-b41f-1a764be41c74\",\n      \"name\": \"GET Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://api.autocontentapi.com/Content/Create\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Authorization\",\n              \"value\": \"Bearer YOUR<API>KEY\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"contentType\": \"raw\",\n        \"rawContentType\": \"application/json\",\n        \"body\": \"={{ \\n  JSON.stringify({\\n    resources: [\\n      {\\n        content: $json[\\\"link\\\"],\\n        type: \\\"website\\\"\\n      }\\n    ],\\n    text: \\\"Create a podcast summary of this article in a conversational, engaging tone.\\\",\\n    outputType: \\\"audio\\\"\\n  })\\n}}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -140,\n        -160\n      ],\n      \"id\": \"8ae2fffa-03ab-4053-9db0-388de34b5287\",\n      \"name\": \"Generate Podcasts\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"9f869aa6-11f0-4664-8d16-d06a6ec52c9f\",\n              \"leftValue\": \"={{ $json.status }}\",\n              \"rightValue\": 100,\n              \"operator\": {\n                \"type\": \"number\",\n                \"operation\": \"equals\"\n              }\n            }\n          ],\n          \"combinator\": \"or\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        520,\n        -160\n      ],\n      \"id\": \"2785e08c-f859-4fa2-b752-9f114e6617bc\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"teezworkspace@gmail.com\",\n        \"subject\": \"={{ $json.audio_title }}\",\n        \"message\": \"={{ $json.audio_title }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"attachmentsUi\": {\n            \"attachmentsBinary\": [\n              {\n                \"property\": \"audio\"\n              }\n            ]\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        1080,\n        80\n      ],\n      \"id\": \"f07b9a91-aa2d-43a9-9095-41497180454f\",\n      \"name\": \"Send Audio to Email\",\n      \"webhookId\": \"0ff65219-e34a-4ad4-b600-f7238569c92d\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"\",\n          \"name\": \"Terry's Gmail\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"inputDataFieldName\": \"audio\",\n        \"name\": \"={{ $json.audio_title }}\",\n        \"driveId\": {\n          \"__rl\": true,\n          \"value\": \"My Drive\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"My Drive\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/my-drive\"\n        },\n        \"folderId\": {\n          \"__rl\": true,\n          \"value\": \"1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Weekly AI Research Audio\",\n          \"cachedResultUrl\": \"https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleDrive\",\n      \"typeVersion\": 3,\n      \"position\": [\n        1080,\n        -120\n      ],\n      \"id\": \"5d9eec4c-f596-48f0-a81e-5f1bc37a082b\",\n      \"name\": \"Upload Audio Folder\",\n      \"credentials\": {\n        \"googleDriveOAuth2Api\": {\n          \"id\": \"\",\n          \"name\": \"Terry Google Drive\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"sendAudio\",\n        \"chatId\": \"6018770135\",\n        \"binaryData\": true,\n        \"binaryPropertyName\": \"audio\",\n        \"additionalFields\": {\n          \"caption\": \"={{ $json.audio_title }}\",\n          \"title\": \"={{ $json.audio_title }}\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.telegram\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        1080,\n        -340\n      ],\n      \"id\": \"6f21e927-a79b-48f3-a5ff-8dd9d460916f\",\n      \"name\": \"Send Audio to Telegram\",\n      \"webhookId\": \"97f48ead-3e73-4928-a555-455722196acc\",\n      \"credentials\": {\n        \"telegramApi\": {\n          \"id\": \"\",\n          \"name\": \"AutoContentAPI Bot \"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"batchSize\": 15,\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -1380,\n        -200\n      ],\n      \"id\": \"fb9a4a7c-2aba-4a17-89e4-6e856bd23d0a\",\n      \"name\": \"URL Loop\"\n    },\n    {\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitInBatches\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -480,\n        -180\n      ],\n      \"id\": \"9ce3486f-0bd6-45fa-bdcc-392c72bfff97\",\n      \"name\": \"Podcast Gen Loop\"\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.audio_url }}\",\n        \"options\": {\n          \"response\": {\n            \"response\": {\n              \"responseFormat\": \"file\",\n              \"outputPropertyName\": \"audio\"\n            }\n          }\n        }\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        760,\n        -180\n      ],\n      \"id\": \"0afdf799-a612-4a07-a2e5-c65b262ef12e\",\n      \"name\": \"Download Audio\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Schedule: Weekly Learning Run\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"RSS Read\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Whitepaper Filter\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"URL Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Whitepaper Filter\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Edit Fields\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"RSS Read\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Request ID\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"GET Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Generate Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Podcast Gen Loop\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Extract Request ID\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"GET Podcasts\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Download Audio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Upload Audio Folder\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"URL Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Edit Fields\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Podcast Gen Loop\": {\n      \"main\": [\n        [],\n        [\n          {\n            \"node\": \"Generate Podcasts\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Download Audio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Audio to Telegram\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Upload Audio Folder\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Send Audio to Email\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": false,\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"57ddc431-4059-4b0e-92dc-325c7296ac9a\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"f9bd58af1591f515777c160d7518c3e5cf0ad788d4a4c3831380e58e9febdfa6\"\n  },\n  \"id\": \"Ece8XCZeyPq6R0Uv\",\n  \"tags\": []\n} https://preview.redd.it/7nryas97v84f1.png?width=2694&format=png&auto=webp&s=1bb4c966619c7cfd520ab6bc47c1dbbd439871e6",
            "header": "Tutorial",
            "link": "https://old.reddit.com/r/n8n/comments/1l0gm98/try_this_podcast_generation_workflow_i_built/",
            "Extracted Links": "[\"https://drive.google.com/drive/folders/1VmAvExINuE6I-xYZnpBnlS5bX1RRPdGL\"]"
          }
        }
      },
      {
        "json": {
          "id": "recjjUVYTcm8WIbGK",
          "createdTime": "2025-07-24T10:42:13.000Z",
          "fields": {
            "Name": "PostGoo",
            "body_text": "I run an Instagram account that publishes short form videos each week that cover the top AI news stories. I used to monitor twitter to write these scripts by hand, but it ended up becoming a huge bottleneck and limited the number of videos that could go out each week. In order to solve this, I decided to automate this entire process by building a system that scrapes the top AI news stories off the internet each day (from Twitter / Reddit / Hackernews / other sources), saves it in our data lake, loads up that text content to pick out the top stories and write video scripts for each. This has saved a ton of manual work having to monitor news sources all day and let‚Äôs me plug the script into ElevenLabs / HeyGen to produce the audio + avatar portion of each video. One of the recent videos we made this way got over 1.8 million views on Instagram and I‚Äôm confident there will be more hits in the future. It‚Äôs pretty random on what will go viral or not, so my plan is to take enough ‚Äúshots on goal‚Äù and continue tuning this prompt to increase my changes of making each video go viral. Here‚Äôs the workflow breakdown 1. Data Ingestion and AI News Scraping The first part of this system is actually in a separate workflow I have setup and running in the background. I actually made another reddit post that covers this in detail so I‚Äôd suggestion you check that out for the full breakdown + how to set it up. I‚Äôll still touch the highlights on how it works here: The main approach I took here involves creating a \"feed\" using RSS.app for every single news source I want to pull stories from (Twitter / Reddit / HackerNews / AI Blogs / Google News Feed / etc). Each feed I create gives an endpoint I can simply make an HTTP request to get a list of every post / content piece that rss.app was able to extract. With enough feeds configured, I‚Äôm confident that I‚Äôm able to detect every major story in the AI / Tech space for the day. Right now, there are around ~13 news sources that I have setup to pull stories from every single day. After a feed is created in rss.app, I wire it up to the n8n workflow on a Scheduled Trigger that runs every few hours to get the latest batch of news stories. Once a new story is detected from that feed, I take that list of urls given back to me and start the process of scraping each  story and returns its text content back in markdown format Finally, I take the markdown content that was scraped for each story and save it into an S3 bucket so I can later query and use this data when it is time to build the prompts that write the newsletter. So by the end any given day with these scheduled triggers running across a dozen different feeds, I end up scraping close to 100 different AI news stories that get saved in an easy to use format that I will later prompt against. 2. Loading up and formatting the scraped news stories Once the data lake / news storage has plenty of scraped stories saved for the day, we are able to get into the main part of this automation. This kicks off off with a scheduled trigger that runs at 7pm each day and will: Search S3 bucket for all markdown files and tweets that were scraped for the day by using a prefix filter Download and extract text content from each markdown file Bundle everything into clean text blocks wrapped in XML tags for better LLM processing - This allows us to include important metadata with each story like the source it came from, links found on the page, and include engagement stats (for tweets). 3. Picking out the top stories Once everything is loaded and transformed into text, the automation moves on to executing a prompt that is responsible for picking out the top 3-5 stories suitable for an audience of AI enthusiasts and builder‚Äôs. The prompt is pretty big here and highly customized for my use case so you will need to make changes for this if you are going forward with implementing the automation itself. At a high level, this prompt will: Setup the main objective Provides a ‚Äúcuration framework‚Äù to follow over the list of news stories that we are passing int Outlines a process to follow while evaluating the stories Details the structured output format we are expecting in order to avoid getting bad data back ```jsx\n<objective>\nAnalyze the provided daily digest of AI news and select the top 3-5 stories most suitable for short-form video content. Your primary goal is to maximize audience engagement (likes, comments, shares, saves). The date for today's curation is {{ new Date(new Date($('schedule_trigger').item.json.timestamp).getTime() + (12 * 60 * 60 * 1000)).format(\"yyyy-MM-dd\", \"America/Chicago\") }} . Use this to prioritize the most recent and relevant news. You MUST avoid selecting stories that are more than 1 day in the past for this date.\n</objective> <curation_framework>\nTo identify winning stories, apply the following virality principles. A story must have a strong \"hook\" and fit into one of these categories: Impactful: A major breakthrough, industry-shifting event, or a significant new model release (e.g., \"OpenAI releases GPT-5,\" \"Google achieves AGI\"). Practical: A new tool, technique, or application that the audience can use now (e.g., \"This new AI removes backgrounds from video for free\"). Provocative: A story that sparks debate, covers industry drama, or explores an ethical controversy (e.g., \"AI art wins state fair, artists outraged\"). Astonishing: A \"wow-factor\" demonstration that is highly visual and easily understood (e.g., \"Watch this robot solve a Rubik's Cube in 0.5 seconds\"). Hard Filters (Ignore stories that are): * Ad-driven: Primarily promoting a paid course, webinar, or subscription service.\n* Purely Political: Lacks a strong, central AI or tech component.\n* Substanceless: Merely amusing without a deeper point or technological significance.\n</curation_framework> <hook_angle_framework>\nFor each selected story, create 2-3 compelling hook angles that could open a TikTok or Instagram Reel. Each hook should be designed to stop the scroll and immediately capture attention. Use these proven hook types: Hook Types: - Question Hook: Start with an intriguing question that makes viewers want to know the answer\n- Shock/Surprise Hook: Lead with the most surprising or counterintuitive element\n- Problem/Solution Hook: Present a common problem, then reveal the AI solution\n- Before/After Hook: Show the transformation or comparison\n- Breaking News Hook: Emphasize urgency and newsworthiness\n- Challenge/Test Hook: Position as something to try or challenge viewers\n- Conspiracy/Secret Hook: Frame as insider knowledge or hidden information\n- Personal Impact Hook: Connect directly to viewer's life or work Hook Guidelines: - Keep hooks under 10 words when possible\n- Use active voice and strong verbs\n- Include emotional triggers (curiosity, fear, excitement, surprise)\n- Avoid technical jargon - make it accessible\n- Consider adding numbers or specific claims for credibility\n</hook_angle_framework> <process>\n1. Ingest: Review the entire raw text content provided below.\n2. Deduplicate: Identify stories covering the same core event. Group these together, treating them as a single story. All associated links will be consolidated in the final output.\n3. Select & Rank: Apply the Curation Framework to select the 3-5 best stories. Rank them from most to least viral potential.\n4. Generate Hooks: For each selected story, create 2-3 compelling hook angles using the Hook Angle Framework .\n</process> <output_format>\nYour final output must be a single, valid JSON object and nothing else. Do not include any text, explanations, or markdown formatting like ` json before or after the JSON object. The JSON object must have a single root key, stories , which contains an array of story objects. Each story object must contain the following keys:\n- title (string): A catchy, viral-optimized title for the story.\n- summary (string): A concise, 1-2 sentence summary explaining the story's hook and why it's compelling for a social media audience.\n- hook_angles (array of objects): 2-3 hook angles for opening the video. Each hook object contains:\n  - hook (string): The actual hook text/opening line\n  - type (string): The type of hook being used (from the Hook Angle Framework)\n  - rationale (string): Brief explanation of why this hook works for this story\n- sources (array of strings): A list of all consolidated source URLs for the story. These MUST be extracted from the provided context. You may NOT include URLs here that were not found in the provided source context. The url you include in your output MUST be the exact verbatim url that was included in the source material. The value you output MUST be like a copy/paste operation. You MUST extract this url exactly as it appears in the source context, character for character. Treat this as a literal copy-paste operation into the designated output field. Accuracy here is paramount; the extracted value must be identical to the source value for downstream referencing to work. You are strictly forbidden from creating, guessing, modifying, shortening, or completing URLs. If a URL is incomplete or looks incorrect in the source, copy it exactly as it is. Users will click this URL; therefore, it must precisely match the source to potentially function as intended. You cannot make a mistake here.\n``` After I get the top 3-5 stories picked out from this prompt, I share those results in slack so I have an easy to follow trail of stories for each news day. 4. Loop to generate each script For each of the selected top stories, I then continue to the final part of this workflow which is responsible for actually writing the TikTok / IG Reel video scripts. Instead of trying to 1-shot this and generate them all at once, I am iterating over each selected story and writing them one by one. Each of the selected stories will go through a process like this: Start by additional sources from the story URLs to get more context and primary source material Feeds the full story context into a viral script writing prompt Generates multiple different hook options for me to later pick from Creates two different 50-60 second scripts optimized for talking-head style videos (so I can pick out when one is most compelling) Uses examples of previously successful scripts to maintain consistent style and format Shares each completed script in Slack for me to review before passing off to the video editor. Script Writing Prompt ```jsx\nYou are a viral short-form video scriptwriter for David Roberts, host of \"The Recap.\" Follow the workflow below each run to produce two 50-60-second scripts (140-160 words). Before you write your final output, I want you to closely review each of the provided REFERENCE_SCRIPTS and think deeploy about what makes them great. Each script that you output must be considered a great script. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 1 ‚Äì Ideate ‚Ä¢ Generate five distinct hook sentences (‚â§ 12 words each) drawn from the STORY_CONTEXT. STEP 2 ‚Äì Reflect & Choose ‚Ä¢ Compare hooks for stopping power, clarity, curiosity. ‚Ä¢ Select the two strongest hooks (label TOP HOOK 1 and TOP HOOK 2). ‚Ä¢ Do not reveal the reflection‚Äîonly output the winners. STEP 3 ‚Äì Write Two Scripts For each top hook, craft one flowing script ‚âà 55 seconds (140-160 words). Structure (no internal labels): ‚Äì Open with the chosen hook. ‚Äì One-sentence explainer. ‚Äì 5-7 rapid wow-facts / numbers / analogies. ‚Äì 2-3 sentences on why it matters or possible risk. ‚Äì Final line = a single CTA ‚Ä¢ Ask viewers to comment with a forward-looking question or ‚Ä¢ Invite them to follow The Recap for more AI updates. Style: confident insider, plain English, light attitude; active voice, present tense; mostly ‚â§ 12-word sentences; explain unavoidable jargon in ‚â§ 3 words. OPTIONAL POWER-UPS (use when natural) ‚Ä¢ Authority bump ‚Äì Cite a notable person or org early for credibility. ‚Ä¢ Hook spice ‚Äì Pair an eye-opening number with a bold consequence. ‚Ä¢ Then-vs-Now snapshot ‚Äì Contrast past vs present to dramatize change. ‚Ä¢ Stat escalation ‚Äì List comparable figures in rising or falling order. ‚Ä¢ Real-world fallout ‚Äì Include 1-3 niche impact stats to ground the story. ‚Ä¢ Zoom-out line ‚Äì Add one sentence framing the story as a systemic shift. ‚Ä¢ CTA variety ‚Äì If using a comment CTA, pose a provocative question tied to stakes. ‚Ä¢ Rhythm check ‚Äì Sprinkle a few 3-5-word sentences for punch. OUTPUT FORMAT  (return exactly this‚Äîno extra commentary, no hashtags) HOOK OPTIONS ‚Ä¢ Hook 1 ‚Ä¢ Hook 2 ‚Ä¢ Hook 3 ‚Ä¢ Hook 4 ‚Ä¢ Hook 5 TOP HOOK 1 SCRIPT [finished 140-160-word script] TOP HOOK 2 SCRIPT [finished 140-160-word script] REFERENCE_SCRIPTS <Pass in example scripts that you want to follow and the news content loaded from before>\n``` 5. Extending this workflow to automate further So right now my process for creating the final video is semi-automated with human in the loop step that involves us copying the output of this automation into other tools like HeyGen to generate the talking avatar using the final script and then handing that over to my video editor to add in the b-roll footage that appears on the top part of each short form video. My plan is to automate this further over time by adding another human-in-the-loop step at the end to pick out the script we want to go forward with ‚Üí Using another prompt that will be responsible for coming up with good b-roll ideas at certain timestamps in the script ‚Üí use a videogen model to generate that b-roll ‚Üí finally stitching it all together with json2video. Depending on your workflow and other constraints, It is really up to you how far you want to automate each of these steps. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=7WsmUlbyjMM The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "Help Please",
            "link": "https://old.reddit.com/r/n8n/comments/1ksnvdm/power_up_your_n8n_workflows_with_diverse_content/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/short_form_video_script_generator.json\", \"https://www.youtube.com/watch?v=7WsmUlbyjMM\"]"
          }
        }
      },
      {
        "json": {
          "id": "recU5cImzyvwY75iE",
          "createdTime": "2025-07-24T10:42:14.000Z",
          "fields": {
            "Name": "Nocare420",
            "body_text": "With the new ChatGPT-4o image generator, I've been considering whether I could move away from actively using LinkedIn. The constant need to create organic content and post daily to gain followers or connect with people feels like a significant time investment. Providing free content and value every day is demanding, and I'm looking for a more efficient approach.\nMy idea is to use Gemini 2.5 for writing the text and ChatGPT-4o for generating images. I have a template in mind, and I'd like a workflow where the AI generates a LinkedIn post based on this template, sends it to me for approval, and only posts if I give it the go-ahead. However, my current setup doesn't integrate with ChatGPT-4o, and I'm unsure how to create this workflow. (And i would like to add a node  where i send them like every week or every month a linkedin post or linkedin profile to inspire from it and post similar like him)\nAny help or suggestions on how to integrate these tools and automate this process would be greatly appreciated)\nP.S. I found a helpful YouTube channel https://youtu.be/o_oSYl6gSO8?si=rj7hAaFtimFn4iIk that inspired this idea, and the template I'm referring to can be found there.\"\nIf i did find  a way how this is created i will post the template next post :)",
            "header": "Need help: workflow automations needed within the realm of content marketing",
            "link": "https://old.reddit.com/r/n8n/comments/1lzz02o/how_much_is_my_workflow_worth/",
            "Extracted Links": "[\"https://youtu.be/o_oSYl6gSO8\"]"
          }
        }
      },
      {
        "json": {
          "id": "recRioYQfiCy6E244",
          "createdTime": "2025-07-24T10:42:14.000Z",
          "fields": {
            "Name": "ProEditor69",
            "body_text": "after going through over 1,000 n8n templates , I‚Äôm convinced most of them are either half-baked, broken, or riddled with bugs. So many look good on GitHub or in the n8n library, but once you import them‚Ä¶ missing nodes, failed integrations, or completely outdated logic. It‚Äôs like people just export random experiments and call them ‚Äúproduction-ready.‚Äù Anyway, after way too many months of testing, debugging, and cursing at my screen, I found a few actually solid workflows worth using especially if you're doing lead gen, enrichment, or routing work. Firstly here are OG n8n Templates these are Awesome and works enescingoz/awesome-n8n-templates ‚Äì A well-curated repo with real-world workflows, including one for lead scoring and CRM integration. https://github.com/wassupjay/n8n-free-templates Bonus : How I Vet GitHub Repos Automatically After getting burned a few times, I stopped trusting stars or even READMEs. So now, I run a quick reputation check before using any GitHub template. Specifically: Is it active? Are people talking about it on Reddit or X ? Are there better forks or open issues? Does it look like a side project someone forgot about? To automate this, I ask BhindiAI to scan Reddit and X for the most relevant discussions about a repo, summarize the general vibe (positive, meh, sketchy), and extract key takeaways. It then logs that info repo link + commentary + score straight into my Google Sheet. That way I‚Äôm not wasting time setting up stuff the internet already knows is broken. Reddit and X Discussions are helpful to know what is a bs better than github stars. The worst part? Some templates had security vulnerabilities that weren't obvious until I dug deeper. One workflow was storing API keys in plain text comments, and another was making unencrypted calls to external services. I now have a strict checklist I run through before implementing anything in production. I've also started forking promising templates immediately and maintaining my own versions. Too many times I've seen maintainers abandon projects or push breaking changes without warning. Now I control my own destiny with these workflows. The community aspect is real though - I've found some of my best templates through Reddit and random Twitter threads where people share their actual working setups. The official n8n community forums are hit or miss, but when you find someone who's actually using these workflows at scale, their insights are gold.",
            "header": "üöÄ New Workflow added of Short Video Creation üé•",
            "link": "https://old.reddit.com/r/n8n/comments/1j7rod6/n8n_workflow_automate_ugc_video_creation_from_any/",
            "Extracted Links": "[\"https://github.com/wassupjay/n8n-free-templates\"]"
          }
        }
      },
      {
        "json": {
          "id": "recF85Vo9C7PU2Wnl",
          "createdTime": "2025-07-24T10:37:52.000Z",
          "fields": {
            "Name": "Valuable_Simple3860",
            "body_text": "I run an AI Tools directory and a large growth opportunity for us is writing \"roundup\" articles that highlight the best AI tools in a particular software category such as \"Best AI Image Editors\", \"Best AI Sales Tools\" , and many more. The problem is, this content takes a ton of time to research, write, and build out rankings for each tool. With the major improvements that have rolled out to \"Deep Research\" tools, we decide to see if we could leverage the output of these deep research reports, feed it into our own prompts, and scale out the process of creating great content. This automation is what we were able to build and I'm happy to report this process has allowed us to significantly scale up our content creation process. I'm pretty amazed at how much depth Chat GPT's deep research tool is able to provide and I think in many cases it is able to provide better insights than if we were doing this by hand. Here's the automation breakdown 1. Trigger / Inputs We use a simple form trigger for this automation that takes in some details specific to the structure of our website such as the slug for the category and the target audience for this article We also accept an optional parameter here for any tools that we know we want to be included in the \"Best Tools by Category\" roundup in order to give us a bit more control of the final output. 2. Building Necessary Context The next few nodes in here our very specific to our own website. We get started by making an HTTP request into our own API to get details about the category we are going to write about. After that, we make another HTTP request to our own API to get a list of all tools that are inside the category we are going to write about. Depending on the type of article you are going to write, this initial \"build context\" step is likely going to look very different for you so be sure you start with the end vision in mind of what you are trying to create and then work backwards from there to build the context you need. 3. Building The Deep Research Prompt The next step here is to take all of that context we just gathered and bundle it together into a prompt that we will feed into a deep research tool like Chat GPT deep research or Gemini Deep research. In our case, we build a prompt that focuses on telling the deep research tool to focus on picking out the top 6-8 tools in the provided category with a strong focus on social signals and real reviews left by users of the tool. Once again, this is going to be something that you will need to customize for your own use case. You should expect to spend a lot of time iterating on this prompt to get it in a good spot. I think we probably went through 30+ iterations of this prompt until we were happy with the deep research output. 4. Human-In-The-Loop Step Once the prompt is generated, we pause the automation with a sendAndWait slack message that will post the full prompt in slack. We then copy that prompt into Chat GPT deep research and let that run until we get the deep research report back After the deep research process finishes, we click the \"Add Result\" button in slack which will let us paste in the output of deep research into a form and continue with the automation. The Chat GPT deep research output is just so good so we opted to go this way. 5. Generating Article Content With the deep research report content now inside our n8n execution, we are then able to proceed with generation our article. We opted to take an approach where we build this section by section (like lego block) instead of trying to one-shot the entire article. We found this to be more effective and result in better output. Here's how we split it up: Intro paragraph and article meta description get written first in two separate prompts After that, we write another paragraph that details the ranking criteria that was used to pick out the top tools and how they were evaluated We then move on to writing the main content for each of the selected tools. We have a single prompt and a format we follow for each tool, so we loop over the selected tools and generate this content for each. Lastly, we take the output for each of these sections and pass that into a final prompt that is responsible for writing a conclusion. 6. Pushing To Our CMS In the final section of this automation, we take the output of each of the nodes from before and bundle it up into into a format that can easily get inserted into our CMS. We use Sanity CMS for hosting all of our content which doesn't have an out of the box API that we can just push content too, so we opted to create our own API endpoint to handle this for us. If you are hosting your content on something like Webflow / WordPress / another CMS, you should be able to replace the node here with an HTTP request that publishes your content to replicate this behavior. Workflow Link + Other Resources Github workflow link for this automation: https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json YouTube video that walks through this workflow step-by-step: https://youtu.be/uDrkgEuEOBA Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "Power Up Your n8n Workflows with Diverse Content: LumenFeed.com Offers 10k Free API Requests/Month!",
            "link": "https://old.reddit.com/r/n8n/comments/1ltsocy/i_have_reviewed_over_1000_n8n_templates_heres_how/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-workflows/blob/main/write_seo_optimized_listicle_article.json\", \"https://youtu.be/uDrkgEuEOBA\"]"
          }
        }
      },
      {
        "json": {
          "id": "recciLP0ZcDSSInhU",
          "createdTime": "2025-07-24T10:37:54.000Z",
          "fields": {
            "Name": "Majestic-Fix-3857",
            "body_text": "So I downloaded 2,050 public n8n workflows and then used claude opus 4 to help me vibe code my way through a detailed analysis. I used cursor as my code running tool, ran the claude scripts over the 2,000 JSON files, created a report, and then summarised into the below actionable doc Here is a video walkthrough of me visually going over the insights + also exploring the recommendations on the n8n canvas: https://youtu.be/BvBa_npD4Og Or if you just wanna read, here is the claude actionable report (hope you legends enjoy and find useful) -- n8n Workflow Best Practices Guide Learnings from Analyzing 2,000+ Production Workflows This guide is based on insights gathered from analyzing 2,050 production n8n workflows containing 29,363 nodes. It highlights common patterns, critical issues, and best practices for building robust, secure, and maintainable automation workflows. üìä Executive Summary Our analysis revealed critical gaps in error handling (97% of workflows lack it), security vulnerabilities (320 public webhooks without auth), and efficiency issues (7% contain unused nodes). This guide provides actionable recommendations to address these issues and build better workflows. Key Statistics: 2,050 workflows analyzed 29,363 total nodes 14.3 average nodes per workflow 97% lack error handling 472 security vulnerabilities found 34.7% are AI/ML workflows üö® Critical Issue #1: Error Handling (97% Gap) The Problem Only 62 out of 2,050 workflows (3%) have any error handling mechanism. This means when things fail, workflows silently break without notification or recovery. Best Practices 1. Always Use Error Triggers // Add an Error Trigger node at the beginning of every workflow\n// Connect it to a notification system (Email, Slack, etc.)\nError Trigger ‚Üí Format Error Message ‚Üí Send Notification 2. Implement Node-Level Error Handling For critical nodes (HTTP requests, database operations, API calls): Enable \"Continue On Fail\" for non-critical operations Add retry logic with exponential backoff Set appropriate timeout values 3. Error Handling Template Start ‚Üí Error Trigger ‚Üí Error Handler\n  ‚Üì\nMain Workflow Logic\n  ‚Üì\nCritical Operation (with retry: 3, delay: 1000ms)\n  ‚Üì\nSuccess Path / Error Path 4. Monitoring Pattern Log all errors to a centralized system Include workflow name, node name, error message, and timestamp Set up alerts for repeated failures üîí Critical Issue #2: Security Vulnerabilities The Problems 320 public webhooks without authentication 152 unsecure HTTP calls 3 workflows with hardcoded secrets Security Best Practices 1. Webhook Security // Always enable authentication on webhooks\nWebhook Settings:\n  - Authentication: Header Auth / Basic Auth\n  - Use HTTPS only\n  - Implement IP whitelisting where possible\n  - Add rate limiting 2. Secure API Communications Never use HTTP - always use HTTPS Store credentials in n8n's credential system, never hardcode Use OAuth2 when available (694 workflows do this correctly) Implement API key rotation policies 3. Authentication Methods (from most to least secure) OAuth2 - Use for major integrations API Keys - Store securely, rotate regularly Basic Auth - Only when necessary, always over HTTPS No Auth - Never for public endpoints 4. Secret Management Checklist [ ] No hardcoded API keys in Code/Function nodes [ ] All credentials stored in n8n credential manager [ ] Regular credential audit and rotation [ ] Environment-specific credentials (dev/staging/prod) üéØ Critical Issue #3: Workflow Efficiency The Problems 144 workflows with unused nodes (264 total unused nodes) 133 workflows with API calls inside loops 175 workflows with redundant transformations Efficiency Best Practices 1. Clean Architecture Input ‚Üí Validate ‚Üí Transform ‚Üí Process ‚Üí Output\n         ‚Üì (fail)\n      Error Handler 2. Avoid Common Anti-Patterns ‚ùå Bad: API in Loop Loop ‚Üí HTTP Request ‚Üí Process Each ‚úÖ Good: Batch Processing Collect Items ‚Üí Single HTTP Request (batch) ‚Üí Process Results 3. Node Optimization Remove unused nodes (7% of workflows have them) Combine multiple Set nodes into one Use Code node for complex transformations instead of chaining Set nodes Cache API responses when possible 4. Performance Guidelines Average workflow should complete in < 10 seconds Use Split In Batches for large datasets Implement parallel processing where possible (only 4.8% currently do) Add progress logging for long-running workflows ü§ñ AI/ML Workflow Best Practices (34.7% of workflows) Common Patterns Observed 346 agent-based workflows 267 multi-model workflows 201 with memory systems 0 with vector databases (RAG pattern opportunity) AI Workflow Best Practices 1. Prompt Engineering // Structure prompts with clear sections\nconst prompt = `\nSystem: ${systemContext}\nContext: ${relevantData}\nTask: ${specificTask}\nFormat: ${outputFormat}\n`; 2. Cost Optimization Use GPT-3.5 for simple tasks, GPT-4 for complex reasoning Implement caching for repeated queries Batch similar requests Monitor token usage 3. Agent Workflow Pattern Trigger ‚Üí Context Builder ‚Üí Agent (with tools) ‚Üí Output Parser ‚Üí Response\n                                ‚Üì\n                          Memory System 4. Error Handling for AI Handle rate limits gracefully Implement fallback models Validate AI outputs Log prompts and responses for debugging üìã Workflow Organization Best Practices The Problem 74.7% of workflows categorized as \"general\" Poor documentation and organization Organization Best Practices 1. Naming Conventions [Category]_[Function]_[Version]\nExamples:\n- Sales_LeadScoring_v2\n- HR_OnboardingAutomation_v1\n- DataSync_Salesforce_Daily_v3 2. Tagging Strategy Essential tags to use: Environment: prod, staging, dev Category: sales, hr, finance, it-ops Frequency: real-time, hourly, daily, weekly Status: active, testing, deprecated 3. Documentation with Sticky Notes The #1 most used node (7,024 times) - use it well: Document complex logic Explain business rules Note dependencies Include contact information 4. Workflow Structure üìù Sticky Note: Workflow Overview\n    ‚Üì\n‚öôÔ∏è Configuration & Setup\n    ‚Üì\nüîÑ Main Process Logic\n    ‚Üì\n‚úÖ Success Handling | ‚ùå Error Handling\n    ‚Üì\nüìä Logging & Monitoring üîÑ Common Node Sequences (Best Patterns) Based on the most frequent node connections: 1. Data Transformation Pattern Set ‚Üí HTTP Request (379 occurrences) Best for: Preparing data before API calls 2. Chained API Pattern HTTP Request ‚Üí HTTP Request (350 occurrences) Best for: Sequential API operations (auth ‚Üí action) 3. Conditional Processing If ‚Üí Set (267 occurrences)\nSwitch ‚Üí Set (245 occurrences) Best for: Data routing based on conditions 4. Data Aggregation Set ‚Üí Merge (229 occurrences) Best for: Combining multiple data sources üõ°Ô∏è Security Checklist for Every Workflow Before Deployment [ ] No hardcoded credentials [ ] All webhooks have authentication [ ] All external calls use HTTPS [ ] Sensitive data is encrypted [ ] Access controls are implemented [ ] Error messages don't expose sensitive info Regular Audits [ ] Review webhook authentication monthly [ ] Rotate API keys quarterly [ ] Check for unused credentials [ ] Verify HTTPS usage [ ] Review access logs üìà Optimization Opportunities 1. For Complex Workflows (17.5%) Break into sub-workflows Use Execute Workflow node Implement proper error boundaries Add performance monitoring 2. For Slow Workflows Identify bottlenecks (usually API calls) Implement caching Use batch operations Add parallel processing 3. For Maintenance Remove unused nodes (found in 7% of workflows) Consolidate redundant operations Update deprecated node versions Document business logic üéØ Top 10 Actionable Recommendations Implement Error Handling - Add Error Trigger to all production workflows Secure Webhooks - Enable authentication on all 320 public webhooks Use HTTPS - Migrate 152 HTTP calls to HTTPS Clean Workflows - Remove 264 unused nodes Batch API Calls - Refactor 133 workflows with APIs in loops Add Monitoring - Implement centralized logging Document Workflows - Use Sticky Notes effectively Categorize Properly - Move from 74.7% \"general\" to specific categories Implement Retry Logic - Add to all critical operations Regular Audits - Monthly security and performance reviews üöÄ Quick Start Templates 1. Error-Handled Webhook Workflow Webhook (with auth) ‚Üí Validate Input ‚Üí Process ‚Üí Success Response\n         ‚Üì                    ‚Üì (error)\n   Error Trigger ‚Üê Error Formatter ‚Üê Error Response 2. Secure API Integration Schedule Trigger ‚Üí Get Credentials ‚Üí HTTPS Request (with retry) ‚Üí Process Data\n                                            ‚Üì (fail)\n                                     Error Handler ‚Üí Notification 3. AI Workflow with Error Handling Trigger ‚Üí Build Context ‚Üí AI Agent ‚Üí Validate Output ‚Üí Use Result\n    ‚Üì            ‚Üì             ‚Üì            ‚Üì\nError Handler ‚Üê Rate Limit ‚Üê Timeout ‚Üê Invalid Output üìö Resources and Next Steps Create Workflow Templates - Build standard templates with error handling Security Audit Tool - Scan all workflows for vulnerabilities Performance Dashboard - Monitor execution times and failures Training Program - Educate team on best practices Governance Policy - Establish workflow development standards üéâ Success Metrics After implementing these practices, aim for: < 5% workflows without error handling 0 public webhooks without authentication 0 HTTP calls (all HTTPS) < 3% workflows with unused nodes > 90% properly categorized workflows < 10s average execution time This guide is based on real-world analysis of 2,050 production workflows. Implement these practices to build more reliable, secure, and maintainable n8n automations.",
            "header": "I analysed 2,000+ n8n workflows and this is what I learned",
            "link": "https://old.reddit.com/r/n8n/comments/1l1f6n8/i_analysed_2000_n8n_workflows_and_this_is_what_i/",
            "Extracted Links": "[\"https://youtu.be/BvBa_npD4Og\"]"
          }
        }
      },
      {
        "json": {
          "id": "recyZnJJTbb5GP3sL",
          "createdTime": "2025-07-24T10:37:42.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "Clipping youtube videos and twitch VODs into tiktoks/reels/shorts is a super common practice for content creators and major brands where they take their long form video content like podcasts and video streams then turn it into many different video clips that later get posted and shared on TikTok + IG Reels. Since I don‚Äôt have an entire team of editors to work on creating these video clips for me, I decided to build an automation that does the heavy lifting for me. This is what I was able to come up with: Here's how the automation works 1. Workflow Trigger / Inputs The workflow starts with a simple form trigger that accepts a YouTube video URL. In your system, you could automate this further by setting up an RSS feed for your youtube channel or podcast. 2. Initial Video Processing Request Once the URL is submitted, the workflow makes an HTTP POST request to the Vizard API to start processing the video: The request includes the YouTube video URL and processing parameters like max_clip_number - IMO the defaults actually work pretty well here so I‚Äôd leave most alone to let their system analyze for the most viral moments in the video By default, it will also add in captions. If you want to customize the style of the video / keep captions consistent with your brand you can also specify a template id in your request The API returns a project ID and initial status code that we'll use to poll for results after the video analysis completes 3. Polling Loop for Processing Status Since video processing can take significant time (especially for longer videos), the workflow uses a simple polling system which will loop over: A simple Wait node pauses execution for 10 seconds between status checks (analyzing long form videos will take a fair bit of time so this will check many times) An HTTP GET request checks the processing status using the project ID from the initial request If the status code is 1000 (still processing), the workflow loops back to wait and check again When the status reaches 2000 (completed), the workflow continues to the next section 4. Filtering and Processing Results Once the video analysis/processing is complete, I get all the video clip results back in the response and I‚Äôm able to continue with further processing. The response I get back from this include a virality score of 1/10 based on the clips potential. Clips are filtered based on virality score - I only keep clips with a score of 9 or higher In my testing, this reduces a lot of the noise / worthless clips from the output After those videos get filtered, I then share a summary message in slack with the title, virality score, and download link for each clip You can also take this further and auto-generate a social media caption + pickout ideal hashtags to use based on the content of the video and where you plan to post it. If you want to auto-post, you would use another tool like blotato to publish to each social media platform you need I personally really like using slack to review all the clips because it centralizes all clips into a single spot for me to review before posting. Costs I‚Äôm currently just on the ‚ÄúCreator‚Äù plan for Vizard which costs $29 / month for 600 upload minutes (of source YouTube material). This fits my needs for the content that I create but if you are running a larger scale clipping operation or working with multiple brands that cost is going to scale up linearly for the minutes of source material you use. Workflow Link + Other Resources YouTube video that walks through this workflow node-by-node: https://www.youtube.com/watch?v=Yb-mZmvHh-I The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json",
            "header": "Discussion",
            "link": "https://old.reddit.com/r/n8n/comments/1m0o40i/i_built_an_ai_workflow_that_analyzes_longform/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/viral_youtube_video_clipper.json\", \"https://www.youtube.com/watch?v=Yb-mZmvHh-I\"]"
          }
        }
      },
      {
        "json": {
          "id": "reckXPE96KtM9s4pw",
          "createdTime": "2025-07-24T10:37:55.000Z",
          "fields": {
            "Name": "hncvj",
            "body_text": "Saw a post here today about an \"I built an AI that runs a viral ASMR channel hands-free.\" Bold claim. Except‚Ä¶ it's not their work. The entire idea, workflow, and execution is taken straight from Nate Herk' YouTube channel and his free S kool community, without a single credit or change. Original here: https://youtu.be/RvuPRX-b7Lc?si=a2iHUF3pqPWDdhAN What's worse? They didn't even bother innovating or acknowledging. Just slapped a \"Workflow - Code Not Included\" banner and called it theirs. This trend of stealing real creators' work for fake clout needs to stop. It's not \"inspired,\" it's not \"repurposed\", it's a direct plagiarism, plain and simple. It's my humble request, respect the creators. Don't build your karma on someone else's grind. What do you guys think?",
            "header": "Workflow - Code Included",
            "link": "https://old.reddit.com/r/n8n/comments/1lunl3h/stealing_someone_elses_project_and_claiming_it_as/",
            "Extracted Links": "[\"https://youtu.be/RvuPRX-b7Lc\"]"
          }
        }
      },
      {
        "json": {
          "id": "recKA8lvoYJdxvgpk",
          "createdTime": "2025-07-24T10:37:56.000Z",
          "fields": {
            "Name": "conor_is_my_name",
            "body_text": "UPDATE: Check the 2nd branch if you want to use cloudflared. TLDR:  Put simply, this is the pro level install that you have been looking for, even if you aren't a power user (yet). I can't be the only one who has struggled with queue mode (the documentation is terrible), but I finally nailed it.  Please take this code and use it so no one else has to suffer through what I did building it.  This version is better in every way than the regular install.  Just leave me a GitHub star. https://github.com/conor-is-my-name/n8n-autoscaling First off, who is this for? Anyone who wants to run n8n either locally or on a single server of any size (ram should be 2gb+, but I'd recommend  8gb+ if using with the other containers linked at the bottom, the scrapers are ram hogs) You want simple setup Desire higher parallel throughput (it won't make single jobs faster) Why is queue mode great? No execution limit bottlenecks scales up and scales down based on load if a worker fails, the jobs gets reassigned Whats inside: A Docker-based autoscaling solution for n8n workflow automation platform. Dynamically scales worker containers based on Redis queue length. No need to deal with k8s or any other container scaling provider, a simple script runs it all and is easily configurable. Includes Puppeteer and Chrome built-in for pro level scraping directly from the n8n code node.  It makes it so much easier to do advanced scraping compared to using the community nodes.  Just paste your puppeteer script in a regular code node and you are rolling.  Use this in conjunction with my Headful Chrome Docker that is linked at the bottom for great results on tricky websites. Everything installs and configures automatically, only prerequisite is having docker installed.  Works on all platforms, but the puppeteer install requires some dependency tweaks if you are using a ARM cpu.  (an AI will know what to do for the dependency changes) Install instructions: Windows or Mac: Install the docker desktop app. Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d Linux: Follow the instructions for the Docker Convenience Script . Copy this to a folder (make sure you get all the files, sometimes .env is hidden).  In that folder open a terminal and run: docker compose up -d That's it.  (But remember to change the passwords) Default settings are for 50 simultaneous workflow executions.  See GitHub page for instructions on changing the worker count and concurrency. A tip for those who are in the process of leveling up their n8n game: move away from google sheets and airtable - they are slow and unstable embrace Postgres - with AI its really easy, just ask it what to do and how to set up the tables Tested on a Netcup 8 core 16gb Root VPS - RS 2000 G11.  Easily ran hundreds of simultaneous executions.  Lower end hardware should work fine too, but you might want to limit the number of worker instances to something that makes sense for your own hardware. If this post inspires you to get a server, use this link .  Or don't, just run this locally for free. I do n8n consulting, send me a message if you need help on a project. check out my other n8n specific GitHub repos: Extremely fast google maps scraper - this one is a masterpiece web scraper server using crawlee for deep scraping - I've scraped millions of pages using this Headful Chrome Docker with Puppeteer for precise web scraping and persistent sessions - for tricky websites and those requiring logins",
            "header": "My n8n Workflows Site ( update )- Find Quality Automations Easily!",
            "link": "https://old.reddit.com/r/n8n/comments/1kgxgo4/i_made_a_docker_compose_for_n8n_queue_mode_with/",
            "Extracted Links": "[\"https://github.com/conor-is-my-name/n8n-autoscaling\"]"
          }
        }
      },
      {
        "json": {
          "id": "recHRfrdss1FUYyX9",
          "createdTime": "2025-07-24T10:37:57.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "There‚Äôs been a huge trend of Bigfoot / Yeti vlog videos exploding across IG and TikTok all created with Veo 3 and I wanted to see if I could replicate and automate the full process of: Taking a simple idea as input Generate an entire story around that simple idea Turn that into a Veo 3 prompt Finally generate those videos inside n8n using FAL. Had a lot of fun building this and am pretty happy with final output . Here‚Äôs the workflow breakdown. 1. Input / Trigger The input and trigger for this workflow is a simple Form Trigger that has a single text field. What goes into here is a simple idea for for what bigfoot will be doing that will later get turned into a fully fleshed-out story. It doesn‚Äôt need any crazy detail, but just needs something the story can be anchored around. Here‚Äôs an example of one of the ones I used earlier to give you a better idea: jsx\nBigfoot discovers a world war 2 plane crash while on a hike through the deep forest that he hasn't explored yet 2. The Narrative Writer Prompt The next main node of this automation is what I call the ‚Äúnarrative writer‚Äù. Its function is very similar to a storyboard artist where it will accept the basic ideas as input and will generate an outline for each clip that needs to be generated for the story. Since Veo 3 has a hard limit of 8 seconds per video generation, that was a constraint I had to define here. So after this runs, I get an outline that splits up the story into 8 distinct clips that are each 8 seconds long. I also added in extra constraints here like what I want bigfoots personality to be like on camera to help guide the dialog and I also specified that I want the first out of the 8 clips to always be an introduction to the video. Here‚Äôs the full prompt I am using: ```jsx Role: You are a creative director specializing in short-form, character-driven video content. Goal: Generate a storyboard outline for a short vlog based on a user-provided concept. The output must strictly adhere to the Persona, Creative Mandate, and Output Specification defined below. [Persona: Bigfoot the Vlogger] Identity: A gentle giant named \"Sam,\" who is an endlessly curious and optimistic explorer. His vibe is that of a friendly, slightly clumsy, outdoorsy influencer discovering the human world for the first time. Voice & Tone: Consistently jolly, heartwarming, and filled with childlike wonder. He is easily impressed and finds joy in small details. His language is simple, and he might gently misuse human slang. PG-rated, but occasional mild exasperation like \"geez\" or \"oh, nuts\" is authentic. His dialog and lines MUST be based around the \"Outdoor Boys\" YouTube channel and he must speak like the main character from that Channel. Avoid super generic language. Physicality: An 8-foot male with shaggy, cedar-brown fur ( #6d6048 ) and faint moss specks. His silhouette is soft and \"huggable\" due to fluffy fur on his cheeks and shoulders. Features soft, medium-amber eyes, rounded cheeks, a broad nose, and short, blunt lower canines visible when he smiles. He holds a simple selfie stick at all times. [Creative Mandate] Visual Style: All scenes are shot 16:9 from a selfie-stick perspective held by Bigfoot. The style must feel like authentic, slightly shaky \"found footage.\" The camera is always on him, not his POV. Narrative Goal: The primary objective is to create audience affection. Each scene must showcase Bigfoot's charm through his gentle humor, endearing discoveries, or moments of vulnerability. The 8-scene arc must have a satisfying and heartwarming payoff. [Output Specification] Structure: Provide a storyboard with exactly 8 sequential scenes, formatted as shown below. Introduction Rule: Scene 1 must be a direct-to-camera introduction. In it, Bigfoot should enthusiastically greet his viewers (e.g., \"Hey everybody!\" or \"Hi friends!\") and briefly state the goal or adventure for the vlog, based on the user's concept. Duration: Each scene represents 8 seconds of footage. Content per Scene: For each scene, provide a single, descriptive paragraph. This paragraph must seamlessly weave together the visual action, Bigfoot's expressions, and his spoken dialogue. Each scene you create should be part of a cohesive story. Example Formats: SCENE 1 (0:00-0:08): Sam the Bigfoot grins warmly into his selfie stick, the background slightly out of focus. He waves a large, furry hand and says, \"Hey friends, Sam here! Today, we're going to try and build a brand new shelter out of... well, whatever we can find! Wish me luck!\" SCENE 3 (0:32-0:40): Sam holds up a rusty, bent bicycle wheel, peering at it curiously with his head tilted. He tries to spin it, a look of concentration on his face. \"Hmm. This is a weird-looking rock. Very... holey. Not good for a wall, I don't think.\" Task Using the rules above, create the storyboard outline for the following concept: {{ $json['Bigfoot Video Idea'] }} ``` And here‚Äôs what the output looks like after this prompt runs: ```jsx\nTitle: Finding History in the Deep Woods - WWII Plane Discovery!\nConcept: Bigfoot discovers a world war 2 plane crash while on a hike through the deep forest Scene 1 | 0:00-0:08\nSam the Bigfoot grins warmly into his selfie stick, towering evergreens framing his fuzzy shoulders. He adjusts his grip on the stick with both hands and beams at the camera. \"Hey everybody, Sam here! Today we're heading way deep into the back country for some serious exploring. You never know what kind of treasures you might stumble across out here!\" Scene 2 | 0:08-0:16\nSam trudges through dense undergrowth, his selfie stick bouncing slightly as he navigates around massive fir trees. Moss hangs like curtains around him, and his amber eyes dart curiously from side to side. \"Man, this forest just keeps getting thicker and thicker. Perfect day for it though - nice and cool, birds are singing. This is what I call the good life, friends!\" Scene 3 | 0:16-0:24\nSam suddenly stops mid-stride, his eyes widening as he stares off-camera. The selfie stick trembles slightly in his grip, showing his surprised expression clearly. \"Whoa, hold on a second here...\" He tilts his shaggy head to one side, his mouth forming a perfect 'O' of amazement. \"Guys, I think I'm seeing something pretty incredible through these trees.\" Scene 4 | 0:24-0:32\nSam approaches cautiously, pushing aside hanging branches with his free hand while keeping the camera steady. His expression shifts from wonder to respectful awe as he gets closer to his discovery. \"Oh my goodness... friends, this is... this is an old airplane. Like, really old. Look at the size of this thing!\" His voice drops to a whisper filled with reverence. Scene 5 | 0:32-0:40\nSam extends the selfie stick to show himself standing next to the moss-covered wreckage of a WWII fighter plane, its metal frame twisted but still recognizable. His expression is one of deep respect and fascination. \"This has got to be from way back in the day - World War Two maybe? The forest has just been taking care of it all these years. Nature's got its own way of honoring history, doesn't it?\" Scene 6 | 0:40-0:48\nSam crouches down carefully, his camera capturing his gentle examination of some scattered debris. He doesn't touch anything, just observes with his hands clasped respectfully. \"You know what, guys? Someone's story ended right here, and that's... that's something worth remembering. This pilot was probably somebody's son, maybe somebody's dad.\" His usual cheerfulness is tempered with genuine thoughtfulness. Scene 7 | 0:48-0:56\nSam stands and takes a step back, his expression shifting from contemplation to gentle resolve. He looks directly into the camera with his characteristic warmth, but there's a new depth in his amber eyes. \"I think the right thing to do here is let the proper folks know about this. Some family out there might still be wondering what happened to their loved one.\" Scene 8 | 0:56-1:04\nSam gives the camera one final, heartfelt look as he begins to back away from the site, leaving it undisturbed. His trademark smile returns, but it's softer now, more meaningful. \"Sometimes the best adventures aren't about what you take with you - they're about what you leave behind and who you help along the way. Thanks for exploring with me today, friends. Until next time, this is Sam, reminding you to always respect the stories the forest shares with us.\"\n``` 3. The Scene Director Prompt The next step is to take this story outline and turn it into a real prompt that can get passed into Veo 3. If we just took the output from the outline and tried to create a video, we‚Äôd get all sorts of issues where the character would not be consistent across scenes, his voice would change, the camera used would change, and things like that. So the next step of this process is to build out a highly detailed script with all technical details necessary to give us a cohesive video across all 8 clips / scenes we need to generate. The prompt here is very large so I won‚Äôt include it here (it is included inside the workflow) but I will share the desired output we are going for. For every single 8 second clip we generate, we are creating something exactly like that will cover: Scene overview Scene description Technical specs like duration, aspect ratio, camera lens Details of the main subject (Bigfoot) Camera motion Lighting Atmosphere Sound FX Audio Bigfoot dialog Really the main goal here is to be as specific as possible so we can get consistent results across each and every scene we generate. ```jsx SCENE 4  ‚ñ∏  ‚ÄúTrail to the Lake‚Äù  ‚ñ∏  0 ‚Äì 8 s Selfie-stick POV. Bigfoot strolls through dense cedar woods toward a sun-sparkled lake in the distance. No spoken dialogue in this beat‚Äîjust ambient forest sound and foot-fall crunches. Keeps reference camera-shake, color grade, and the plush, lovable design. SCENE DESCRIPTION POV selfie-stick vlog: Bigfoot walks along a pine-needle path, ferns brushing both\nsides. Sunbeams flicker through the canopy. At the 6-second mark the shimmering\nsurface of a lake appears through the trees; Bigfoot subtly tilts the stick to\nhint at the destination. TECHNICAL SPECS ‚Ä¢ Duration‚ÄÉ8 s‚ÄÉ‚Ä¢ 29.97 fps‚ÄÉ‚Ä¢ 4 K UHD‚ÄÉ‚Ä¢ 16 : 9 horizontal ‚Ä¢ Lens‚ÄÉ24 mm eq, ∆í/2.8‚ÄÉ‚Ä¢ Shutter‚ÄÉ1/60 s (subtle motion-blur) ‚Ä¢ Hand-held wobble amplitude cloned from reference clip (small ¬±2¬∞ yaw/roll). SUBJECT DETAILS  (LOCK ACROSS ALL CUTS) ‚Ä¢ 8-ft male Bigfoot, cedar-brown shaggy fur #6d6048 with faint moss specks. ‚Ä¢ Fluffier cheek & shoulder fur ‚Üí plush, huggable silhouette. ‚Ä¢ Eyes: soft medium-amber, natural catch-lights only ‚Äî no glow or excess brightness. ‚Ä¢ Face: rounded cheeks, gentle smile crease; broad flat nose; short blunt lower canines. ‚Ä¢ Hands: dark leathery palms, 4-inch black claws; right paw grips 12-inch carbon selfie stick. ‚Ä¢ Friendly, lovable, gentle vibe. CAMERA MOTION 0 ‚Äì 2 s‚ÄÉStick angled toward Bigfoot‚Äôs chest/face as he steps onto path. 2 ‚Äì 6 s‚ÄÉSmooth forward walk; slight vertical bob; ferns brush lens edges. 6 ‚Äì 8 s‚ÄÉStick tilts ~20¬∞ left, revealing glinting lake through trees; light breeze ripples fur. LIGHTING & GRADE Late-morning sun stripes across trail; teal-olive mid-tones, warm highlights,\ngentle film grain, faint right-edge lens smudge (clone reference look). ATMOSPHERE FX ‚Ä¢ Dust motes / pollen drifting in sunbeams. ‚Ä¢ Occasional leaf flutter from breeze. AUDIO BED  (NO SPOKEN VOICE) Continuous forest ambience: songbirds, light wind, distant woodpecker;\nsoft foot-crunch on pine needles; faint lake-lap audible after 6 s. END FRAME Freeze at 7.8 s with lake shimmering through trees; insert one-frame white-noise\npop to preserve the series‚Äô hard-cut rhythm.\n``` 3. Human in the loop approval The middle section of this workflow is a human in the loop process where we send the details of the script to a slack channel we have setup and wait for a human to approve or deny it before we continue with the video generation. Because generation videos this way is so expensive ($6 per 8 seconds of video), we want to review this before before potentially being left with a bad video. 4. Generate the video with FAL API The final section of this automation is where actually take the scripts generated from before, iterate over each, and call in to FAL‚Äôs Veo 3 endpoint to queue up the video generation request and wait for it to generate. I have a simple polling loop setup to check its status every 10 seconds which will loop until the video is completely rendered. After that is done, the loop will move onto the next clip/scene it needs to generate until all 8 video clips are rendered. Each clip get‚Äôs uploaded to a Google Drive I have configured so my editor can jump in and stitch them together into a full video. If you wanted to extend this even further, you could likely use the json2video API to do that stitching yourself, but that ultimately depends on how far or not you want to automate. Notes on keeping costs down Like I mentioned above, the full cost of running this is currently very expensive. Through the FAL API it costs $6 for 8 seconds of video so this probably doesn‚Äôt make sense for everyone‚Äôs use case. If you want to keep costs down, you can still use this exact same workflow and drop the 3rd section that uses the FAL API. Each of the prompts that get generated for the full script can simply be copied and pasted into Gemini or Flow to generate a video of the same quality but it will be much cheaper to do so. Workflow Link + Other Resources Github n8n workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=C65c8itWvf4 Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "I built an AI workflow that analyzes long-form YouTube videos and generates short form clips optimized for TikTok / IG Reels / YT Shorts",
            "link": "https://old.reddit.com/r/n8n/comments/1lkbx2b/i_built_this_ai_automation_that_generates_viral/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/veo_3_viral_bigfoot_vlog_generator.json\", \"https://www.youtube.com/watch?v=C65c8itWvf4\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec0UBmuJiLs8pP91",
          "createdTime": "2025-07-24T10:37:58.000Z",
          "fields": {
            "Name": "conor_is_my_name",
            "body_text": "Hey everyone! Today I am sharing my custom built google maps scraper.  It's extremely fast compared to most other maps scraping services and produces more reliable results as well. I've spent thousands of dollars over the years on scraping using APIFY, phantom buster, and other services.  They were ok but I also got many formatting issues which required significant data cleanup. Finally went ahead and just coded my own.  Here's the link to the GitHub repo, just give me a star: https://github.com/conor-is-my-name/google-maps-scraper It includes example json for n8n workflows to get started in the n8n nodes folder.  Also included the Postgres code you need to get basic tables up and running in your database. These scrapers are designed to be used in conjunction with my n8n build linked below.  They will work with any n8n install, but you will need to update the IP address rather than just using the container name like in the example. https://github.com/conor-is-my-name/n8n-autoscaling If using the 2 together, make sure that you set up the external docker network as described in the instructions.  Doing so makes it much easier to get the networking working. Why use this scraper? Best in class speed and reliability You can scale up with multiple containers on multiple computers/servers, just change the IP. A word of warning: Google will rate limit you if you just blast this a million times.  Slow and steady wins the race.  I'd recommend starting at no more than 1 per minute per IP address.  There are 1440 minutes in a day x 100 results per search = 144,000 results per day. https://preview.redd.it/ozwqpz31is0f1.png?width=644&format=png&auto=webp&s=a0b472a70594a4b3fbd72ffc7e429e7ae7acbb11 Example Search: Query = Hotels in 98392 (you can put anything here) language = en limit results = 1 (any number) headless = true [\n  {\n    \"name\": \"Comfort Inn On The Bay\",\n    \"place_id\": \"0x549037bf4a7fd889:0x7091242f04ffff4f\",\n    \"coordinates\": {\n      \"latitude\": 47.543005199999996,\n      \"longitude\": -122.6300069\n    },\n    \"address\": \"1121 Bay St, Port Orchard, WA 98366\",\n    \"rating\": 4,\n    \"reviews_count\": 735,\n    \"categories\": [\n      \"Hotel\"\n    ],\n    \"website\": \"https://www.choicehotels.com/washington/port-orchard/comfort-inn-hotels/wa167\",\n    \"phone\": \"3603294051\",\n    \"link\": \"https://www.google.com/maps/place/Comfort+Inn+On+The+Bay/data=!4m10!3m9!1s0x549037bf4a7fd889:0x7091242f04ffff4f!5m2!4m1!1i2!8m2!3d47.5430052!4d-122.6300069!16s%2Fg%2F1tfz9wzs!19sChIJidh_Sr83kFQRT___BC8kkXA?authuser=0&hl=en&rclk=1\"\n  },",
            "header": "I built an AI that's smarter than most real estate agents. Here's the n8n blueprint.",
            "link": "https://old.reddit.com/r/n8n/comments/1kml8st/i_made_a_google_maps_scraper_designed/",
            "Extracted Links": "[\"https://github.com/conor-is-my-name/google-maps-scraper\", \"https://github.com/conor-is-my-name/n8n-autoscaling\"]"
          }
        }
      },
      {
        "json": {
          "id": "recgef1IIt8Hq8X6U",
          "createdTime": "2025-07-24T10:37:43.000Z",
          "fields": {
            "Name": "LargePay1357",
            "body_text": "First, an AI agent goes back and forth with you to finalize the title and headings. Then, it calls the next workflow to write the blog post. Code for both workflows: https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json Video tutorial: https://www.youtube.com/watch?v=6AgfcKUMkVY",
            "header": "Workflow - Code Not Included",
            "link": "https://old.reddit.com/r/n8n/comments/1l0nj7m/i_built_a_workflow_that_generates_longform_blog/",
            "Extracted Links": "[\"https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Agent.json\", \"https://github.com/shabbirun/redesigned-octo-barnacle/blob/cdb9dd03ee1bfa63318fd460bf6342c5ab4609ce/Blog_Post_Writer_Workflow.json\", \"https://www.youtube.com/watch?v=6AgfcKUMkVY\"]"
          }
        }
      },
      {
        "json": {
          "id": "recBjzHI2ywSYb6J5",
          "createdTime": "2025-07-24T10:42:19.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "I wanted to put together a quick guide and walk through on how you can use n8n to be the backend that powers your mobile apps / web apps / internal tools. I‚Äôve been using Lovable a lot lately and thought this would be the perfect opportunity to put together this tutorial and showcase this setup working end to end. The Goal - Clone the main app functionality Cal AI I thought a fun challenge for this would be cloning the core feature of the Cal AI mobile app which is an AI calorie tracker that let‚Äôs you snap a picture of your meal and get a breakdown of all nutritional info in the meal. I suspected this all could be done with a well written prompt + an API call into Open AI‚Äôs vision API (and it turns out I was right). 1. Setting up a basic API call between lovable and n8n Before building the whole frontend, the first thing I wanted to do was make sure I could get data flowing back and forth between a lovable app and a n8n workflow. So instead of building the full app UI in lovable, I made a very simple lovable project with 3 main components: Text input that accepts a webhook url (which will be our n8n API endpoint) File uploader that let‚Äôs me upload an image file for our meal we want scanned Submit button to make the HTTP request to n8n When I click the button, I want to see the request actually work from lovable ‚Üí n8n and then view the response data that actually comes back (just like a real API call). Here‚Äôs the prompt I used: jsx\nPlease build me a simple web app that contains three components. Number one, a text input that allows me to enter a URL. Number two, a file upload component that lets me upload an image of a meal. And number three, a button that will submit an HTTP request to the URL that was provided in the text input from before. Once that response is received from the HTTP request, I want you to print out JSON of the full details of the successful response. If there's any validation errors or any errors that come up during this process, please display that in an info box above. Here‚Äôs the lovable project if you would like to see the prompts / fork for your own testing: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 2. Setting up the n8n workflow for our backend Next up we need to setup the n8n workflow that will be our ‚Äúbackend‚Äù for the app. This step is actually pretty simple to get n8n working as your backend, all you need is the following: A Webhook Trigger on your workflow Some sort of data processing in the middle (like loading results from your database or making an LLM-chain call into an LLM like GPT) A Respond To Webhook node at the very end of the workflow to return the data that was processed On your initial Webhook Trigger it is very important that you change the Respond option set to Using ‚ÄòRespond To Webhook‚Äô Node. If you don‚Äôt have this option set, the webhook is going to return data immediately instead of waiting for any of your custom logic to process such as loading data from your database or calling into a LLM with a prompt. In the middle processing nodes, I ended up using Open AI‚Äôs vision API to upload the meal image that will be passed in through the API call from lovable and ran a prompt over it to extract the nutritional information from the image itself. Once that prompt finished running, I used another LLM-chain call with an extraction prompt to get the final analysis results into a structured JSON object that will be used for the final result. I found that using the Auto-fixing output parser helped a lot here to make this process more reliable and avoided errors during my testing. Meal image analysis prompt: ```jsx\n<identity>\nYou are a world-class AI Nutrition Analyst.\n</identity> <mission>\nYour mission is to perform a detailed nutritional analysis of a meal from a single image. You will identify the food, estimate portion sizes, calculate nutritional values, and provide a holistic health assessment.\n</mission> Analysis Protocol\n1. Identify: Scrutinize the image to identify the meal and all its distinct components. Use visual cues and any visible text or branding for accurate identification.\n2. Estimate: For each component, estimate the portion size in grams or standard units (e.g., 1 cup, 1 filet). This is critical for accuracy.\n3. Calculate: Based on the identification and portion estimates, calculate the total nutritional information for the entire meal.\n4. Assess & Justify: Evaluate the meal's overall healthiness and your confidence in the analysis. Justify your assessments based on the provided rubrics. Output Instructions\nYour final output MUST be a single, valid JSON object and nothing else. Do not include json markers or any text before or after the object. Error Handling\nIf the image does not contain food or is too ambiguous to analyze, return a JSON object where confidenceScore is 0.0 , mealName is \"Unidentifiable\", and all other numeric fields are 0 . OUTPUT_SCHEMA json\n{\n  \"mealName\": \"string\",\n  \"calories\": \"integer\",\n  \"protein\": \"integer\",\n  \"carbs\": \"integer\",\n  \"fat\": \"integer\",\n  \"fiber\": \"integer\",\n  \"sugar\": \"integer\",\n  \"sodium\": \"integer\",\n  \"confidenceScore\": \"float\",\n  \"healthScore\": \"integer\",\n  \"rationale\": \"string\"\n} Field Definitions\n*   ** mealName : A concise name for the meal (e.g., \"Chicken Caesar Salad\", \"Starbucks Grande Latte with Whole Milk\"). If multiple items of food are present in the image, include that in the name like \"2 Big Macs\".\n*   ** calories : Total estimated kilocalories.\n*   ** protein : Total estimated grams of protein.\n*   ** carbs : Total estimated grams of carbohydrates.\n*   ** fat : Total estimated grams of fat.\n*   ** fiber : Total estimated grams of fiber.\n*   ** sugar : Total estimated grams of sugar (a subset of carbohydrates).\n*   ** sodium : Total estimated milligrams (mg) of sodium.\n*   ** confidenceScore : A float from 0.0 to 1.0 indicating your certainty. Base this on:\n    *   Image clarity and quality.\n    *   How easily the food and its components are identified.\n    *   Ambiguity in portion size or hidden ingredients (e.g., sauces, oils).\n*   ** healthScore : An integer from 0 (extremely unhealthy) to 10 (highly nutritious and balanced). Base this on a holistic view of:\n    *   Level of processing (whole foods vs. ultra-processed).\n    *   Macronutrient balance.\n    *   Sugar and sodium content.\n    *   Estimated micronutrient density.\n*   ** rationale **: A brief (1-2 sentence) explanation justifying the healthScore and confidenceScore . State key assumptions made (e.g., \"Assumed dressing was a standard caesar\" or \"Portion size for rice was difficult to estimate\").\n``` On the final Respond To Webhook node it is also important to node that this is the spot where we will be cleaning up the final data setting the response Body for the HTTP request / API call. For my use-case where we are wanting to send back nutritional info for the provided image, I ended up formatting my response as JSON to look like this: jsx\n{\n  \"mealName\": \"Grilled Salmon with Roasted Potatoes and Kale Salad\",\n  \"calories\": 550,\n  \"protein\": 38,\n  \"carbs\": 32,\n  \"fat\": 30,\n  \"fiber\": 7,\n  \"sugar\": 4,\n  \"sodium\": 520,\n  \"confidenceScore\": 0.9,\n  \"healthScore\": 4\n} 3. Building the final lovable UI and connecting it to n8n With the full n8n backend now in place, it is time to spin up a new Lovable project and build the full functionality we want and style it to look exactly how we would like. You should expect this to be a pretty iterative process. I was not able to get a fully working app in 1-shot and had to chat back and forth in lovable to get the functionality working as expected. Here‚Äôs some of the key points in the prompt / conversation that had a large impact on the final result: Initial create app prompt: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8pekjpfeyrs52bdf1m1dm7 Style app to more closely match Cal AI: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rbd2wfvkrxxy7pc022n0e Setting up iphone mockup container: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jx8rs1b8e7btc03gak9q4rbc Wiring up the app to make an API call to our n8n webhook: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxajea31e2xvtwbr1kytdxbb Updating app functionality to use real API response data instead of mocked dummy data (important - you may have to do something similar): https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739?messageId=aimsg_01jxapb65ree5a18q99fsvdege If I was doing this again from the start, I think it would actually be much easier to get the lovable functionality working with default styles to start with and then finish up development by styling everything you need to change at the very end. The more styles, animations, other visual elements that get added in the beginning, the more complex it is to change as you get deeper into prompting. Lovable project with all prompts used: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 4. Extending this for more complex cases + security considerations This example is a very simple case and is not a complete app by any means. If you were to extend this functionality, you would likely need to add in many more endpoints to take care of other app logic + features like saving your history of scanned meals, loading up your history of scanned meals, other analysis features that can surface trends. So this tutorial is really meant to show you a bit of what is possible between lovable + n8n. The other really important thing I need to mention here is the security aspect of a workflow like this. When following my instructions above, your webhook url will not be secure. This means that if your webhook url leaks, it is completely possible for someone to make API requests into your backend and eat up your entire quota for n8n executions and run up your Open AI bill. In order to get around this for a production use-case, you will need to implement some form of authentication to protect your webhook url from malicious actors. This can be something as simple as basic auth where web apps that consume your API need to have a username / password or you could build out a more advanced auth system to protect your endpoints. My main point here is, make sure you know what you are doing before you publically rollout a n8n workflow like this or else you could be hit with a nasty bill or users of your app could be accessing things they should not have access to. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json Lovable project links: Simple n8n API request: https://lovable.dev/projects/621373bd-d968-4aff-bd5d-b2b8daab9648 Full Cal AI cloned project: https://lovable.dev/projects/cd8fe427-c0ed-433b-a2bb-297aad0fd739 YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=4c-kYOiksFg Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "Looking for Review!",
            "link": "https://old.reddit.com/r/n8n/comments/1lineqi/how_you_can_setup_and_use_n8n_as_your_backend_for/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/cal_ai_clone_backend.json\", \"https://www.youtube.com/watch?v=4c-kYOiksFg\"]"
          }
        }
      },
      {
        "json": {
          "id": "recPprAHKoo6ct5YP",
          "createdTime": "2025-07-24T10:37:40.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "I built a content repurposing system that I have been using for the past several weeks that my YouTube video as input ‚Üí scrapes the transcript ‚Üí repurposes it into a post that is optimized for engagement on the platform I am posting to (right now just Twitter and LinkedIn but can be extended to many more). My social accounts are still pretty young so I don‚Äôt have great before/after stats to share, but I‚Äôm confident that the output quality here is on-par with what other creators are making and going viral with. My goal with this is to share a basic setup that you can take an run with in your own business to be customized for your niche / industry and add additional target platforms that you want to repurpose to. You could even change the main input to a long form blog post as your starting point instead of a youtube video. Here's a full breakdown of the automation 1. Workflow Trigger / Input The workflow starts with a simple form trigger that accepts a YouTube video URL as input. This is specific to our business since we always start with creating YouTube content first and then repurpose it into other formats. Form trigger accepts YouTube video URL as required text input If your content workflow starts with blog posts or other formats, you'll need to modify this trigger accordingly The URL gets passed through to the scraping operation (If your company and or your client‚Äôs company starts with a blog post first, I‚Äôd suggested simply using a tool to scrape that web page to load of that text content) 2. Scrape YouTube Video This is where we extract the video metadata and full transcript using a YouTube Scraper on Apify. Starts by using the streamers/youtube-scraper actor from the apify store (Costs $5 per 1,000 videos you scrape) Makes an HTTP request to the /run-sync-get-dataset-items endpoint to start scraping / get results back I like using this endpoint when consuming apify actors as it returns data back in the same http request we make. No need to setup polling or extra n8n nodes to use The scraper extracts title, metadata, and most importantly the full transcript in SRT format (timestamps w/ the text that was said in the video) 3. Generate Twitter Post The Twitter repurposing path follows a structured approach using a few examples I want to replicate + a detailed prompt. Set Twitter Examples : Simple ‚ÄúSet Field‚Äù node where I curated and put in 8 high-performing tweet examples that define the style and structure I want to replicate Build Master Prompt : Another Set Field node where I build a prompt that will tell the LLM to: Analyze the source YouTube transcript material Study the Twitter examples for structure and tone Generate 3 unique viral tweet options based on the content LLM Chain Call : Pass the complete prompt to Claude Sonnet Format and Share : Clean up the output and share the best 3 tweet options to Slack for me to review ```jsx ROLE: You are a world-class social media copywriter and viral growth hacker. Your expertise is in the AI, automation, and no-code space on Twitter/X. You are a master at deconstructing viral content and applying its core principles to generate new, successful posts. OBJECTIVE: Your mission is to generate three distinct, high-potential viral tweets . This tweet will promote a specific n8n automation, with the ultimate goal of getting people to follow my profile, retweet the post, and comment a specific keyword to receive the n8n workflow template via DM. STEP 1: ANALYZE SOURCE MATERIAL First, meticulously analyze the provided YouTube video transcript below. Do not summarize it. Instead, your goal is to extract the following key elements:\n1. The Core Pain Point: What is the single most frustrating, time-consuming, or tedious manual task that this automation eliminates?\n2. The \"Magic\" Solution: What is the most impressive or \"wow\" moment of the automation? What does it enable the user to do that felt impossible or difficult before?\n3. The Quantifiable Outcome: Identify any specific metrics of success mentioned (e.g., \"saves 10 hours a week,\" \"processes 100 leads a day,\" \"automates 90% of the workflow\"). If none are mentioned, create a powerful and believable one. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES Next, study the structure, tone, and psychological hooks of the following successful tweets. These examples are your primary source for determining the structure of the tweets you will generate. <twitter_tweet_examples>\n{{ $('set_twitter_examples').item.json.twitter_examples }}\n</twitter_tweet_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE TWEETS Now you will generate the 3 unique, viral tweet options. Your primary task is to act as a structural analyst: analyze the provided examples, identify the most effective structures, and then apply those structures to the content from Step 1. Your process: 1. Identify Core Structures: Analyze the <twitter_tweet_examples> . Identify the different underlying formats. For instance, is there a \"Problem ‚Üí Solution\" structure? A \"Shocking Result ‚Üí How-to\" structure? A \"Controversial Statement ‚Üí Justification\" structure? Identify the 3 most distinct and powerful structures present.\n2. Map Content to Structures: For each of the 3 structures you identified, map the \"Pain Point,\" \"Magic Solution,\" and \"Outcome\" from Step 1 into that framework.\n3. Craft the Tweets: Generate one tweet for each of the 3 structures you've chosen. The structure of each tweet (the hook, the flow, the tone) should directly mirror the style of the example it is based on. Essential Components: While you choose the overall structure, ensure each tweet you craft contains these four key elements, integrated naturally within the chosen format:\n- A Powerful Hook: The opening line that grabs attention.\n- A Clear Value Proposition: The \"what's in it for me\" for the reader.\n- An Irresistible Offer: The free n8n workflow template.\n- A High-Engagement Call to Action (CTA): The final call to action must include elements the ask for a follow, a retweet, and a comment of the \"[KEYWORD]\". CONSTRAINTS: -   Vary light use of emojis to add personality and break up the text. Not all Tweets you write should have emojis.\n-   Keep the tone energetic, confident, and educational, mirroring the tone found in the examples.\n-   Ensure the chosen [KEYWORD] is simple, relevant, and in all caps. Now, generate the 3 distinct tweet options, clearly labeled as Tweet Option 1 , Tweet Option 2 , and Tweet Option 3 . For each option, briefly state which example structure you are applying. (e.g., \"Tweet Option 1: Applying the 'Problem ‚Üí Solution' structure from Example 2.\").\n``` 4. Generate LinkedIn Post The LinkedIn path follows a similar but platform-specific approach (better grammar and different call to action): Set LinkedIn Examples : Curated examples of high-performing LinkedIn posts with different formatting and professional tone Build LinkedIn-Specific Prompt : Modified prompt that positions the LLM as a \"B2B content strategist and LinkedIn growth expert\" rather than a viral Twitter copywriter Generate Multiple Options : Creates 3 different LinkedIn post variations optimized for professional engagement Review Process : Posts all options to Slack for me to review The key difference is tone and structure - LinkedIn posts are longer, more professional, minimize emoji usage, and focus on business value rather than viral hooks. It is important to know your audience here and have a deep understanding of the types of posts that will do well. ```jsx ROLE: You are a world-class B2B content strategist and LinkedIn growth expert. Your expertise lies in creating compelling professional content around AI, automation, and no-code solutions. You are a master of professional storytelling, turning technical case studies into insightful, engaging posts that drive meaningful connections and establish thought leadership. OBJECTIVE: Your mission is to generate three distinct, high-potential LinkedIn posts . Each post will promote a specific n8n automation, framing it as a professional case study. The ultimate goals are to:\n1.  Grow my LinkedIn professional network (followers).\n2.  Establish my profile as a go-to resource for AI and automation.\n3.  Drive awareness and interest in my YouTube channel.\n4.  Get users to comment for a lead magnet (the n8n workflow). STEP 1: ANALYZE SOURCE MATERIAL (THE BUSINESS CASE) First, meticulously analyze the provided YouTube video transcript. Do not summarize it. Instead, extract the following key business-oriented elements:\n1. The Business Pain Point: What common, frustrating, or inefficient business process does this automation solve? Frame it in terms of lost time, potential for human error, or missed opportunities.\n2. The Strategic Solution: How does the n8n automation provide a smart, strategic solution? What is the core \"insight\" or \"lever\" it uses to create value?\n3. The Quantifiable Business Impact: What is the measurable outcome? Frame it in business terms (e.g., \"reclaimed 10+ hours for strategic work,\" \"achieved 99% accuracy in data processing,\" \"reduced new client onboarding time by 50%\"). If not explicitly mentioned, create a powerful and believable metric. <youtube_video_transcript>\n{{ $('set_youtube_details').item.json.transcript }}\n</youtube_video_transcript> STEP 2: STUDY INSPIRATIONAL EXAMPLES (LINKEDIN POSTS) Next, study the structure, tone, and especially the Call to Action (CTA) of the following successful LinkedIn posts. These examples are your primary source for determining the structure of the posts you will generate. Pay close attention to the length of the examples as they \"feel\" right in length. <linkedin_post_examples>\n{{ $('set_linked_in_examples').item.json.linked_in_examples }}\n</linkedin_post_examples> STEP 3: DECONSTRUCT EXAMPLES & GENERATE POSTS Now you will generate 3 unique LinkedIn post options. Your primary task is to act as a content strategist: analyze the provided LinkedIn examples, identify the most effective post structures, and then apply those structures to the business case from Step 1. Your process: 1. Identify Core Structures: Analyze the <linkedin_post_examples> . Identify 3 distinct formats (e.g., \"Problem/Agitate/Solve,\" \"Personal Story ‚Üí Business Lesson,\" \"Contrarian Take ‚Üí Justification\").\n2. Map Content to Structures: For each structure, weave the \"Business Pain Point,\" \"Strategic Solution,\" and \"Business Impact\" into a compelling narrative.\n3. Craft the Posts: Generate one post for each chosen structure. The post should be highly readable, using short paragraphs and ample white space. Essential Components for each LinkedIn Post: - An Intriguing Hook: A first line that stops the scroll and speaks to a professional ambition or frustration.\n- A Relatable Story/Problem: Briefly set the scene using the \"Business Pain Point.\"\n- The Insightful Solution: Explain the \"Strategic Solution\" as the turning point.\n- A Dynamic, High-Engagement Call to Action (CTA): This is critical. Instead of a fixed format, you will craft the most effective CTA by analyzing the examples provided. Your CTA must accomplish two things:\n    1.  Clearly state how to get the free n8n workflow template by commenting with a specific [KEYWORD] .\n    2.  Naturally encourage following my profile and sharing the post. Draw inspiration for the wording and style directly from the successful CTAs in the examples. If it fits the narrative, you can subtly mention that more deep dives are on my YouTube. CONSTRAINTS: -   Use emojis sparingly and professionally (e.g., ‚úÖ, üí°, üöÄ) to enhance readability.\n-   The tone must be professional, insightful, and helpful.\n-   The [KEYWORD] should be a professional, single word in all caps (e.g., BLUEPRINT, WORKFLOW, SYSTEM). FINAL OUTPUT FORMAT: You MUST format your entire response as a single, valid JSON object. The root of the object should be a key named \"post_options\", which contains an array of three post objects. Adhere strictly to the following structure for each object:\n{\n  \"analysis\": \"<string: Explain which LinkedIn example structure was applied>\",\n  \"post_text\": \"<string: The full text of the LinkedIn post, with line breaks>\"\n}\nDo not include any text or explanations outside of the JSON object.\n``` 5. Final Output Review Both paths conclude by sharing the generated content to Slack channels for human review. This gives me 3 Twitter options and 3 LinkedIn options to choose from, each optimized for best engagement. All I have to do is copy and paste the one I like the most into my social media scheduling tool then I‚Äôm done. Extending the System The best part about this is it is very easy to extend this system for any type of repurposing you need to do. LinkedIn / Twitter is only the starting point, it can be taken much further. Instagram carousel posts - Take the transcript ‚Üí pull out a few quotes ‚Üí generate an image using either Canva an AI Image generator Newsletter sections - Take the transcript + video url ‚Üí build a prompt that will write a mini-promo section for your video to be included in your newsletter Blog post / tutorial post - Take the transcript ‚Üí write a prompt that will turn it into a text-based tutorial to be published on your blog. Each new path would follow the same pattern: curate platform-specific examples, build targeted prompts, and generate multiple options for review. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=u9gwOtjiYnI The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json",
            "header": "How you can setup and use n8n as your backend for a Lovable.dev app (I cloned the mobile app Cal AI)",
            "link": "https://old.reddit.com/r/n8n/comments/1lvlb8n/i_built_a_content_repurposing_system_that_turns/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/content_repurposing_factory.json\", \"https://www.youtube.com/watch?v=u9gwOtjiYnI\"]"
          }
        }
      },
      {
        "json": {
          "id": "recuyWN25MOfra4A8",
          "createdTime": "2025-07-24T10:42:20.000Z",
          "fields": {
            "Name": "conor_is_my_name",
            "body_text": "I've shared a number of custom n8n builds and custom scrapers: https://github.com/conor-is-my-name/n8n-autoscaling https://github.com/conor-is-my-name/google-maps-scraper https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer https://github.com/conor-is-my-name/crawlee-server What do you want next? Things I'm considering: a real enterprise grade chatbot using Chatwoot and AI agents Scraper deep dive using crawlee Workflow I used to update 50k product listings AI content creation for wordpress that doesn't seem like AI spam.  Actual scholarly articles. anything else? I'll probably do all 4 on the list at some point. Bonus points if you send me a real database of stuff to connect to, or a specific site you want data from.  I can demo a whole integration then.",
            "header": "MVP Generator",
            "link": "https://old.reddit.com/r/n8n/comments/1ktun9q/you_guys_liked_my_custom_n8n_autoscaling_build/",
            "Extracted Links": "[\"https://github.com/conor-is-my-name/n8n-autoscaling\", \"https://github.com/conor-is-my-name/google-maps-scraper\", \"https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\", \"https://github.com/conor-is-my-name/crawlee-server\"]"
          }
        }
      },
      {
        "json": {
          "id": "reciWsL70puBb4cpd",
          "createdTime": "2025-07-24T10:37:58.000Z",
          "fields": {
            "Name": "hashpanak",
            "body_text": "I automated the creation of long-form, faceless YouTube videos using AI and n8n. This system churns out engaging videos and uploads them to Google Drive and YouTube, all in a no-code workflow. It‚Äôs perfect for anyone wanting to run a YouTube channel on autopilot without breaking the bank. Use It For: Creating faceless YouTube channels for motivational, educational, or storytelling content Generating video ideas and scripts for niches like history, self-improvement, or book summaries or any other use case. Automating daily, hourlyvideo uploads to grow your audience I explain the setup here -> https://youtu.be/8sIJdMwifSE I automated from ideation to publishing using n8n, OpenAI, and Flux AI (Fal.ai or GPT 4o image generation), with no third-party video assembly APIs - no JSON2VIDEO or CREATOMATE. Here‚Äôs the blueprint. How It Works This workflow turns a niche idea into a polished YouTube video in 5 stages: Video Ideation AI-Powered Ideas: Input a niche (e.g., ‚Äúmotivational‚Äù or ‚Äúhistory‚Äù) and topics (e.g., ‚Äúphilosophy‚Äù), and an AI agent (GPT-4.1 mini) generates JSON-formatted video ideas with category, story, duration, and tone. Google Sheets: Ideas are stored in Google Sheets, ready for manual tweaks or approval. Script and Audio Generation Story Agent: A second AI agent crafts a script based on the selected idea, tailored to the duration. Text-to-Speech: OpenAI‚Äôs TTS (or 11 Labs) converts the script into audio, with word-by-word transcription for precise timing. Scene Cutting and Image Generation Scene Editor: An AI agent splits the transcript into 2-5 second scenes, ensuring dynamic pacing. achieves this by transcript granularity set to words for whisper model and using an ai agent to cut at the end of a word Visuals via Flux  (Fal.ai): Another agent generates cinematic image prompts for each scene, rendered by Flux  Schnell model ($0.003/megapixel) - one of the cheapest. Video Assembly FFmpeg Magic: A custom 700-line code node uses FFmpeg to combine audio, images, and captions, cutting scenes to match spoken words with zoom-pan effects. No Third-Party APIs: Unlike JSON2Video or Creatomate, this runs locally or via a hosted API you control, or with a Code node. Publishing Google Drive Upload: The final video lands in a designated Drive folder. YouTube Push: The workflow creates a YouTube upload link and streams the video, setting title, description, and privacy status. Why This Works Save 10+ Hours/Week: Automates scripting, editing, and uploading - no manual work, except approvals. Grow your audience on autopilot Engaging Content: Dynamic scene cuts and AI-generated visuals keep viewers hooked. Budget-Friendly: NO CREATOMATE OR JSON2VIDEO - no manual editing I explain the setup here -> https://youtu.be/8sIJdMwifSE https://preview.redd.it/rja5cl4my80f1.png?width=2604&format=png&auto=webp&s=912478508f8f565686611262d7af5afc3a7f91ae https://preview.redd.it/wa7x7c7ny80f1.png?width=1236&format=png&auto=webp&s=63b19e6d209a23ebd8a5ec478992b2d4eeb29c54 Example: https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4/view?usp=drive_link",
            "header": "Question",
            "link": "https://old.reddit.com/r/n8n/comments/1kkhh1m/i_built_an_aipowered_automation_that_creates/",
            "Extracted Links": "[\"https://youtu.be/8sIJdMwifSE\", \"https://drive.google.com/file/d/1bei5oc3-zN90he_VgMG3gBzGIVIvZDw4\"]"
          }
        }
      },
      {
        "json": {
          "id": "recvr32lM8G0Wz46w",
          "createdTime": "2025-07-24T10:37:59.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "Now before I get started, I know this automation may be a bit controversial as there's a lot of spam already on Twitter, but I truly believe it is possible to build a Twitter / X reply bot that is useful to people if you get your messaging down and do a good job of filtering out irrelevant messages that don't make much sense to reply to. I currently run an AI Tools directory and we noticed that each day, there are a bunch of Tweets that get posted that ask for advice on choosing the best AI Tool for a specific task or job such as \"What is the best AI Tool for writing blog posts?\" or \"What is the best AI Tool for clipping short form videos?\" Tweets like this are perfect opportunity for us to jump in, and share a link to a category page or list of tools on our directory to help them find and explore exactly what they are looking for. The problem with this is it just would take forever to do this manually as I'd have to be in front of the screen all day watching Twitter instead of doing 'real work'. So, we decided to build an AI automation that completely automates this. At a high level, we use Mention.com to monitor and alert for AI Tool questions getting asked on twitter -> use a prompt to evaluate each of these tweets individually to see if it is a good and relevant question -> fetch a list of category pages from our own website -> write a helpful reply that mentions we have a page specifically for the type of tools they are looking for. Each reply we share here doesn't amount to a ton of impressions or traffic, but ultimately this is something we believe will compound over time as it lets us have this marketing motion turned on that wasn't feasible before. Here's a full breakdown of the automation 1. Trigger / Inputs The entry point into this whole automation starts with Mention.com , we setup a new keyword alert that monitors for phrases like \"Is there any AI Tool\" or \"How can I use AI to\", etc. This setup is really important as you need to filter out a bunch of the noise that doesn't make sense to reply to. It is also important that your alert that you have setup is going to be your target customer or persona you are trying to get in front of. After the alert is configured, we used the Mention.com <> Slack integration to post the feed of all alerts into a dedicated slack channel setup just for this. 2. Initial Filtering & Validation The next couple of nodes are responsible for further filtering out ineligible Tweets that we don't want to respond too. This includes checking if the Tweet from the alert is a Retweet or if the Tweet from the alert actually was from the account we want to with (avoid our own reply causing an infinite execution loop) 3. Evaluation Prompt + LLM Call The first LLM call we make here is a simple prompt that checks the text content of the Tweet from the alert and makes a decision if we want to proceed with creating a reply or if we should exit early out of the workflow. If you are taking this workflow and extending it for your own use-case, it will be important that you change this for your own goals. In this prompt, I found it most effective to include examples of Tweets that we did want to reply to and Tweets that we wanted to skip over. 4. Build Context for Tweet Reply This step is also going to be very specific to your own goals and how you want to modify this workflow. In our case, we are making an HTTP request to our own API in order to get back a JSON list of all category pages on our website. We then take that JSON and format it nicely into more LLM-friendly text We finally take that text and will include it in our next prompt to actually write the Tweet reply If you are going to use this workflow / automation, this step must be changed and customized for the kind of reply you are trying to create. If you are trying to share helpful resources with potential leads and customers, it would be a good idea to retrieve and build up that context at this step. 5. Write The Tweet Reply In this step we take all of the context created from before and use Claude to write a Tweet reply. For our reply, we like to keep it short + include a link to one of the category pages on the AI Tools website. Since our goal is to share these pages with people asking for AI Tool suggestions, we found it most effective to include Tweet input + good examples of a reply Tweet that we would personally write if we were doing this manually. 6. Posting The Reply + Notifying In Slack The final step here was actually using the X / Twiter node in n8n to post the reply to the original Tweet we got an alert for. All that is needed here is to pass in the initial Tweet Id we need to reply to and the output of our LLM call to claude which wrote the Tweet. After that, we have a couple of Slack nodes hooked up that leave a checkmark reaction and will share the Tweet output that claude decided to go with so we can easily monitor and make changes to the prompt if we found that the reply was not quite what we were looking for. Most of the work here comes from iterating on the prompt so its important to have a good feedback loop in place so you can see what is happening as the automation runs over more and more Tweets. Workflow Link + Other Resources Github workflow link: https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=Q_b5uPndsLY Also wanted to share that my team and I run a free Skool community called AI Automation Mastery where we build and share the automations we are working on. Would love to have you as a part of it if you are interested!",
            "header": "I Built an AI-Powered Automation That Creates Long-Form YouTube Videos From Scratch ‚Äî Steal My Workflow",
            "link": "https://old.reddit.com/r/n8n/comments/1l1mk0e/i_built_an_ai_workflow_that_monitors_twitter_x/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-workflows/blob/main/twitter_reply_guy_agent.json\", \"https://www.youtube.com/watch?v=Q_b5uPndsLY\"]"
          }
        }
      },
      {
        "json": {
          "id": "recne1ehFd88UjjwB",
          "createdTime": "2025-07-24T10:42:22.000Z",
          "fields": {
            "Name": "Marvomatic",
            "body_text": "Hey everyone, I wanted to share something I‚Äôve been using in my own workflow that‚Äôs saved me a ton of time: a set of free n8n templates for automating SERP analysis. I built these mainly to speed up keyword research and competitor analysis for content creation, and thought they might be useful for others here too. What these workflows do: Basically, you enter a focus keyword and a target country, and the workflow fetches organic search results, related searches, and FAQs from Google (using either SerpAPI or Serper). It grabs the top results for both mobile and desktop, crawls the content of those pages (using either Crawl4AI or Firecrawl), and then runs some analysis on the content with an LLM (I‚Äôm using GPT-4o-mini, but you can swap in any LLM you prefer). How it works: You start by filling out a simple form in n8n with your keyword and country. The workflow pulls SERP data (organic results, related searches, FAQs) for both device types. It then crawls the top 3 results (you can adjust this) and analyzes the content by using an LLM. The analysis includes article summaries, potential focus keywords, long-tail keyword ideas, and even n-gram analysis if there‚Äôs enough content. All the data gets saved to Google Sheets, so you can easily review or use it for further research. What the output looks like: At the end, you get a Google Soreadsheet with: The top organic results (URLs, titles, snippets) Summaries of each top result Extracted FAQs and related searches Lists of suggested keywords and long-tail variations N-gram breakdowns for deeper content analysis Why Three Templates? I included three templates to give you flexibility based on your preferred tools, budget, and how quickly you want to get started. Each template uses a different combination of SERP data providers (SerpApi or Serper) and content crawlers (Crawl4AI or Firecrawl). This way, you can choose the setup that best fits your needs‚Äîwhether you want the most cost-effective option, the fastest setup, or a balance of both. Personally, I‚Äôm using the version with Serper and Crawl4AI, which is pretty cost-effective (though you do need to set up Crawl4AI). If you want to get started even faster, there‚Äôs also a version that uses Firecrawl instead. You can find the templates on my GitHub profile https://github.com/Marvomatic/n8n-templates . Each template has it's own set up instructions in a sticky node. If anyone‚Äôs interested, I‚Äôm happy to answer questions. Would love to hear any feedback or suggestions for improvement!",
            "header": "I built an AI workflow that monitors Twitter (X) for relevant keywords and posts a reply to promote my business (Mention.com + X API)",
            "link": "https://old.reddit.com/r/n8n/comments/1kc5kin/efficient_serp_analysis_export_results_to_google/",
            "Extracted Links": "[\"https://github.com/Marvomatic/n8n-templates\"]"
          }
        }
      },
      {
        "json": {
          "id": "reclPrI5hHzkyDozs",
          "createdTime": "2025-07-24T10:37:59.000Z",
          "fields": {
            "Name": "Legitimate_Fee_8449",
            "body_text": "The best way to make money is by finding and solving real-world problems that people are actively discussing. This guide provides a step-by-step framework for building a no-code AI agent that automatically finds these business problems on Reddit and generates potential money-making solutions, helping you uncover profitable niches you might have otherwise missed. This workflow automates the entire discovery process. Here are the actionable tips to build this idea factory yourself: Step 1: Find the Problems (Reddit Node) Start your n8n workflow with the \"Reddit\" node. Configure it to search specific subreddits, like r/smallbusiness , for posts containing keywords that indicate a need, such as \"looking for a solution\" or \"recommendation\". Step 2: Filter the Noise (IF Node) Add an \"IF\" node to filter the results for quality. You can set conditions to only process posts with real engagement, like having at least two upvotes, and ensure the idea is recent by only including posts from the last six months. Step 3: AI Problem Identification Connect an AI Agent to the filtered posts. Its only job is to read the post and determine if it describes a business-related problem, then output a simple \"yes\" or \"no\". Step 4: AI Solution Generation For every post that the AI identifies as a problem, send it to another AI tool, like the OpenAI node. Prompt this AI to suggest a business idea or service that could solve the problem described in the post. Step 5: Store Your Goldmine (Google Sheets) Finally, take all the organized data‚Äîthe original post, the problem summary, and the new business idea‚Äîand save it into a Google Sheet for easy review and analysis.\nIf you can do this, you will have an automated system that finds profitable niches and delivers validated business ideas straight to a spreadsheet. Made a full video to explain everything:- https://youtu.be/Aovh878cJc4 What subreddits would you monitor with a system like this? Let me know in the comments!",
            "header": "So far this is what I have learnt in last 6 months in n8n, what else do I need to do next so as to be able to say... work for an enterprise client needing AI automation ...may be using n8n.",
            "link": "https://old.reddit.com/r/n8n/comments/1lfcnar/from_reddit_problem_to_business_idea_in_5_minutes/",
            "Extracted Links": "[\"https://youtu.be/Aovh878cJc4\"]"
          }
        }
      },
      {
        "json": {
          "id": "recsIQBXf8FOVuL26",
          "createdTime": "2025-07-24T10:38:00.000Z",
          "fields": {
            "Name": "ChaseAI",
            "body_text": "There was a great post on r/AI_Agents the other week that went through 6 months of Upwork data to get some insight on what sorts of AI systems companies are no shit paying money for. Believe it or not, it wasn't ASMR glass fruit cutting videos. Here is the quote: The Common Pain Points I Saw Repeated Over and Over: ‚ÄúI‚Äôm drowning in lead gen, I need this to run on autopilot‚Äù\n‚ÄúI get too many junk messages on WhatsApp / LinkedIn ‚Äî need something to filter and qualify leads‚Äù\n‚ÄúI have 10,000 rows of customer data and no time to sort through it manually‚Äù\n‚ÄúI want to turn YouTube videos into blog posts, tweets, summaries‚Ä¶ automatically‚Äù\n‚ÄúCan someone just connect GPT to my CRM and make it smart?‚Äù Exact Automations Clients Paid For: WhatsApp ‚Üí GPT lead qualification ‚Üí Google Sheets CRM\nAuto-reply bots for DMs that qualify and tag leads\nBrowser automations for LinkedIn scraping & DM follow-ups\nn8n flows that monitor RSS feeds and creates a custom news aggregator for finance companies For anyone whose been working with real clients in this space for any amount of time, this should come as no surprise. I think you can divide this up into roughly four categories: lead generation automations, lead qualification automations, content creation agents, and CRM integrated RAG agents. Those four alone cover probably 80% of the solutions I've sold since I started my agency so it makes sense to see that echoed in the data. I think people who want to get into the ai agency business get extremely overwhelmed with the breadth of \"viral\" n8n workflows shoved in their face on social media so I thought it would be helpful to not only call out the ones mentioned in this post but also run through an example for each. I'll link my YouTube video that goes over each of them in depth. The workflows for each are also freely available in the respective video description. ----- 1) Lead Generation https://www.youtube.com/watch?v=bFxWRkWAFzs User inputs the leads he wants, research is conducted on the sourced leads via Apollo + Tavily, research is used to create custom messaging, all info is then sent to Instantly.ai. I like this one since it all gets rolled into Instantly which handles all the cold email BS (email warm up, etc) and you won't have to bother with creating some fancy dashboard from scratch. 2) Lead Qualification https://www.youtube.com/watch?v=Vmgmva3dL44 This is a very simple foundation you could take a build upon to qualify leads. This uses gmail, but that could obviously be swapped for a true CRM input. Lead's info comes in, AI sorts / classifies / qualifies, and then you go down whatever path is appropriate from there. Really easy to customize and implement for a client. 3) Content Creation https://www.youtube.com/watch?v=mzxdtDCZbNY https://www.youtube.com/watch?v=Qp3h7WLYpH0 Obviously content comes in a billion different forms but it doesn't need to be complicated. Honestly I've found that clients are more pressed when it comes to just consistently posting the content across multiple platforms than they are about creating it but almost everyone wants some mega-simple LinkedIn post generator. 4) RAG https://www.youtube.com/watch?v=nwR5519zTC8 This form of RAG is as simple as it gets but I'm telling you most client's \"RAG Agent\" is a glorified FAQ chatbot that is able to locate and link internal documents to lazy employees. Understanding this very basic form of RAG will get you 90% of the way there. ----- The big takeaway is that the technical bar you must cross in order to make money selling AI solutions is not nearly as high as you think it is (the real hurdle is sourcing clients but that's an entirely different problem). The main issue is people get pulled into a million different directions trying to copy these over-engineered and flashy n8n workflows that are usually completely worthless instead of just mastering a handful of tried and true value generators. Hope this helps.",
            "header": "How To SelfHost n8n On Railway, Setup Community Nodes, and Integrate FireCrawl MCP Server without touching a Single Line Of Code !",
            "link": "https://old.reddit.com/r/n8n/comments/1lyd7pe/four_types_of_ai_systems_that_actually_sell_and/",
            "Extracted Links": "[\"https://www.youtube.com/watch?v=bFxWRkWAFzs\", \"https://www.youtube.com/watch?v=Vmgmva3dL44\", \"https://www.youtube.com/watch?v=mzxdtDCZbNY\", \"https://www.youtube.com/watch?v=Qp3h7WLYpH0\", \"https://www.youtube.com/watch?v=nwR5519zTC8\"]"
          }
        }
      },
      {
        "json": {
          "id": "recjGR2lgT4xi4ocF",
          "createdTime": "2025-07-24T10:38:01.000Z",
          "fields": {
            "Name": "Silent-Willow-7543",
            "body_text": "In this step-by-step tutorial, I show you how to build a fully automated workflow using AI Agents and n8n to generate geography explainer videos on autopilot ‚Äî perfect for teachers, creators, or anyone looking to share knowledge at scale. What you‚Äôll learn:\n‚úÖ Generate video scripts with AI Agents\n‚úÖ Automatically source images, videos, and maps\n‚úÖ Assemble and edit videos hands-free\n‚úÖ Publish educational content consistently This workflow saves hours and helps you teach the world smarter without coding or manual editing. Tools used: LLM model (Gemini)\nGoogle APIs (Image generation)\nVideo generation: Runway ML\nVoice over (Elevenlabs)\nCaptions (Elevenlabs)\nFFmeg for combining the audio, video and captions into one MP4 file Sample video - Life in the Amazon: Uncontacted Tribes https://youtu.be/EZySnlX_yyQ",
            "header": "Workflow - Code Included",
            "link": "https://old.reddit.com/r/n8n/comments/1lt91sn/i_built_an_n8n_ai_agent_that_produces_educational/",
            "Extracted Links": "[\"https://youtu.be/EZySnlX_yyQ\"]"
          }
        }
      },
      {
        "json": {
          "id": "reccms6epZRvBhwwF",
          "createdTime": "2025-07-24T10:42:24.000Z",
          "fields": {
            "Name": "gochapachi1",
            "body_text": "Workflow Do give an upvote you guys, Discover how to create a professional AI news reporter video using an automated n8n workflow! In this video, we demonstrate an end-to-end process that integrates various AI tools and automated video editing techniques to produce a fully polished news video. Here's what you'll learn: AI Video Model Generation: Automatically generate realistic video models using AI. AI Audio Creation: Generate high-quality AI audio for the model with perfect lipsync. AI Music Generation: Create custom background music using AI to add the perfect vibe to your video. Automated Editing & Transitions: Utilize advanced video editing techniques and seamless transitions with ffmpeg integrated into the n8n workflow. Complete End-to-End Automation: Watch as the entire process‚Äîfrom content creation to final editing‚Äîis fully automated, saving time and effort. Whether you're a content creator, media professional, or just curious about the power of automation and AI, this workflow offers a glimpse into the future of video production. Workflow:- https://github.com/gochapachi/AI-news-Reporter Youtube :- https://youtu.be/Km2u6193pDU If you enjoyed this video, please like, comment, and subscribe for more content on AI-driven automation and innovative video production techniques. Let's revolutionize content creation with AI and automation! üëâ Follow Us on Social Media for More Updates: üß† Reddit: https://www.reddit.com/user/gochapachi1/ üìò Facebook: https://facebook.com/gochapachi/ üì∏ Instagram: https://www.instagram.com/gochapachi/ üé• YouTube: https://www.youtube.com/@gochapachi üíº LinkedIn: https://www.linkedin.com/in/gochapachi/ üìû whatsapp: +91-8400210108 üì© Email:¬†[ sanjeevcs0034@gmail.com ](mailto: sanjeevcs0034@gmail.com )",
            "header": "Tutorial",
            "link": "https://old.reddit.com/r/n8n/comments/1j7vk1j/ai_news_reporter_ai_video_ai_audio_ai_music_ai/",
            "Extracted Links": "[\"https://github.com/gochapachi/AI-news-Reporter\", \"https://youtu.be/Km2u6193pDU\"]"
          }
        }
      },
      {
        "json": {
          "id": "rechPnyUPBLC5oVwF",
          "createdTime": "2025-07-24T10:42:24.000Z",
          "fields": {
            "Name": "croos-sime",
            "body_text": "This is how you can empower your AI Agents using MCPs (Model Context Protocols). MCPs are a great way to extract and reuse logic between different agents. If you're reading this and you're a developer, a quick comparison would be with a microservice or an API: you encapsulate specific logic within an MCP. If you're not, it's a very cool way to save code (rare to say this in a no-code tool) and reuse it. It's recommended that each MCP does one thing well, that it be a service focused on a single functionality. For example, one MCP specializes in Reddit-related actions, another for Calendar, and another for Supabase. A very powerful approach to starting to implement MCPs in your agents (especially since it's so easy to integrate with N8n) is to leverage this tool as the primary means of communication with the agents. It's the best way to do it because each MCP can clearly explain what it needs, and if you follow a pattern like the one I use (where each MCP has multiple actions, and each action requires a specific JSON), implementation becomes much simpler. I've been developing software for over eight years, and in my experience, this clear, modular structure allows you to maintain scalability and order as your system grows. It's a much more organized alternative than connecting 10 different nodes for each functionality within the agent. You'll simplify the structure, reduce friction in prompts, and make the agent's job easier. This is an agent I use daily because I need to search for content on Reddit, read comments, keep my database up to date, and always know if I have an upcoming meeting or class. It's tailored to my real problems and needs. But you can create as many agents as you want, and the most interesting thing is that MCPs can be reused across different agents, which increases their value. The approach I use for each MCP is that it calls a node in N8n that executes another workflow. All MCPs call the same base workflow, but they are passed a value called an action. This action determines which path the workflow will take when processing the request. Each workflow may require a different JSON, and when writing the description of the MCP (and its actions) for the agent, that content serves as a reference to know, based on the user's request, what action to use, how to use it, and what JSON to send to the MCP. This allows for flexible and precise interaction, without the need to code rigid paths for each feature. All code here: https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md In the link you have a youtube video to dive deep on it",
            "header": "How I turned n8n automation into software with Supabase and Windsurf",
            "link": "https://old.reddit.com/r/n8n/comments/1lvynu2/a_fully_modular_ai_agentfree_code_with_mcps_for/",
            "Extracted Links": "[\"https://github.com/simealdana/ai-agent-n8n-course/blob/main/Examples_extra/telegram_assistant_empowered_mcp/introduction.md\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec4rAprnVo8nKYRM",
          "createdTime": "2025-07-24T10:42:25.000Z",
          "fields": {
            "Name": "gnosjah",
            "body_text": "full workflow My customer recently asked me to build a fully automated content publishing system with WordPress and n8n. Naturally, I went looking for inspiration‚Äîbut to my surprise, there wasn‚Äôt a single public or usable WordPress + n8n basic workflow out there. So I rolled up my sleeves and built one from scratch. üíÄ It took more time than I expected, but the result was worth it: This workflow now runs daily, hands-free, and has already brought my client real traffic and positive ROI. They even started recommending it to others. That‚Äôs why I‚Äôve decided to share it with the community‚Äîso you can use it, improve it, or even build on it to do something better. I have posted the code here: https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json ‚∏ª Happy to give back to this amazing community üôå",
            "header": "Workflow - Code Not Included",
            "link": "https://old.reddit.com/r/n8n/comments/1m4tjkj/share_a_wordpress_autopublishing_workflow/",
            "Extracted Links": "[\"https://github.com/c137ff/n8nauts/blob/main/Automate_WordPress_Blog_Workflow.json\"]"
          }
        }
      },
      {
        "json": {
          "id": "recDKHA5DZkceWZsF",
          "createdTime": "2025-07-24T10:42:26.000Z",
          "fields": {
            "Name": "kyle4real",
            "body_text": "https://preview.redd.it/3gav577mn4ef1.png?width=1920&format=png&auto=webp&s=dd788d8ebfe15a113d6bcd973cba8162c95fcfc3 Hey r/n8n !! If you're into AI automation and content creation, I just dropped a tutorial on building an AI agent in n8n that analyzes YouTube channels on the fly. You chat with it about any channel, and it scrapes video data (views, likes, duration, etc.), saves it to a Google Sheet, and answers smart market analysis questions to help brainstorm better content strategies. Why it's cool for creators/businesses: Ask stuff like: \"What are this channel‚Äôs top 5 videos?\" \"Which ones went viral?\" \"Average engagement rate?\" \"Videos with unusual view-to-like ratios?\" or \"Which underperformed?\" It pulls insights you can't get from just browsing a channel's popular vids ‚Äì perfect for niche research or competitor spying. Handles multiple channels for comparisons, with optimizations to avoid redundant API calls. Video structure: Quick demo (00:53) High-level overview (02:13) Step-by-step build: Get Channel Info (04:18), Get Channel Data (09:13), AI Agent setup (19:16), System Prompt (22:40) Testing & thoughts (24:43) What you'll learn in n8n: Creating a chat-based AI agent (using OpenAI) Setting up multi-agent workflows Capturing YouTube data via APIs/HTTP nodes Reading/updating Google Sheets Connecting credentials for YouTube, Sheets, and OpenAI It's a great example of chaining n8n nodes for real-world AI tools ‚Äì from HTTP requests and code parsing to tool calling and prompts. Watch here: https://www.youtube.com/watch?v=h_oPOkDLw28 Download template/code here: https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI?usp=sharing If you build this, share your results or mods! Anyone used n8n for similar YouTube/API projects? Tips on scaling for big channels or adding more features? Let's chat ‚Äì open to suggestions for future vids too. Thanks for the support! üöÄ (Music creds in vid desc)",
            "header": "Why is reading 2 inputs so difficult in a code node?",
            "link": "https://old.reddit.com/r/n8n/comments/1m55gyt/built_an_ai_agent_in_n8n_for_youtube_market/",
            "Extracted Links": "[\"https://www.youtube.com/watch?v=h_oPOkDLw28\", \"https://drive.google.com/drive/folders/1h-dctyTDnmr1ouiDk66WAI9AvNgUk-bI\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec4pXWVFCzXn4e1F",
          "createdTime": "2025-07-24T10:38:04.000Z",
          "fields": {
            "Name": "kenmiranda",
            "body_text": "Snippet of the workflow I noticed an uptick in content here and in some of the other subreddits I follow. Like a lot of people, I kept thinking ‚ÄúI just want to see more of X instead of Y\". So I built an n8n workflow that pulls posts from your favorite subreddits, runs them through a filter using upvote ratio and AI Agents, and then sends the best ones to a Slack channel. It also summarizes longer posts and checks for duplicates using Supabase. I included the SQL DDL as this will be required for this setup. It's currently setup to use supabase as the database, Openai as the LLM, and slack as the communication app. Feel free to tear it apart and make it your own. Have fun! Here is the GitHub repo: https://github.com/kenandrewmiranda/reddit-content-filter-n8n Here is just the JSON: https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json Example of an output: https://preview.redd.it/q8kd44w9cicf1.png?width=1906&format=png&auto=webp&s=9735474a772c45980318a1ea3844b9f5d2a07534",
            "header": "N8n workflow for graphics/infographics with templates",
            "link": "https://old.reddit.com/r/n8n/comments/1lyb3nc/reddit_curation_automated/",
            "Extracted Links": "[\"https://github.com/kenandrewmiranda/reddit-content-filter-n8n\", \"https://github.com/kenandrewmiranda/reddit-content-filter-n8n/blob/main/reddit_content_flow.json\"]"
          }
        }
      },
      {
        "json": {
          "id": "recZQHVuEXwc6iNH6",
          "createdTime": "2025-07-24T10:38:04.000Z",
          "fields": {
            "Name": "dudeson55",
            "body_text": "I‚Äôve gotten a bunch of questions from a previous post I made about how I go about scraping Twitter / X data to generate my AI newsletter so I figured I‚Äôd put together and share a mini-tutorial on how we do it. Here's a full breakdown of the workflow / approaches to scrape Twitter data This workflow handles three core scraping scenarios using Apify's tweet scraper actor ( Tweet Scraper V2 ) and saves the result in a single Google Sheet (in a production workflow you should likely use a different method to persist the tweets you scrape) 1. Scraping Tweets by Username Pass in a Twitter username and number of tweets you want to retrieve The workflow makes an HTTP POST request to Apify's API using their \"run actor synchronously and get dataset items\" endpoint I like using this when working with Apify because it returns results in the response of the initial http request. Otherwise you need to setup a polling loop and this just keeps things simple. Request body includes maxItems for the limit and twitterHandles as an array containing the usernames Results come back with full tweet text, engagement stats (likes, retweets, replies), and metadata All scraped data gets appended to a Google Sheet for easy access ‚Äî This is for example only in the workflow above, so be sure to replace this with your own persistence layer such as S3 bucket, Supabase DB, Google Drive, etc Since twitterHandles is an array, this can be easily extended if you want to build your own list of accounts to scrape. 2. Scraping Tweets by Search Query This is a very useful and flexible approach to scraping tweets for a given topic you want to follow. You can really customize and drill into a good output by using twitter‚Äôs search operations. Documentation link here: https://developer.x.com/en/docs/x-api/v1/rules-and-filtering/search-operators Input any search term just like you would use on Twitter's search function Uses the same Apify API endpoint (but with different parameters in the JSON body) Key difference is using searchTerms array instead of twitterHandles I set onlyTwitterBlue: true and onlyVerifiedUsers: true to filter out spam and low-quality posts The sort parameter lets you choose between \"Top\" or \"Latest\" just like Twitter's search interface This approach gives us much higher signal-to-noise ratio for curating content around a specific topic like ‚ÄúAI research‚Äù 3. Scraping Tweets from Twitter Lists This is my favorite approach and is personally the main one we use to capture and save Tweet data to write our AI Newsletter - It allows us to first curate a list on twitter of all of the accounts we want to be included. We then pass the url of that twitter list into the request body that get‚Äôs sent to apify and we get back a list of all tweets from users who are on that list. We‚Äôve found this to be very effective when filtering out a lot of the noise on twitter and keeping costs down for number of tweets we have to process. Takes a Twitter list URL as input (we use our manually curated list of 400 AI news accounts) Uses the startUrls parameter in the API request instead of usernames or search terms Returns tweets from all list members in a single result stream Cost Breakdown and Business Impact Using this actor costs 40 cents per 1,000 tweets versus Twitter's $200 for 15,000 tweets a month. We scrape close to 100 stories daily across multiple feeds and the cost is negligible compared to what we'd have to pay Twitter directly. Tips for Implementation and working with Apify Use Apify's manual interface first to test your parameters before building the n8n workflow. You can configure your scraping settings in their UI, switch to JSON mode, and copy the exact request structure into your HTTP node. The \"run actor synchronously and get dataset items\" endpoint is much simpler than setting up polling mechanisms. You make one request and get all results back in a single response. For search queries, you can use Twitter's advanced search syntax to build more targeted queries. Check Apify's documentation for the full list of supported operators. Workflow Link + Other Resources YouTube video that walks through this workflow step-by-step: https://www.youtube.com/watch?v=otK0ILpn4GQ The full n8n workflow, which you can copy and paste directly into your instance, is on GitHub here: https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json",
            "header": "Novice question about changing a step in workflow",
            "link": "https://old.reddit.com/r/n8n/comments/1lrkam5/minitutorial_how_to_easily_scrape_data_from/",
            "Extracted Links": "[\"https://github.com/lucaswalter/n8n-ai-workflows/blob/main/twitter_x_scraping.json\", \"https://www.youtube.com/watch?v=otK0ILpn4GQ\"]"
          }
        }
      },
      {
        "json": {
          "id": "recoyRgrv6CO2xkRB",
          "createdTime": "2025-07-24T10:38:05.000Z",
          "fields": {
            "Name": "conor_is_my_name",
            "body_text": "TLDR: This Docker container gives you full visual control of Chrome with VNC access‚Äîperfect for scraping tricky sites, testing, or logged-in sessions. If you are new to web scraping this makes a lot of things easier! üîó GitHub Link: https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer Who is this for? Scrapers battling sites requiring logins, CAPTCHAs, or dynamic content. Developers who need to debug visually or automate complex interactions. Anyone who has wasted hours trying to make Puppeteer/Playwright work headlessly when a real browser would‚Äôve taken 5 minutes. (this is me) Stealth mode users who want the most realistic browser usage with minimal chance of detection. I made this because I wanted to do analysis on long form journalism articles.  All of my sources required logins to read the articles, and had pretty strong subscription and login checking protocols.  Even though I actually do pay for these subscriptions and have valid credentials, it was tricky to get the logins to work in headless mode. Basically, you can connect to a full GUI chrome running on a server, raspberry pi, even your own local machine, and then control it programmatically.  In my case, I remote into the GUI, log into the website as needed in a fully normal chrome browser instance, and then run my scripts. Use page.close() instead of browser.close() to end your scripts.  This will keep the browser open and ready for a new command. You will need to restart the container if you pass a browser.close() command. Why this beats headless mode: Full Chrome GUI in a container‚Äîjust like your local browser, but remote-controlled. VNC access (with audio support if needed). Pre-loaded with Puppeteer for scripting inside or outside the container. Persistent sessions (no more re-logging in every scrape). Downsides: S low R esource Heavy (but sometimes it doesn't matter: skipping login scripting and captchas can more than make up for a slow scraper) What‚Äôs inside? Chrome Stable (+ all dependencies). VNC for multiple remote access options. Puppeteer/Playwright-compatible ‚Äîuse your existing scripts. Easy volume mounts to save profiles/sessions. n8n json starter Install in 2 commands: git clone https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\n\ndocker compose up -d Then connect via VNC (default password: password ) Example n8n nodes are included: U pdate the IP address, everything else will be automatic. Use Code Node for your scripts.  This allows way more customization than using the community nodes. Tested on: 4GB+ RAM recommended for smooth VNC. Raspberry Pi 4 (ARM support included!). Here's the link to the ARM version Pair this with my n8n autoscaling setup for next-level workflow execution speed . Need n8n consulting? DM me for help with your projects.",
            "header": "Learn Faster: 8 Common Mistakes I Correct with Every Student I Mentor",
            "link": "https://old.reddit.com/r/n8n/comments/1kr506d/i_made_a_docker_chrome_with_full_vnc_access_for/",
            "Extracted Links": "[\"https://github.com/conor-is-my-name/Headful-Chrome-Remote-Puppeteer\"]"
          }
        }
      },
      {
        "json": {
          "id": "rec2R4AheAQ5dtqEG",
          "createdTime": "2025-07-24T10:42:28.000Z",
          "fields": {
            "Name": "Silent-Willow-7543",
            "body_text": "This workflow is designed to fully automate your AI-powered LoFi Girl YouTube channel, enabling you to scale content creation and grow your channel with minimal ongoing effort. It handles everything‚Äîfrom generating visuals and animations to composing music and posting directly to YouTube‚Äîso you can focus on strategy, branding, or other creative pursuits. The workflow leverages multiple AI tools and APIs to streamline each part of the content creation pipeline. First, it uses Google image APIs to generate unique background images based on prompt inputs or trending aesthetic themes. These images serve as the visual foundation for each video. Next, Runway ML‚Äôs API is integrated to animate the LoFi Girl and other scene elements, bringing your visuals to life with smooth, cinematic motion. This part of the workflow adds a professional polish to your videos and helps differentiate your content in a crowded niche. Note that while Runway ML offers a powerful animation engine, it is the only paid tool in this stack. For music, the workflow connects to Segmind, a free AI tool that generates LoFi-style music tracks. These tracks are unique, royalty-free, and tuned to the vibe you specify, making it easy to build a consistent sonic brand across your uploads. Finally, the entire asset bundle‚Äîimage, animation, and audio‚Äîis compiled into a video. Using YouTube‚Äôs Data API, the workflow can auto-upload the video with relevant metadata, thumbnails, and scheduling options. You can even predefine tags and titles for SEO optimization. Here‚Äôs an example output from the workflow, showing how all these tools work together to produce a high-quality LoFi video‚Äîautomatically.\n - https://youtu.be/ISNGL9IvQik",
            "header": "Mini-Tutorial: How to easily scrape data from Twitter / X using Apify",
            "link": "https://old.reddit.com/r/n8n/comments/1leypz5/ai_lofi_girl_youtube_channel_automation/",
            "Extracted Links": "[\"https://youtu.be/ISNGL9IvQik\"]"
          }
        }
      },
      {
        "json": {
          "id": "recc4kYMtLXHLdjEc",
          "createdTime": "2025-07-24T10:38:06.000Z",
          "fields": {
            "Name": "Valuable-Range-6785",
            "body_text": "Hi everyone, I‚Äôve been automating my content creation and distribution workflow lately, and I thought I‚Äôd share something that might help those of you building with AI + no-code tools. A few days ago I created a system that: Generates faceless, illustrated AI videos automatically Schedules & posts them to all major social platforms (YouTube Shorts, TikTok, Instagram Reels, LinkedIn) Does 100% for free using open-source and free-tier tools Powered by n8n , with triggers, GPT prompts, video-generation, and posting all set up in a workflow I go through: How to set up your n8n environment (no server, no subscription) How to generate the visuals, script, and voice from text How to stitch the video together and post automatically Customizations: branding, posting cadence, scheduling logic For anyone looking to build a hands-free content pipeline or learn how to combine AI + no-code, this could be a helpful reference. The setup runs entirely on the free tier of tools! Watch the full tutorial here: üëâ https://youtu.be/TMGsnqit6o4?si=Y7sxXSV7y4yZ0D0p",
            "header": "I built an agentic recruiting pipeline with n8n that analyzes resumes against job requirements - here's how",
            "link": "https://old.reddit.com/r/n8n/comments/1luuwkp/how_i_built_a_100_free_aipowered_faceless_video/",
            "Extracted Links": "[\"https://youtu.be/TMGsnqit6o4\"]"
          }
        }
      },
      {
        "json": {
          "id": "recO6WF9qPsMyqdSA",
          "createdTime": "2025-07-24T10:38:07.000Z",
          "fields": {
            "Name": "ComfortableArm121",
            "body_text": "I got tired of editing my essays on chatgpt because it was annoying to pinpoint the corrections. Now, I just get a comment of what could be better, and I can choose to accept or deny the edit. I generated this with https://alpha.osly.ai/ and I've attached the code here https://github.com/Osly-AI/google-doc-editor .",
            "header": "Discussion",
            "link": "https://old.reddit.com/r/n8n/comments/1m1oqa9/i_built_a_workflow_that_creates_edit_comments_for/",
            "Extracted Links": "[\"https://github.com/Osly-AI/google-doc-editor\"]"
          }
        }
      },
      {
        "json": {
          "id": "recp9HO1ntxDcY4x0",
          "createdTime": "2025-07-24T10:42:29.000Z",
          "fields": {
            "Name": "Milan_SmoothWorkAI",
            "body_text": "Discover how to easily extend an AI-generated video from a single image using KAI.AI , VEO3, and automation tools like NCA Toolkit and FFmpeg. Learn how to automate video sequences, extract key frames, and create longer, more dynamic videos ‚Äî perfect for creative projects or content creation! Video: https://www.youtube.com/watch?v=qc00jJ7bCBA Workflow: https://pub-7710cf36e9f24295acffe6938f40f147.r2.dev/Extend%20Video%20with%20NCA%20Toolkit.json",
            "header": "Question",
            "link": "https://old.reddit.com/r/n8n/comments/1lvht0i/i_built_an_automation_to_autoorganize_my_inbox/",
            "Extracted Links": "[\"https://www.youtube.com/watch?v=qc00jJ7bCBA\"]"
          }
        }
      },
      {
        "json": {
          "id": "recPYfSZSSS66knt3",
          "createdTime": "2025-07-24T10:42:30.000Z",
          "fields": {
            "Name": "Legitimate_Fee_8449",
            "body_text": "I automated a full AI-powered video pipeline that creates faceless vertical stories like the miniature worker / tiny character genre you‚Äôre seeing blow up right now. Why? Because writing, designing, animating, soundtracking, and publishing even one of these manually is a multi-hour grind - and I wanted to publish daily without touching CapCut. So I built an end-to-end workflow in n8n that does it all, 100% automated. Perfect for: AI-generated short stories & mini animations YouTube Shorts / Instagram Reels / TikTok Faceless vertical video channels (well you can adapt it for 16:9 as well - change the aspect ratio) Clients who want storytelling content with AI flair Follow along this tutorial here. Explanation here : https://youtu.be/Z1n6nU9O0BA Example video with the lowest settings https://youtube.com/shorts/LN46liFamoY üß† How It Works The system turns a scheduled trigger into a fully animated AI-generated story , complete with voiceover, visuals, and sound FX, and background music. Here‚Äôs the full 8-step pipeline: 1. Story Idea Generation GPT-4 creates original video ideas e.g., ‚ÄúTiny workers making a large pizza‚Äù 2. Scene Breakdown + Visual Planning Each story is split into multiple scenes with structured prompts and reference descriptions. 3. Reference + Scene Images via FAL / FreePik FAL‚Äôs image generation API creates high-quality scene visuals - one per segment, based on the story and setting. In the workflow you have option for using fal/general for using lora's and controlnets if you wish. Freepik integration is also there - Started with this, kept in the workflow (good images, but slow generation) - absolutely free image generation - with rate limiting of course. 4. Scene Animation via Kling  - or Minimax (ran out of minimax's $50 dollar credit so Swtiched to Kling via Fal) Each image is passed to an animation engine that gives movement, transitions, and subtle effects (like camera pan or zoom) for that short-form storytelling vibe. 5. Bacground music + SFX from ElevenLabs AI-generated voiceover narrates the story, matched with ambient music and sound effects, all created using ElevenLabs. 6. Video Compilation via FFmpeg A custom command stitches the scenes, SFX, and music together, handles crossfades, syncs timing, and applies volume balancing. No JSON2Video or Creatomate - they can become expensive. 7. YouTube Upload The finished video is uploaded directly to YouTube Shorts via resumable upload. Upload to reels and Tiktok (coming soon) 8. Logged in Google Sheets Each published video is logged for tracking, repurposing, or reuse. üîß Tools Used Function Tool Idea & Script OpenAI GPT-4 Images & Scenes FAL (Flux) with lora support Animation Hailuo API / Kling Voice + SFX ElevenLabs Stitching FFmpeg Automation n8n Publishing YouTube API Tracking Google Sheets ‚öôÔ∏è Key Features No video editing required. Fully modular, works scene-by-scene. Customize prompt style for any genre (horror, fantasy, sci-fi, slice of life). Built in n8n , no-code setup. Ideal for scaling faceless content at volume. AI is not perfect always, it will make mistakes üöÄ Why This Works Saves 10 - 15 hours/week Produces daily, high-quality, AI-first content Scalable for multiple channels or client brands Zero editing tools or manual uploads needed Great for storytelling niches, explainer content, or abstract visual art Explanation here: https://youtu.be/Z1n6nU9O0BA",
            "header": "Discussion",
            "link": "https://old.reddit.com/r/n8n/comments/1lebpkb/from_reddit_complaint_to_saas_idea_in_5_minutes/",
            "Extracted Links": "[\"https://youtu.be/Z1n6nU9O0BA\"]"
          }
        }
      },
      {
        "json": {
          "id": "recCDCAWMT4rLK6WK",
          "createdTime": "2025-07-24T10:42:30.000Z",
          "fields": {
            "Name": "automayweather",
            "body_text": "Just built a seamless automation using n8n and the Veo 3 Fast API from Ulazai.com. Automatically generates AI videos in seconds Cheapest pricing available for Veo 3 Fast (just $1 per video!) No KYC, instant API access Supports the new vertical (9:16) aspect ratio Bonus: use OpenAI‚Äôs image generation without organizational verification‚Äîespecially helpful now after OpenAI‚Äôs recent model update! ulazai.com - VEO 3 Fast Ideal for automation enthusiasts, marketers, and developers looking to leverage cutting-edge AI content creation without complex setup. üëâ Quick-start guide & API docs + Workflow**:** https://ulazai.com download the workflow: https://gist.github.com/smrht/82c072241a679ae827421a1c61fd35b9",
            "header": "Create Powerful AI Videos with Veo 3 Fast in n8n ‚Äì No KYC or Org Verification Needed!",
            "link": "https://old.reddit.com/r/n8n/comments/1m1octi/create_powerful_ai_videos_with_veo_3_fast_in_n8n/",
            "Extracted Links": "[]"
          }
        }
      },
      {
        "json": {
          "id": "recE50SDR7yHUMBVC",
          "createdTime": "2025-07-24T10:42:31.000Z",
          "fields": {
            "Name": "Horizon-Dev",
            "body_text": "Hey everyone! I've been experimenting with n8n and AI automation to streamline the process of creating YouTube Shorts , and I wanted to share my results. In this video, I walk through how I: ‚úÖ Used n8n to automate the entire workflow ‚úÖ Integrated AI tools to generate content dynamically ‚úÖ Automated script creation ‚úÖ Grabbed Videos to clip together ‚úÖ Generated voice, and grabbed music ‚úÖ Optimized the system to make the process scalable If you guys have questions, or would do something differently let me know! üîó Watch the full breakdown here: https://youtu.be/g3XsT_55xTA",
            "header": "Built my first advanced flow! Help me simplify it?",
            "link": "https://old.reddit.com/r/n8n/comments/1i66chs/i_used_ai_and_n8n_to_automate_youtube_shorts/",
            "Extracted Links": "[\"https://youtu.be/g3XsT_55xTA\"]"
          }
        }
      },
      {
        "json": {
          "id": "recFWL5GO4emuzv19",
          "createdTime": "2025-07-24T10:42:31.000Z",
          "fields": {
            "Name": "automayweather",
            "body_text": "I just built an automation that turns Trustpilot reviews into actual lead gen opportunities ‚Äî especially useful for freelancers, outreach agencies, and DFY reputation management services. üîß What it does (fully automated via n8n): Scans any Trustpilot profile (even unclaimed ones) Detects if the business already responded to reviews Uses Google Gemini (via OpenRouter) to write realistic, helpful replies Sends a clean HTML report via Gmail to the business Logs everything in Google Sheets with timestamps & outreach status Works with Dutch üá≥üá± and English üá¨üáß reviews (auto-detect) üí° Why it matters: Most ecommerce stores don‚Äôt reply to reviews at all. That‚Äôs a huge reputational gap ‚Äî and a perfect entry point for value-based outreach. Instead of spamming \"hey, want SEO?\", you send: üì¨ ‚ÄúHere‚Äôs what your customers said. Here‚Äôs how you could reply. Want to automate it?‚Äù ‚Üí Value first. Pitch second. üí• Built with: n8n (open source Zapier alternative) OpenRouter (Google Gemini model) Gmail Google Sheets üìΩÔ∏è Loom walkthrough + full workflow template available. https://reddit.com/link/1lq3glp/video/0tte6vqi7iaf1/player {\n  \"name\": \"Review Responder & Content Trustpilot V2 Samautomation.work\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"operation\": \"appendOrUpdate\",\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"columns\": {\n          \"mappingMode\": \"defineBelow\",\n          \"value\": {\n            \"TrustPilotUrL\": \"={{ $('Limit').item.json.TrustPilotUrL }}\",\n            \"Claimed\": \"={{ $('Limit').item.json.Claimed }}\",\n            \"Contacted\": \"={{ $now }}\",\n            \"Email\": \"={{ $('Limit').item.json.Email }}\"\n          },\n          \"matchingColumns\": [\n            \"TrustPilotUrL\"\n          ],\n          \"schema\": [\n            {\n              \"id\": \"TrustPilotUrL\",\n              \"displayName\": \"TrustPilotUrL\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Claimed\",\n              \"displayName\": \"Claimed\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Added\",\n              \"displayName\": \"Added\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Contacted\",\n              \"displayName\": \"Contacted\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            },\n            {\n              \"id\": \"Email\",\n              \"displayName\": \"Email\",\n              \"required\": false,\n              \"defaultMatch\": false,\n              \"display\": true,\n              \"type\": \"string\",\n              \"canBeUsedToMatch\": true,\n              \"removed\": false\n            }\n          ],\n          \"attemptToConvertTypes\": false,\n          \"convertFieldsToString\": false\n        },\n        \"options\": {\n          \"useAppend\": true\n        }\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        2520,\n        -180\n      ],\n      \"id\": \"d0c3ea12-2a69-47ba-822a-b79f5eb4a44c\",\n      \"name\": \"Update Row Geplaatst\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"operation\": \"extractHtmlContent\",\n        \"extractionValues\": {\n          \"values\": [\n            {\n              \"key\": \"article_html\",\n              \"cssSelector\": \".styles_reviewCard__Qwhpy\",\n              \"returnValue\": \"html\",\n              \"returnArray\": true\n            },\n            {\n              \"key\": \"email_html\",\n              \"cssSelector\": \"a[href^=\\\"mailto:\\\"]\"\n            },\n            {\n              \"key\": \"unclaimed_profile\",\n              \"cssSelector\": \"div[class*=\\\"styles_label\\\"]\",\n              \"returnArray\": true\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"8ecff1e8-b4ee-49b4-9441-7e8d83f1c676\",\n      \"name\": \"Extract Article URLs\",\n      \"type\": \"n8n-nodes-base.html\",\n      \"position\": [\n        2000,\n        -540\n      ],\n      \"typeVersion\": 1.2\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"data\"\n            }\n          ]\n        },\n        \"options\": {\n          \"mergeLists\": true\n        }\n      },\n      \"id\": \"6c617d6b-1651-4d62-b77b-b18bf86698e4\",\n      \"name\": \"Combine Articles\",\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"position\": [\n        2420,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"html\": \"={{ $json.article_html }}\",\n        \"options\": {}\n      },\n      \"id\": \"88aa1a9b-ebd0-4e87-afd6-b5ae6007e426\",\n      \"name\": \"Markdown\",\n      \"type\": \"n8n-nodes-base.markdown\",\n      \"position\": [\n        2220,\n        -540\n      ],\n      \"typeVersion\": 1\n    },\n    {\n      \"parameters\": {\n        \"url\": \"={{ $json.UpdatedTrustpilotURL }}\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        1780,\n        -540\n      ],\n      \"id\": \"0c289d7a-b90b-4a3d-974e-6f20babd9384\",\n      \"name\": \"HTTP Request1\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Je reageert als een echt persoon, niet als AI. Gebruik NOOIT volgende corporate/AI-zinnen in welke taal dan ook.\\n\\nINHOUD REVIEW:\\n{{ $json['data[0]'] }}\\n\\nInstructies:\\n- Reageer namens de webshop\\n- Gebruik dezelfde taal als de review\\n- Ga concreet in op de aangehaalde punten\\n- Houd het professioneel √©n persoonlijk\\n- Lengte: 2-4 zinnen\\n- Gebruik geen typische AI-woorden!\\n\\nGeef alleen de tekst van de reactie terug, zonder verdere opmaak.\\n\",\n        \"options\": {\n          \"systemMessage\": \"=Je bent een warme, persoonlijke klantenservicemedewerker voor Nederlandse webwinkels. Schrijf korte, oprechte reacties op klantbeoordelingen in vloeiend Nederlands.\\n\\n## EERSTE STAP - VERPLICHTE CONTROLE\\n\\n**DENK EERST NA:** Analyseer de ontvangen data grondig voordat je begint met schrijven.\\n\\n**CONTROLEER ALTIJD EERST:** Is er al een reactie van het bedrijf aanwezig in de data? \\n- Zoek naar termen zoals: \\\"reactie van\\\", \\\"antwoord van\\\", \\\"bedrijf reageerde\\\", \\\"webshop antwoordt\\\", of vergelijkbare tekst die duidt op een bestaande bedrijfsreactie\\n- Zoek naar gestructureerde reacties die niet van de klant afkomstig zijn\\n- Let op tekst die duidelijk een professioneel antwoord van de webshop is\\n\\n**ALS ER AL EEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Stop onmiddellijk met verder analyseren\\n- Schrijf geen nieuwe reactie\\n- GEEF ALLEEN DEZE OUTPUT: \\\"{bedrijfsnaam} heeft al gereageerd.\\\"\\n\\n**ALS ER GEEN BEDRIJFSREACTIE AANWEZIG IS:**\\n- Ga verder met het schrijven van een nieuwe reactie volgens onderstaande regels\\n\\n## BELANGRIJKE REGELS\\n\\n- Gebruik nooit een em-dash (‚Äî); gebruik een gewoon koppelteken (-)\\n- Laat elke reactie natuurlijk klinken ‚Äì nooit als AI\\n- Wissel begroetingen en afsluitingen af - nooit 2x dezelfde achter elkaar\\n\\n**Negatieve reviews:**\\n- Als {{ $('Limit').item.json.Email }} een waarde heeft ‚Üí eindig de reactie met: \\\"Mail ons op {{ $('Limit').item.json.Email }} zodat we je direct verder kunnen helpen.\\\"\\n- Heeft het veld g√©√©n waarde ‚Üí noem geen e-mail\\n\\n## SCHRIJFSTIJL\\n\\n- Begin met \\\"Beste [Naam]\\\". Geen naam? Gebruik \\\"Beste klant\\\"\\n- Verwerk concrete details uit de review (product, datum, levering, enz.)\\n- Toon oprechte empathie, vermijd corporate jargon\\n- Gebruik dezelfde aanspreekvorm (je/jij of u) als de klant\\n- Sluit af met een uitnodiging of hulpaanbod - wissel dit af\\n\\n## STRUCTUUR (2-4 zinnen)\\n\\n1. **Begroet & bedank:** \\\"Beste [Naam], dank voor ‚Ä¶\\\" - Wees creatief en varieer\\n2. **Reageer specifiek:** Ga in op de concrete punten uit de review\\n3. **Toon waardering/empathie:** \\n   - Positief ‚Üí benadruk blijdschap\\n   - Negatief ‚Üí erken probleem, bied oplossing (+ e-mail indien beschikbaar)\\n4. **Nodig uit tot vervolg:** Varieer je afsluitingen\\n\\n## RESULTAAT\\n\\nLever alleen de tekst van de reactie, zonder verdere opmaak; 2-4 zinnen, warm, menselijk en in helder Nederlands.\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        820,\n        -180\n      ],\n      \"id\": \"466305a5-312e-4080-95ad-a2785fe953e2\",\n      \"name\": \"Review Responder\",\n      \"executeOnce\": false\n    },\n    {\n      \"parameters\": {\n        \"fieldToSplitOut\": \"data[0]\",\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.splitOut\",\n      \"typeVersion\": 1,\n      \"position\": [\n        2620,\n        -540\n      ],\n      \"id\": \"a8cd687e-db79-46c9-a2dc-e11e6ad10504\",\n      \"name\": \"Split Out\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// Haal de originele URL en het gewenste paginanummer op\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\nconst pageNumber = 1;\\n\\n// Controleer of de originele URL al een paginaquery bevat\\nlet updatedUrl;\\nif (originalUrl.includes('?page=')) {\\n  // Vervang bestaande pagina-parameter\\n  updatedUrl = originalUrl.replace(/(\\\\?page=)(\\\\d+)/, `$1${pageNumber}`);\\n} else if (originalUrl.includes('?')) {\\n  // Voeg toe als er al andere queryparameters zijn\\n  updatedUrl = `${originalUrl}&page=${pageNumber}`;\\n} else {\\n  // Voeg toe als er nog geen queryparameters zijn\\n  updatedUrl = `${originalUrl}?page=${pageNumber}`;\\n}\\n\\n// Geef de nieuwe URL terug in de output\\nreturn [\\n  {\\n    json: {\\n      ...$json,\\n      UpdatedTrustpilotURL: updatedUrl\\n    }\\n  }\\n];\\n\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1580,\n        -540\n      ],\n      \"id\": \"ed03ac93-fc67-4e54-b1b7-07cfcac5104d\",\n      \"name\": \"Add Page to URL\"\n    },\n    {\n      \"parameters\": {\n        \"fieldsToAggregate\": {\n          \"fieldToAggregate\": [\n            {\n              \"fieldToAggregate\": \"output\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.aggregate\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1180,\n        -180\n      ],\n      \"id\": \"5b9793b6-a45b-496f-aa4a-797450ea6f52\",\n      \"name\": \"Aggregate\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üìà {{ $json.bedrijfsnaam }} klantrespons rapport - Gratis\",\n        \"message\": \"={{ $('HTML parser').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -20\n      ],\n      \"id\": \"2021c9b1-c1db-4580-a05e-ef71a7a27d7e\",\n      \"name\": \"Send Review Respsonse\",\n      \"webhookId\": \"32b66b21-c68d-4386-b1db-3a11f8e9e2ea\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"rule\": {\n          \"interval\": [\n            {\n              \"field\": \"minutes\",\n              \"minutesInterval\": 15\n            }\n          ]\n        }\n      },\n      \"type\": \"n8n-nodes-base.scheduleTrigger\",\n      \"typeVersion\": 1.2,\n      \"position\": [\n        800,\n        -540\n      ],\n      \"id\": \"1f2df98b-7a80-425d-9831-cdb2d9825b97\",\n      \"name\": \"Schedule Trigger\"\n    },\n    {\n      \"parameters\": {\n        \"documentId\": {\n          \"__rl\": true,\n          \"value\": \"1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\",\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Trustpilot Reviews\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit?usp=drivesdk\"\n        },\n        \"sheetName\": {\n          \"__rl\": true,\n          \"value\": 178011805,\n          \"mode\": \"list\",\n          \"cachedResultName\": \"Leads NL\",\n          \"cachedResultUrl\": \"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM/edit#gid=178011805\"\n        },\n        \"filtersUI\": {\n          \"values\": [\n            {\n              \"lookupColumn\": \"Claimed\",\n              \"lookupValue\": \"Geclaimd profiel\"\n            },\n            {\n              \"lookupColumn\": \"Contacted\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.googleSheets\",\n      \"typeVersion\": 4.6,\n      \"position\": [\n        1000,\n        -540\n      ],\n      \"id\": \"69b6abc7-6df7-4f7a-a01d-1dab75c550b0\",\n      \"name\": \"Google Sheets\",\n      \"credentials\": {\n        \"googleSheetsOAuth2Api\": {\n          \"id\": \"Iu6Q3jIAAfXvDuGi\",\n          \"name\": \"Google Sheets Seosshopify\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1360,\n        60\n      ],\n      \"id\": \"4fc10a76-c8a3-4651-8e4e-733db12b624e\",\n      \"name\": \"OpenRouter Chat Model2\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"={{ $('Customer Response Aggregatio').item.json.output }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Nederlandse Klantrespons Analyse Agent (JSON Output)\\n\\n\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1960,\n        -180\n      ],\n      \"id\": \"74977264-1c23-4b4a-ad54-9bd0894df187\",\n      \"name\": \"International Customer Response Analysis Agent\"\n    },\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=Reviews: {{ $json.output }}\\nExtract van de trustpilot URL de bedrijfsnaam: {{ $('Limit').first().json.TrustPilotUrL }}\",\n        \"options\": {\n          \"systemMessage\": \"=# Klantrespons-Aggregatie-Specialist met Gepersonaliseerde Ice Breaker\\n\\n## Rol: \\nn```\"\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1380,\n        -180\n      ],\n      \"id\": \"adac1c81-9ccf-4725-883a-f351d8ee7b83\",\n      \"name\": \"Customer Response Aggregatio\"\n    },\n    {\n      \"parameters\": {\n        \"conditions\": {\n          \"options\": {\n            \"caseSensitive\": true,\n            \"leftValue\": \"\",\n            \"typeValidation\": \"strict\",\n            \"version\": 2\n          },\n          \"conditions\": [\n            {\n              \"id\": \"d83b559f-1dfb-414c-9365-69e7193b77af\",\n              \"leftValue\": \"={{ $json.Email }}\",\n              \"rightValue\": \"NO EMAIL\",\n              \"operator\": {\n                \"type\": \"string\",\n                \"operation\": \"notEquals\"\n              }\n            }\n          ],\n          \"combinator\": \"and\"\n        },\n        \"options\": {}\n      },\n      \"type\": \"n8n-nodes-base.if\",\n      \"typeVersion\": 2.2,\n      \"position\": [\n        1180,\n        -540\n      ],\n      \"id\": \"1b53040d-986f-4e2f-8d42-788481c65a8b\",\n      \"name\": \"If\"\n    },\n    {\n      \"parameters\": {},\n      \"type\": \"n8n-nodes-base.limit\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1400,\n        -660\n      ],\n      \"id\": \"24401a8f-8657-411b-9723-a369ecb56efb\",\n      \"name\": \"Limit\"\n    },\n    {\n      \"parameters\": {\n        \"content\": \"## Create Subworkflow\\n\\nStuur de NO EMAIL naar de subworkflow.\"\n      },\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1360,\n        -900\n      ],\n      \"typeVersion\": 1,\n      \"id\": \"22618369-e14f-4818-898f-89eea60d889a\",\n      \"name\": \"Sticky Note\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {\n          \"frequencyPenalty\": 0.5,\n          \"presencePenalty\": 0.7,\n          \"temperature\": 0.9\n        }\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        800,\n        40\n      ],\n      \"id\": \"e266310b-6362-47b0-b399-e5845a28f859\",\n      \"name\": \"OpenRouter Chat Model\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -300\n      ],\n      \"id\": \"d1de90ed-0b98-4534-8d0b-a08de0dbeac4\",\n      \"name\": \"Code\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// In Gmail node (JavaScript functie)\\nconst originalUrl = $input.first().json.TrustPilotUrL;\\n\\n// Extract en clean de domain\\nconst cleanDomain = originalUrl\\n  .replace(/https?:\\\\/\\\\/(nl\\\\.)?trustpilot\\\\.com\\\\/review\\\\//, '') // Verwijder trustpilot deel\\n  .replace(/\\\\?.*$/, '') // Verwijder query parameters (?page=1 etc)\\n  .replace(/^www\\\\./, ''); // Verwijder www.\\n\\n// Extract bedrijfsnaam (zonder extensie voor mooiere naam)\\nconst bedrijfsnaam = cleanDomain\\n  .replace(/\\\\.(com|nl|dk|de|be|fr|co\\\\.uk)$/, '') // Verwijder extensies\\n  .replace(/\\\\./g, ' ') // Vervang punten met spaties (voor sub.domain.com)\\n  .split(' ')\\n  .map(word => word.charAt(0).toUpperCase() + word.slice(1)) // Capitalize elk woord\\n  .join(' ');\\n\\n// Email subject opties (kies je favoriet):\\nconst emailSubject = `üéÅ Gratis klantfeedback analyse voor ${bedrijfsnaam}`;\\n// Of: `üìä ${bedrijfsnaam}: Jouw klantreviews geanalyseerd`\\n// Of: `üöÄ ${bedrijfsnaam} - Gratis inzicht in je klantfeedback`\\n\\n// Set voor gebruik in email\\nreturn {\\n  originalUrl: originalUrl,\\n  cleanDomain: cleanDomain,        // \\\"2trendy.dk\\\" \\n  bedrijfsnaam: bedrijfsnaam,      // \\\"2trendy\\\"\\n  emailSubject: emailSubject       // \\\"üéÅ Gratis klantfeedback analyse voor 2trendy\\\"\\n};\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2880,\n        -20\n      ],\n      \"id\": \"63fb494e-6878-4fed-b70d-37075f81cb3c\",\n      \"name\": \"Code1\"\n    },\n    {\n      \"parameters\": {\n        \"sendTo\": \"={{ $('Extract Article URLs').first().json.email_html }}\",\n        \"subject\": \"=üîç Dit zeggen klanten over {{ $json.bedrijfsnaam }}\",\n        \"message\": \"={{ $('Code2').first().json.html }}\",\n        \"options\": {\n          \"appendAttribution\": false,\n          \"bccList\": \"support@samautomation.work\",\n          \"replyTo\": \"support@samautomation.work\"\n        }\n      },\n      \"type\": \"n8n-nodes-base.gmail\",\n      \"typeVersion\": 2.1,\n      \"position\": [\n        3160,\n        -300\n      ],\n      \"id\": \"e5491ac2-0fc7-4f4e-9b52-2ccc26de78ff\",\n      \"name\": \"Review Analyse Rapport\",\n      \"webhookId\": \"b70f3f19-ee55-4572-9445-abe53056511c\",\n      \"credentials\": {\n        \"gmailOAuth2\": {\n          \"id\": \"V4IPSlv4eL3pfLyj\",\n          \"name\": \"Gmail Rimas J\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"description\": \"Use the tool to think about and analyze the review data before responding. It will not obtain new information or change the database, but just append your analysis to the log.\\nUse it specifically for:\\n\\nReviewing data structure: Analyze the incoming review data to identify all components (reviewer name, content, rating, date, etc.)\\nDetecting existing responses: Check if there's already a business response present in the data before writing a new one\\nLanguage analysis: Determine the language and tone of the review to match it appropriately\\nContent analysis: Identify key points mentioned (products, service aspects, specific complaints/compliments)\\nStrategy planning: Decide on the appropriate response approach (positive reinforcement, problem resolution, etc.)\\nQuality checking: Review your planned response before finalizing to ensure it meets the guidelines\\n\\nAlways think first before writing any review response to ensure proper analysis and avoid duplicate responses.\"\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.toolThink\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1000,\n        40\n      ],\n      \"id\": \"f3a6ce09-2bea-4701-99e4-8b403a0f6957\",\n      \"name\": \"Think\"\n    },\n    {\n      \"parameters\": {\n        \"model\": \"google/gemini-2.5-flash\",\n        \"options\": {}\n      },\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOpenRouter\",\n      \"typeVersion\": 1,\n      \"position\": [\n        1940,\n        40\n      ],\n      \"id\": \"11a5d333-abb6-45a2-96b9-1b5efcdc7dc0\",\n      \"name\": \"OpenRouter Chat Model3\",\n      \"credentials\": {\n        \"openRouterApi\": {\n          \"id\": \"qYc830nbpHiUWKL0\",\n          \"name\": \"OpenRouter account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"//timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        1740,\n        -180\n      ],\n      \"id\": \"064c4a24-fe58-4944-b3fc-38b7d1750f6f\",\n      \"name\": \"HTML parser\"\n    },\n    {\n      \"parameters\": {\n        \"jsCode\": \"// \\n        timestamp: new Date().toISOString()\\n      }\\n    }\\n  ];\\n}\"\n      },\n      \"type\": \"n8n-nodes-base.code\",\n      \"typeVersion\": 2,\n      \"position\": [\n        2320,\n        -180\n      ],\n      \"id\": \"14b30bdd-c774-45b0-ad98-e9c3a14da488\",\n      \"name\": \"Code2\"\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Update Row Geplaatst\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Code1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Extract Article URLs\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Markdown\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Markdown\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Combine Articles\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTTP Request1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Extract Article URLs\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Combine Articles\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Split Out\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Split Out\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Responder\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Aggregate\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Add Page to URL\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTTP Request1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Aggregate\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Schedule Trigger\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Google Sheets\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Google Sheets\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"If\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model2\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Customer Response Aggregatio\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"International Customer Response Analysis Agent\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Code2\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Customer Response Aggregatio\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"HTML parser\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"If\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Limit\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ],\n        []\n      ]\n    },\n    \"Limit\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Add Page to URL\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Review Analyse Rapport\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Send Review Respsonse\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Review Analyse Rapport\": {\n      \"main\": [\n        []\n      ]\n    },\n    \"Think\": {\n      \"ai_tool\": [\n        [\n          {\n            \"node\": \"Review Responder\",\n            \"type\": \"ai_tool\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"OpenRouter Chat Model3\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"HTML parser\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"International Customer Response Analysis Agent\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Code2\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Update Row Geplaatst\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": true,\n  \"settings\": {\n    \"executionOrder\": \"v1\",\n    \"callerPolicy\": \"workflowsFromSameOwner\",\n    \"errorWorkflow\": \"SGT2yyLX0uoBgIBI\"\n  },\n  \"versionId\": \"895363e2-bac3-46f0-b678-f31d7b874eea\",\n  \"meta\": {\n    \"templateCredsSetupCompleted\": true,\n    \"instanceId\": \"190e1b81e9845aeb2f58812b2e7f9c8a86cb13b1e8883f6c2bfaa8050710ae79\"\n  },\n  \"id\": \"kuYaiJDh24Ta0NuG\",\n  \"tags\": []\n}",
            "header": "Is AI about to become the most groundbreaking productivity tool in human history, outpacing the wheel and the internet?",
            "link": "https://old.reddit.com/r/n8n/comments/1lq3glp/showoff_i_automated_trustpilot_outreach_review/",
            "Extracted Links": "[\"https://docs.google.com/spreadsheets/d/1gRuHYQt35Tq0CEK79ObUBW2_DidsISNgVUPerPVcJfM\"]"
          }
        }
      },
      {
        "json": {
          "id": "recDLjnqfvJ1F1qHW",
          "createdTime": "2025-07-24T10:42:32.000Z",
          "fields": {
            "Name": "paperjamai",
            "body_text": "Workflow code: https://jsonblob.com/1392186102303088640 YouTube video walkthrough: https://youtu.be/fVegeKYAzz8 Disclaimer: In order to use this workflow you'll have to subscribe to Upwork's Freelancer Plus plan, which is $20/mo and includes job alert email notifications (among other things). I assume most people who frequently work on Upwork are already subscribed, but don't want any surprises. --- When applying for freelance jobs on Upwork, minutes matter. The first quality application is more often than not the one that‚Äôs ultimately selected. Upwork's job alert emails are helpful, but filters are very limited. As a result, you often receive several dozen emails a day and it takes a lot of time to manually go through each email and determine if each job fits your criteria. I built a workflow that scans your Gmail every few minutes, finds all Upwork job alerts, scores them based on your profile/preferences, and sends a Slack channel message for jobs that are strong potential matches. I use this workflow myself and find that I've been applying to (and winning) more jobs than before, despite the fact that it filters out most alerts. The ones that come through are the right ones for me, and I have more confidence that I'm acting quickly and not wasting my time. Hopefully you find it easy to both get up and running and customize to your liking. Very open to feedback/questions!",
            "header": "Question",
            "link": "https://old.reddit.com/r/n8n/comments/1luvgkn/create_an_aipowered_upwork_job_scorer_and/",
            "Extracted Links": "[\"https://youtu.be/fVegeKYAzz8\"]"
          }
        }
      },
      {
        "json": {
          "id": "reclQ5tHPwhlLnjvQ",
          "createdTime": "2025-07-24T10:42:33.000Z",
          "fields": {
            "Name": "Aggravating-Put-9464",
            "body_text": "Hi! I had a client with >70 files in knowledge base - docs, presentation, tables and videos, who was needed to build RAG agent for internal use. The problem is that files have different overlapping data that doesn't suit for each request - we decided to add additional step for data retrieval to have more focused results. I built a two-step retrieval pipeline using Supabase + n8n, where you first filter files by their description/metadata, then only run vector search within those relevant files. The main problem that there is no way in original Supabase nodes in n8n to setup metadata filtration via array of values - there is hardcoded \"equal\" condition. To solve it I made custom tools with code and special Supabase functions. Video guide: https://youtu.be/asXVOHg89hs How it works: 1. When chat message received Starts the workflow whenever a user sends a question to the chatbot. 2. Find Files Takes the user‚Äôs query Generates an embedding (OpenAI) Runs a semantic search (via Supabase match_files function) Returns a list of files (with their IDs & similarity) that are the best match, based on the file description/metadata‚Äînot full file content yet 3. Filter Similarity Filters file matches so only those above a certain similarity score are kept Controls ‚Äúhow relevant‚Äù is relevant enough Useful for not passing irrelevant/weak matches 4. Aggregate Files Collects all matching file IDs into a list/array This is your shortlist of files to actually search in the next step 5. Database Retrieval (Knowledge Base) Takes the user‚Äôs original query Uses OpenAI to generate embedding again Runs Supabase vector search (match_documents) but filters by the file IDs from the previous step Returns only chunks from the shortlisted files, sorted by similarity, with file names and URLs 6. OpenAI (LLM Completion) Feeds the filtered, relevant chunks to the LLM to answer the user‚Äôs question Always returns the file URL as reference 7. Set Output Prepares and formats the final output for the user/chatbot",
            "header": "From Star Trek to N8N: How I turned my 20 years of Corporate Ops experience into real-world automations",
            "link": "https://old.reddit.com/r/n8n/comments/1lkwqz7/multilevel_rag_ai_agent_guide/",
            "Extracted Links": "[\"https://youtu.be/asXVOHg89hs\"]"
          }
        }
      },
      {
        "json": {
          "id": "recmngXSj4QCRl8ps",
          "createdTime": "2025-07-24T10:42:33.000Z",
          "fields": {
            "Name": "Maamriya",
            "body_text": "Smarter Chatbots with n8n Agent Node & OpenAI: Text + Voice, Step-by-Step I want to share a practical structure for building next-level chatbots and assistants by combining the n8n Agent node with OpenAI‚Äîhandling both text and voice messages in Telegram, all with an AI agent. What‚Äôs unique about this approach is how seamlessly n8n acts as the bridge: taking a message from Telegram, sending it to OpenAI via the API, receiving the AI-generated answer, and then passing it back to the user‚Äîall within your workflow. This transforms n8n into a true orchestrator of conversations, letting the Agent node manage message routing, AI logic, and response delivery in one automated loop. Here‚Äôs a high-level view: A user sends a message (text or voice) to your Telegram bot. n8n captures that message and, if needed, transcribes voice to text. n8n passes the user‚Äôs message to OpenAI via API (through the Agent node). OpenAI generates a reply ‚Äîas smart or as specific as you want, guided by your system prompt. The answer is returned to n8n , which then handles sending the reply straight back to your Telegram user‚Äîcompleting the loop. This lets you build real conversational AI bots with no code, using just nodes, flows, and your own creativity. üîπ The Core Idea AI everywhere: The Agent node lets you plug advanced LLMs (OpenAI, Claude, Grok, etc.) right into n8n. Not just text: My workflow also handles Telegram voice notes‚Äîthese get transcribed, then processed by the AI agent. Unified logic: Whether the user types or talks, the agent understands and replies‚Äîinstantly. üõ†Ô∏è How I Built It: Step-By-Step (Technical Outline) Here‚Äôs the practical structure so you can recreate (or adapt) it: Trigger (Telegram node): Set up a bot in Telegram and connect it to n8n. The trigger is ‚ÄúOn Message,‚Äù capturing every message (text/voice). Switch node (Type Check): Branch workflow: Is the message text or voice? Use a Switch node to check if message.text exists (text) or message.voice exists (voice). Text Path: If it‚Äôs text, pass the message content directly toward the Agent node. Voice Path: If it‚Äôs a voice note: Get File: Use Telegram‚Äôs ‚ÄúGet File‚Äù node to download the voice message using its File ID. Transcribe: Add the OpenAI ‚ÄúTranscribe Audio‚Äù node (Whisper) to convert voice to text. Output: You now have clean text, ready for the AI agent. Agent Node (The AI Core): Add the n8n Agent node after both paths (merge/join if needed). Select your model (e.g., OpenAI Chat). Configure a System Prompt to guide the AI agent‚Äôs tone/behavior (e.g., ‚ÄúYou are a helpful assistant. Answer every question clearly and professionally.‚Äù). Pass in the user message (original text or transcribed text). Behind the scenes: The Agent node sends the message to OpenAI‚Äôs API, gets the answer, and hands it back to your workflow. Reply (Telegram ‚ÄúSend Message‚Äù node): Take the output from the Agent node (AI reply). Send it back to the user in Telegram via their Chat ID. (Optional) : Log chats, add extra steps (e.g., Sheets, Notion), or expand the flow based on use case. No code needed‚Äîjust node configuration and logical connections! üí° What Makes This Special? Handles both text and voice in one clean flow. Supports multiple LLMs‚Äîswap OpenAI for Claude, Grok, Mistral, etc. System prompt makes it easy to customize your AI agent‚Äôs ‚Äúpersonality.‚Äù Reusable for other platforms: WhatsApp, Discord, web forms, and more. üöÄ What You Can Build 24/7 smart Telegram/WhatsApp bots. Voice-based Q&A or help desk agents. Multichannel support workflows, all using the same logic. üé• Full Video Tutorial Want the full step-by-step tutorial with screen sharing and live build ? https://www.youtube.com/watch?v=EYxBm42ja0k",
            "header": "Discussion",
            "link": "https://old.reddit.com/r/n8n/comments/1lxxzcw/showcase_smarter_chatbots_with_n8n_agent_node/",
            "Extracted Links": "[\"https://www.youtube.com/watch?v=EYxBm42ja0k\"]"
          }
        }
      },
      {
        "json": {
          "id": "reccZc89BE2L4fApj",
          "createdTime": "2025-07-24T10:42:34.000Z",
          "fields": {
            "Name": "prometheus_one",
            "body_text": "I was spending 4+ hours per video just reformatting the same content for different platforms. YouTube descriptions, Twitter threads, LinkedIn posts, Instagram captions - pure manual hell. So I built an AI workflow that does it all automatically. The magic: Paste transcript ‚Üí Hit \"start\" ‚Üí Get optimized content for 5+ platforms in 2 minutes. My N8N + Airtable + Gemini AI system generates everything: SEO titles, platform-specific captions, hashtag strategies, even creates organized Google Drive folders. Manual content creation in 2025 = digital manual labor. Work smarter, not harder. Want the complete workflow? I'm sharing the N8N JSON, Airtable templates, and setup guide for free: https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk/ Next i will build the content validation and autoposting with postiz. i found with ai you typically have to challenge the initial outcome to get a better result.",
            "header": "Workflow - Code Not Included",
            "link": "https://old.reddit.com/r/n8n/comments/1loejcq/how_i_save_time_with_n8n_as_a_content_creator/",
            "Extracted Links": "[\"https://docs.google.com/document/d/1ShDW4zEgfmdvu1xAU-v9yNsS01nsVLW7hc0MhypDsrk\"]"
          }
        }
      }
    ]
  },
  "connections": {
    "Schedule Trigger": {
      "main": [
        [
          {
            "node": "Search Parameters",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Search Parameters": {
      "main": [
        []
      ]
    },
    "Reddit API Call": {
      "main": [
        [
          {
            "node": "Quality Filter",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Quality Filter": {
      "main": [
        [
          {
            "node": "Code",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "On form submission": {
      "main": [
        [
          {
            "node": "Reddit API Call",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code": {
      "main": [
        [
          {
            "node": "Airtable Storage",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "35e4c864-43bd-42bd-827a-a39206e54489",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "a13f966a3caeec769bd4e4c4bb2e042c59e2a50fa8b23db7384d39a89c87a458"
  },
  "id": "EkbyqOYfZhKjMKIh",
  "tags": [
    {
      "createdAt": "2025-07-24T10:46:32.953Z",
      "updatedAt": "2025-07-24T10:46:32.953Z",
      "id": "1SBT7uoKd9o075Yr",
      "name": "Productivity"
    }
  ]
}